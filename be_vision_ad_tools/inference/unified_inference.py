"""Intelligent inference system that automatically selects between serial, parallel, and HPC multinode execution"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/13_inference.unified_inference.ipynb.

# %% auto 0
__all__ = ['resolve_test_folders_smart', 'in_jupyter_notebook', 'has_bsub_command', 'detect_execution_environment',
           'run_jupyter_inference', 'run_parallel_inference', 'unified_inference']

# %% ../../nbs/13_inference.unified_inference.ipynb 5
import os
import sys
import shutil
import numpy as np
import subprocess
from pathlib import Path
from typing import Union, List, Dict, Any, Optional, Set,Callable
from multiprocessing import Pool, cpu_count
import json
from tqdm import tqdm
import os
from platform import system

from fastcore.all import *
from fastcore.test import *


# %% ../../nbs/13_inference.unified_inference.ipynb 9
# Import from existing notebooks
from be_vision_ad_tools.inference.prediction_system import (
    predict_image_list_from_file_enhanced
)

from .multinode_inference import *

from be_vision_ad_tools.inference.multinode_from_aiop_tool import (
    HPC_Job,
    DistributeHPC,
    print_status
)


# %% ../../nbs/13_inference.unified_inference.ipynb 13
def resolve_test_folders_smart(
    test_folders: Union[str, Path, List[Union[str, Path]]]  # Folder(s), file(s), or mixed
) -> List[Path]:  # Returns list of image paths
    """Resolve test_folders to image paths - handles lists, flat folders, and nested folders."""
    
    if not isinstance(test_folders, list):
        test_folders = [test_folders]
    
    image_paths = []
    
    for folder_or_file in test_folders:
        path = Path(folder_or_file)
        
        if not path.exists():
            print(f"âš ï¸  Warning: '{folder_or_file}' does not exist")
            continue
        
        if path.is_file() and is_image_file(path):
            # It's an image file
            image_paths.append(path)
            
        elif path.is_dir():
            # It's a directory - use smart folder scanning
            try:
                folder_info = scan_folder_structure(path)
                image_paths.extend(folder_info['all_image_paths'])
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to scan '{folder_or_file}': {e}")
                # Fallback to old behavior
                for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif']:
                    image_paths.extend(path.glob(ext))
                    image_paths.extend(path.glob(ext.upper()))
        else:
            print(f"âš ï¸  Warning: '{folder_or_file}' is not a valid file or directory")
    
    # Remove duplicates and sort
    unique_paths = sorted(set(image_paths))
    
    print(f"ðŸ“ Resolved {len(unique_paths)} images from {len(test_folders)} input path(s)")
    return unique_paths


# %% ../../nbs/13_inference.unified_inference.ipynb 19
def in_jupyter_notebook() -> bool:
    """Check if code is running in a Jupyter notebook."""
    try:
        # Check for IPython shell
        shell = get_ipython().__class__.__name__
        if shell == 'ZMQInteractiveShell':
            return True  # Jupyter notebook or qtconsole
        elif shell == 'TerminalInteractiveShell':
            return False  # Terminal running IPython
        else:
            return False  # Other type
    except NameError:
        return False  # Probably standard Python interpreter


# %% ../../nbs/13_inference.unified_inference.ipynb 21
def has_bsub_command() -> bool:
    """Check if bsub command is available (HPC environment)."""
    return shutil.which("bsub") is not None


# %% ../../nbs/13_inference.unified_inference.ipynb 23
def detect_execution_environment() -> str:
    """Detect execution environment and return appropriate mode."""
    if in_jupyter_notebook():
        return "jupyter"
    elif has_bsub_command():
        return "hpc"
    else:
        return "parallel"


# %% ../../nbs/13_inference.unified_inference.ipynb 30
def run_jupyter_inference(
    model_path: Union[str, Path], # Path to the model file
    image_path: Union[str, Path, List[Path]], # List of image paths to process
    output_dir: Union[str, Path] = "inference_results", # Output directory
    save_heatmaps: bool = False, # Whether to save heatmaps
    heatmap_style: str = "side_by_side", # Style of heatmap to save
    jpeg_quality: int = 95,
    compress: bool = True,
    preprocessing_fn=None, # Preprocessing function
    preprocessing_kwargs=None, # Preprocessing kwargs
    **kwargs
) -> Dict[str, Any]:
    """Execute inference serially using simple for-loop (Jupyter mode)."""

    image_paths = resolve_test_folders_smart(image_path)
    
    print(f"ðŸ““ Running in Jupyter mode (serial for-loop)")
    print(f"   Processing {len(image_paths)} images sequentially...")
    
    model_path = Path(model_path)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Create temporary batch file with all images
    batch_file = output_dir / "jupyter_batch_images.txt"
    create_batch_list_file(image_paths, batch_file)
    
    # Use existing prediction system
    results = predict_image_list_from_file_enhanced(
        model_path=model_path,
        image_list_file=batch_file,
        batch_id="jupyter_batch",
        output_dir=output_dir,
        save_results=save_heatmaps,
        heatmap_style=heatmap_style,
        compress=compress,
        jpeg_quality=jpeg_quality,
        preprocessing_fn=preprocessing_fn,
        preprocessing_kwargs=preprocessing_kwargs,
        **kwargs
    )
    
    print(f"âœ… Jupyter inference complete: {len(image_paths)} images processed")
    
    return {
        "mode": "jupyter",
        "total_images": len(image_paths),
        "output_dir": str(output_dir),
        "results": results
    }


# %% ../../nbs/13_inference.unified_inference.ipynb 44
def _process_batch_worker(args: tuple) -> Dict[str, Any]:
    """Worker function for parallel batch processing."""
    model_path, batch_images, batch_id, output_dir, save_heatmaps, heatmap_style, compress, jpeg_quality, preprocessing_fn, preprocessing_kwargs, kwargs = args
    
    # Create batch file
    batch_list_file = Path(output_dir) / "batch_lists" / f"{batch_id}_images.txt"
    create_batch_list_file(batch_images, batch_list_file)
    
    # Process batch
    results = predict_image_list_from_file(
        model_path=model_path,
        image_list_file=batch_list_file,
        batch_id=batch_id,
        output_dir=output_dir,
        save_results=save_heatmaps,
        heatmap_style=heatmap_style,
        compress=compress,
        jpeg_quality=jpeg_quality,
        preprocessing_fn=preprocessing_fn,
        preprocessing_kwargs=preprocessing_kwargs,
        **kwargs
    )
    
    return {
        "batch_id": batch_id,
        "num_images": len(batch_images),
        "results": results
    }


# %% ../../nbs/13_inference.unified_inference.ipynb 45
def run_parallel_inference(
    model_path: Union[str, Path],
    image_path: Union[str, Path, List[Path]],
    batch_size: int = 100,
    num_workers: Optional[int] = None,
    output_dir: Union[str, Path] = "parallel_results",
    save_heatmaps: bool = False,
    heatmap_style: str = "side_by_side",
    compress: bool = True,
    jpeg_quality: int = 95,
    num_nodes: int = 10,
    preprocessing_fn=None,
    preprocessing_kwargs=None,
    **kwargs
) -> Dict[str, Any]:
    """Execute inference in parallel using multiprocessing.Pool."""

    print("ðŸš€ SMART FOLDER INFERENCE DISTRIBUTION")
    print("="*70)
    
    # Step 1: Validate inputs (fail fast)
    print("\nðŸ“‹ Step 1: Validating inputs...")
    model_path, root_path = validate_inference_inputs(
        model_path=model_path,
        root_path=image_path,
        batch_size=batch_size,
        num_nodes=num_nodes
    )
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ… Output directory: {output_dir}")
    
    # Step 2: Scan folder structure (auto-detect flat vs nested)
    print(f"\nðŸ“¡ Step 2: Scanning folder structure...")
    folder_info = scan_folder_structure(root_path)
    
    # Step 3: Create smart batches
    print(f"\nðŸ”¨ Step 3: Creating smart batches...")
    batches = create_smart_batches(folder_info, batch_size)



    # Split into batches
    image_batches = create_smart_batches(
        Path(image_path), batch_size=batch_size)
    
    if num_workers is None:
        num_workers = cpu_count()
    
    print(f"âš¡ Running in parallel mode with {num_workers} workers")
    
    model_path = Path(model_path)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    
    # Prepare worker arguments
    worker_args = []
    for i, batch in enumerate(image_batches):
        batch_id = f"batch_{i+1:04d}"
        batch_kwargs = {
            "save_heatmaps": save_heatmaps,
            "heatmap_style": heatmap_style,
            "compress": compress,
            "jpeg_quality": jpeg_quality,
            "preprocessing_fn": preprocessing_fn,
            "preprocessing_kwargs": preprocessing_kwargs,
            **kwargs
        }
        worker_args.append((model_path, batch, batch_id, output_path, batch_kwargs))
    
    # Execute in parallel
    with Pool(processes=num_workers) as pool:
        batch_results = list(tqdm(
            pool.imap(_process_batch_worker, worker_args),
            total=len(worker_args),
            desc="Processing batches"
        ))
    
    print(f"âœ… Parallel inference complete: {len(image_batches)} batches processed")
    
    return {
        "mode": "parallel",
        "total_images": len(image_paths),
        "num_batches": len(image_batches),
        "num_workers": num_workers,
        "output_dir": str(output_path),
        "results": batch_results
    }


# %% ../../nbs/13_inference.unified_inference.ipynb 51
def unified_inference(
    model_path: Union[str, Path],
    test_folders: Union[str, Path, List[Union[str, Path]]],
    batch_size: int = 100,
    execution_mode: str = "auto",
    num_nodes: int = 4,
    num_workers: Optional[int] = None,
    output_dir: Optional[Union[str, Path]] = None,
    save_heatmaps: bool = False,
    heatmap_style: str = "cv2_side_by_side",
    **kwargs
) -> Dict[str, Any]:
    """Unified inference function that automatically selects execution strategy.
    
    Args:
        model_path: Path to trained model
        test_folders: Image folder(s) or file path(s)
        batch_size: Maximum images per batch
        execution_mode: "auto", "jupyter", "hpc", or "parallel"
        num_nodes: Number of HPC nodes (for HPC mode)
        num_workers: Number of parallel workers (for parallel mode, default: cpu_count())
        output_dir: Output directory for results
        save_heatmaps: Whether to save visualization heatmaps
        heatmap_style: Visualization style
        **kwargs: Additional arguments passed to inference functions
    
    Returns:
        Dictionary with inference results and metadata
    """
    
    print("ðŸš€ Starting Unified Inference System")
    print("=" * 50)
    
    # Auto-detect or validate execution mode
    if execution_mode == "auto":
        execution_mode = detect_execution_environment()
        print(f"ðŸŽ¯ Auto-detected mode: {execution_mode}")
    else:
        if execution_mode not in ["jupyter", "hpc", "parallel"]:
            raise ValueError(f"Invalid execution_mode: {execution_mode}")
        print(f"ðŸŽ¯ Using specified mode: {execution_mode}")
    
    # Resolve image paths (handles lists, flat folders, and nested folders)
    image_paths = resolve_test_folders_smart(test_folders)
    
    if not image_paths:
        raise ValueError("No valid images found in test_folders")
    
    # Set default output directory based on mode
    if output_dir is None:
        output_dir = f"{execution_mode}_inference_results"
    
    # Execute based on detected/specified mode
    print("=" * 50)
    
    if execution_mode == "jupyter":
        results = run_jupyter_inference(
            model_path=model_path,
            image_paths=image_paths,
            output_dir=output_dir,
            save_heatmaps=save_heatmaps,
            heatmap_style=heatmap_style,
            **kwargs
        )
    
    elif execution_mode == "hpc":
        results = run_hpc_multinode_inference(
            model_path=model_path,
            image_paths=image_paths,
            batch_size=batch_size,
            num_nodes=num_nodes,
            output_dir=output_dir,
            save_heatmaps=save_heatmaps,
            heatmap_style=heatmap_style,
            **kwargs
        )
    
    elif execution_mode == "parallel":
        results = run_parallel_inference(
            model_path=model_path,
            image_paths=image_paths,
            batch_size=batch_size,
            num_workers=num_workers,
            output_dir=output_dir,
            save_heatmaps=save_heatmaps,
            heatmap_style=heatmap_style,
            **kwargs
        )
    
    print("=" * 50)
    print(f"ðŸŽ‰ Unified inference complete!")
    print(f"   Mode: {results['mode']}")
    print(f"   Images: {results['total_images']}")
    print(f"   Output: {results['output_dir']}")
    
    return results

