"""Performing inference on multiple node"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/10_inference.multinode_infer.ipynb.

# %% auto 0
__all__ = ['CURRETNT_NB', 'resolve_test_folders', 'validate_model_path', 'split_image_list', 'create_batch_list_file',
           'create_inference_command', 'create_hpc_job', 'create_multinode_jobs_fresh', 'submit_hpc_jobs',
           'wait_and_summarize_jobs', 'run_multinode_inference']

# %% ../../nbs/10_inference.multinode_infer.ipynb 3
import sys
from pathlib import Path
from typing import List
from fastcore.all import *
from fastcore.script import *


# %% ../../nbs/10_inference.multinode_infer.ipynb 5
from be_vision_ad_tools.inference.multinode_from_aiop_tool import (
    DistributeHPC, HPC_Job
)


# %% ../../nbs/10_inference.multinode_infer.ipynb 6
CURRETNT_NB='/home/ai_dsx.work/data/projects/be-vision-ad-tools/nbs'

# %% ../../nbs/10_inference.multinode_infer.ipynb 7
from be_vision_ad_tools.inference.prediction_system import (
    split_image_list, generate_hpc_commands, predict_image_list_from_file, 
    merge_batch_results
)


# %% ../../nbs/10_inference.multinode_infer.ipynb 8
from typing import Union, List, Optional, Dict, Any, Tuple
from fastcore.test import *
import os
import glob
from pathlib import Path
import json

# %% ../../nbs/10_inference.multinode_infer.ipynb 9
def resolve_test_folders(
    test_folders: Union[str, Path,List[[Union[str, Path]]]]  # Could be str , image list ,[image_list +*.png + .jpg]
    ) -> List[Path]:
    """
    Resolve test_folders parameter to a list of image paths.
        
    Example:
        >>> folders = resolve_test_folders("path/to/images")
        >>> folders = resolve_test_folders(["folder1", "folder2"])  
        >>> folders = resolve_test_folders(["folder1", "image1.jpg", "folder2"])
    """
    if not isinstance(test_folders, list):
        test_folders = [test_folders]
    
    image_paths = []
    supported_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
    
    for folder_or_file in test_folders:
        path = Path(folder_or_file)
        
        if path.is_file() and path.suffix.lower() in supported_extensions:
            # It's an image file
            image_paths.append(path)
        elif path.is_dir():
            # It's a directory - find all images
            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif']:
                image_paths.extend(path.glob(ext))
                image_paths.extend(path.glob(ext.upper()))
        else:
            print(f"âš ï¸  Warning: '{folder_or_file}' is not a valid file or directory")
    
    # Remove duplicates and sort
    unique_paths = sorted(set(image_paths))
    
    print(f"ðŸ“ Resolved {len(unique_paths)} images from {len(test_folders)} input path(s)")
    return unique_paths

# %% ../../nbs/10_inference.multinode_infer.ipynb 13
def validate_model_path(model_path: Union[str, Path]) -> Path:
    """Validate that model path exists and return Path object."""
    model_path = Path(model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"Model not found: {model_path}")
    return model_path

# %% ../../nbs/10_inference.multinode_infer.ipynb 24
def split_image_list(
    image_list: List[Union[str, Path]],  # List of image paths to split
    batch_size: int,  # Maximum number of images per batch
    batch_strategy: str = "round_robin"  # "round_robin" (balanced) or "chunk" (consecutive)
) -> List[List[Path]]:  # Returns list of image path lists, one for each batch
    """Split a large image list into batches based on batch size for parallel processing."""
    
    if not image_list:
        return []
    
    if batch_size <= 0:
        raise ValueError("Batch size must be positive")
    
    # Convert to Path objects
    paths = [Path(img) for img in image_list]
    
    if batch_size >= len(paths):
        # Batch size larger than total images - return single batch with all images
        return [paths]
    
    # Calculate number of batches needed
    num_batches = (len(paths) + batch_size - 1) // batch_size  # Ceiling division
    
    batches = [[] for _ in range(num_batches)]
    
    if batch_strategy == "round_robin":
        # Distribute images evenly across batches (balanced)
        for i, path in enumerate(paths):
            batch_idx = i % num_batches
            batches[batch_idx].append(path)
    
    elif batch_strategy == "chunk":
        # Split into consecutive chunks of batch_size
        for i in range(0, len(paths), batch_size):
            batch_idx = i // batch_size
            batch_end = min(i + batch_size, len(paths))
            batches[batch_idx] = paths[i:batch_end]
    
    else:
        raise ValueError(f"Unknown batch strategy: {batch_strategy}")
    
    # Remove empty batches (shouldn't happen with correct logic, but safety check)
    batches = [batch for batch in batches if batch]
    
    print(f"ðŸ“¦ Split {len(paths)} images into {len(batches)} batches (max {batch_size} per batch):")
    for i, batch in enumerate(batches):
        print(f"   Batch {i+1}: {len(batch)} images")
    
    return batches

# %% ../../nbs/10_inference.multinode_infer.ipynb 30
def create_batch_list_file(batch: List[Path], batch_list_file: Path) -> None:
    """Create text file with image paths for batch processing."""
    batch_list_file.parent.mkdir(parents=True, exist_ok=True)
    with open(batch_list_file, 'w') as f:
        for img_path in batch:
            f.write(f"{img_path}\n")

# %% ../../nbs/10_inference.multinode_infer.ipynb 35
def create_inference_command(
    model_path: Path, 
    batch_list_file: Path, 
    batch_id: str, 
    output_dir: Path,
    save_heatmaps: bool = True, 
    heatmap_style: str = "combined"
) -> List[str]:  # Returns list of command arguments for proper HPC execution
    """Create Python command list for batch inference."""
    python_code = (
        f"from be_vision_ad_tools.inference.prediction_system import predict_image_list_from_file; "
        f"predict_image_list_from_file("
        f"model_path='{model_path}', "
        f"image_list_file='{batch_list_file}', "
        f"batch_id='{batch_id}', "
        f"output_dir='{output_dir}', "
        f"save_heatmaps={save_heatmaps}, "
        f"heatmap_style='{heatmap_style}')"
    )
    
    # Return proper command list format for HPC execution
    return ["python", "-c", python_code]

# %% ../../nbs/10_inference.multinode_infer.ipynb 46
def create_hpc_job(
    batch_id: str,  # Unique identifier for the batch
    command: Union[str, List[str]],  # Command string or list to execute
    job_name_prefix: str = "anomaly_inference",  # Prefix for job naming
    cores: int = 4,  # Number of CPU cores to request
    **hpc_kwargs  # Additional HPC job parameters
) -> HPC_Job:  # Returns configured HPC job object
    """Create single HPC job for batch inference with correct parameters."""
    
    # Handle both string and list command formats
    if isinstance(command, str):
        # Legacy string format - wrap in list
        cmd_list = [command]
    elif isinstance(command, list):
        # New list format - use directly
        cmd_list = command
    else:
        raise ValueError(f"Command must be string or list, got {type(command)}")
    
    job = HPC_Job(
        cmd=cmd_list,
        cores=cores,
        **hpc_kwargs
    )
    
    # Store batch_id for identification
    job.description = f"{job_name_prefix}_{batch_id}"
    
    return job

# %% ../../nbs/10_inference.multinode_infer.ipynb 48
# Create a better __repr__ method for HPC_Job using fastcore's patch
#@patch_to(HPC_Job)
#def __repr__(self: HPC_Job) -> str:
    #"""Better representation for HPC_Job objects."""
    #state_names = {
        #1: "NONE", 2: "SUBMITTED", 4: "WAITING", 
        #8: "RUNNING", 16: "COMPLETED", 4096: "BSUB_FAILED", 8192: "TASK_FAILED"
    #}
    #state_name = state_names.get(self.state, f"UNKNOWN({self.state})")
    
    #cmd_display = self.command[:2] if len(self.command) > 2 else self.command
    #if len(self.command) > 2:
        #cmd_display = f"{cmd_display}... [{len(self.command)} args]"
    
    #return (f"HPC_Job(cmd={cmd_display}, "
            #f"state={state_name}, "
            #f"job_id={self.lsf_job_id or 'None'}, "
            #f"description='{self.description}')")


# %% ../../nbs/10_inference.multinode_infer.ipynb 51
def create_multinode_jobs_fresh(
    model_path: Union[str, Path],  # Path to trained model checkpoint
    test_folders: Union[str, Path, List[Union[str, Path]]],  # Test image folders/files
    batch_size: int = 100,  # Maximum images per batch
    output_dir: str = "multinode_results",  # Output directory for results
    **job_kwargs  # Additional HPC job parameters
) -> List[HPC_Job]:  # Returns list of HPC jobs for execution
    """Create list of HPC jobs for multinode inference with batch size control."""
    model_path = validate_model_path(model_path)
    image_paths = resolve_test_folders(test_folders)
    
    if not image_paths:
        raise ValueError("No valid images found in test_folders")
    
    # Split into batches based on batch_size and create jobs
    image_batches = split_image_list(image_paths, batch_size=batch_size)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    jobs = []
    for i, batch in enumerate(image_batches):
        batch_id = f"batch_{i+1:04d}"
        batch_list_file = output_path / "batch_lists" / f"{batch_id}_images.txt"
        
        create_batch_list_file(batch, batch_list_file)
        command = create_inference_command(
            model_path, 
            batch_list_file, 
            batch_id, 
            output_path)
        
        # Use corrected create_hpc_job function
        job = create_hpc_job(batch_id, command, **job_kwargs)
        jobs.append(job)
    
    return jobs

# %% ../../nbs/10_inference.multinode_infer.ipynb 53
def submit_hpc_jobs(jobs: List[HPC_Job], num_nodes: int = 4) -> DistributeHPC:
    """Submit HPC jobs and return distributor for monitoring."""
    
    # Create DistributeHPC with worker parameter (not jobs parameter)
    distributor = DistributeHPC(worker=num_nodes)
    
    # Set jobs using the set_jobs method
    distributor.set_jobs(jobs)
    
    # Start the job execution
    distributor.start()
    
    print(f"âœ… Submitted {len(jobs)} jobs to {num_nodes} worker nodes")
    return distributor

# %% ../../nbs/10_inference.multinode_infer.ipynb 62
def wait_and_summarize_jobs(distributor: DistributeHPC, jobs: List[HPC_Job]) -> Dict[str, Any]:
    """Wait for jobs completion and return summary statistics."""
    
    # Note: DistributeHPC API may not have wait_for_all_jobs method
    # Check if the method exists before calling
    if hasattr(distributor, 'wait_for_all_jobs'):
        try:
            distributor.wait_for_all_jobs()
            print("âœ… All jobs completed!")
        except Exception as e:
            print(f"âš ï¸ Error waiting for jobs: {e}")
    else:
        print("âš ï¸ DistributeHPC doesn't have wait_for_all_jobs method")
        print("ðŸ’¡ You may need to monitor jobs manually using distributor status")
    
    # Count job statuses - check if jobs have status methods
    successful = 0
    failed = 0
    other = 0
    
    for job in jobs:
        if hasattr(job, 'get_status'):
            status = job.get_status()
            if status == "completed":
                successful += 1
            elif status == "failed":
                failed += 1
            else:
                other += 1
        elif hasattr(job, 'state'):
            # Use state attribute if available
            state = job.state
            if state == job.JOB_COMPLETED:
                successful += 1
            elif state == job.JOB_TASK_FAILED:
                failed += 1
            else:
                other += 1
        else:
            other += 1
    
    print(f"ðŸ“Š Results: âœ… {successful} successful, âŒ {failed} failed, ðŸ”„ {other} other")
    
    return {
        "total_jobs": len(jobs),
        "successful_jobs": successful, 
        "failed_jobs": failed,
        "other_jobs": other
    }

# %% ../../nbs/10_inference.multinode_infer.ipynb 66
def run_multinode_inference(
    model_path: Union[str, Path],  # Path to trained anomaly detection model
    test_folders: Union[str, Path, List[Union[str, Path]]],  # Test image sources
    num_nodes: int = 4,  # Number of HPC nodes to use
    batch_size: int = 100,  # Maximum images per batch (not images_per_batch)
    output_dir: str = "multinode_results",  # Output directory path
    wait_for_completion: bool = True,  # Whether to wait for job completion
    **job_kwargs  # Additional HPC job parameters
) -> Dict[str, Any]:  # Returns results dictionary with job information
    """Run multinode anomaly detection inference using HPC jobs with batch size control."""
    print("ðŸš€ Starting Multinode Inference")
    
    # Create and submit jobs
    jobs = create_multinode_jobs(model_path, test_folders, batch_size, output_dir, **job_kwargs)
    distributor = submit_hpc_jobs(jobs, num_nodes)
    
    # Wait and get results if requested
    results = {"jobs": jobs, "distributor": distributor, "output_dir": output_dir}
    
    if wait_for_completion:
        job_summary = wait_and_summarize_jobs(distributor, jobs)
        results.update(job_summary)
    
    return results
