"""Comprehensive system for loading trained models and predicting on new images"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/06_inference.prediction_system.ipynb.

# %% auto 0
__all__ = ['logger', 'read_image_cv_', 'calculate_heatmap_summary', 'show_prediction_result', 'predict_image',
           'process_single_image', 'validate_image_list', 'filter_valid_images', 'ensure_valid_images_exist',
           'process_images_batch', 'calculate_batch_statistics', 'print_batch_summary', 'prepare_json_results',
           'save_results_to_json', 'print_processing_info', 'setup_output_directory', 'predict_image_list',
           'predict_image_list_from_file_enhanced', 'get_images_', 'create_poster_from_results_', 'run_inference_batch',
           'create_inference_poster_', 'train_model_and_create_posters', 'save_heatmap_visualization', 'batch_predict',
           'split_image_list', 'generate_hpc_commands', 'predict_image_list_from_file', 'merge_batch_results']

# %% ../../nbs/06_inference.prediction_system.ipynb 4
import os
import sys
from pathlib import Path
from typing import Union, List, Optional, Dict, Any, Tuple, Callable
import warnings
from dataclasses import dataclass
from inspect import signature
import json

# Core libraries
import numpy as np
import matplotlib.pyplot as plt
import cv2

from PIL import Image
import torch
from torchvision import transforms
from torchvision.transforms.v2.functional import to_dtype, to_image

# FastCore
from fastcore.all import *


import random
from datetime import datetime
import inspect

# %% ../../nbs/06_inference.prediction_system.ipynb 7
# Anomalib imports
import anomalib
from anomalib import TaskType
from anomalib.deploy import TorchInferencer, OpenVINOInferencer
from anomalib.models import (
    Padim, Patchcore, Cflow, Fastflow, Stfpm,
    EfficientAd, Draem, ReverseDistillation,
    Dfkde, Dfm, Ganomaly, Cfa, Csflow, Dsr, Fre, Rkde, Uflow
)
from anomalib.utils.visualization.image import ImageVisualizer, VisualizationMode, ImageResult
from anomalib.data.utils import read_image


# %% ../../nbs/06_inference.prediction_system.ipynb 8
# Import our training config
# Import specific classes to avoid circular import
from ..training.flexible_trainer import ModelType, FlexibleTrainingConfig


# %% ../../nbs/06_inference.prediction_system.ipynb 9
# Configure logging according to cursor rules
import logging
from typing import Any

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Set up proper logging configuration
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Configure anomalib logging
logging.getLogger('anomalib').setLevel(logging.WARNING)

# Add console handler if not already present
if not logger.handlers:
    console_handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)


# %% ../../nbs/06_inference.prediction_system.ipynb 12
def _validate_inputs(
    model_path: Path, # Path of the model file
    image_path: Path # Path of the image file or image directory
    ) -> None:
    """Validate input paths for prediction functions."""
    if not model_path.exists():
        raise FileNotFoundError(f"Model not found: {model_path}")
    if not image_path.exists():
        raise FileNotFoundError(f"Image not found: {image_path}")

# %% ../../nbs/06_inference.prediction_system.ipynb 14
def _detect_device(device: str) -> str:
    """Auto-detect computing device for inference."""
    if device == "auto":
        return "cuda" if torch.cuda.is_available() else "cpu"
    return device

# %% ../../nbs/06_inference.prediction_system.ipynb 16
def _detect_model_class_from_filename(model_path: Path) -> Any:
    """Auto-detect anomaly detection model class from checkpoint filename."""
    filename = model_path.name.lower()

    # PaDiM model (primary focus according to cursor rules)
    if 'padim' in filename:
        return Padim
    elif 'patchcore' in filename:
        return Patchcore
    elif 'cflow' in filename:
        return Cflow
    elif 'fastflow' in filename:
        return Fastflow
    elif 'stfpm' in filename:
        return Stfpm
    elif 'efficientad' in filename:
        return EfficientAd
    elif 'draem' in filename:
        return Draem
    elif 'reverse_distillation' in filename or 'reversedistillation' in filename:
        return ReverseDistillation
    elif 'dfkde' in filename:
        return Dfkde
    elif 'dfm' in filename:
        return Dfm
    elif 'ganomaly' in filename:
        return Ganomaly
    elif 'cfa' in filename:
        return Cfa
    elif 'csflow' in filename:
        return Csflow
    elif 'dsr' in filename:
        return Dsr
    elif 'fre' in filename:
        return Fre
    elif 'rkde' in filename:
        return Rkde
    elif 'uflow' in filename:
        return Uflow
    else:
        # Default to PaDiM as per cursor rules (focus on PaDiM model)
        logger.warning(f"Could not detect model type from {filename}, defaulting to PaDiM")
        return Padim

# %% ../../nbs/06_inference.prediction_system.ipynb 18
def _use_torch_op(
    preprocessing_fn:Callable
) -> bool:
    """Detect if preprocessing function uses Torch or TorchVision operations.
       to apply preprocessing , whether image/255 or not , if tensor then pytorch format and 0->1
    """
    try:
        source = inspect.getsource(preprocessing_fn)
        return 'torch' in source.lower() or 'torchvision' in source.lower()
    except (OSError, TypeError):
        sig = inspect.signature(
            preprocessing_fn)

        for param in sig.parameters.values():
            if param.annotation and 'torch' in str(param.annotation).lower():
                return True
        return False






# %% ../../nbs/06_inference.prediction_system.ipynb 19
def read_image_cv_(
    im_path:Union[str, Path]
    )->np.ndarray:
    img = cv2.imread(str(im_path))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    return img






# %% ../../nbs/06_inference.prediction_system.ipynb 20
def _numpy_to_tensor(
    image:np.ndarray, # Input image as numpy array
    as_tensor:bool=True, # Whether to return a tensor
    preprocessing_fn:Optional[Callable]=None, # Preprocessing function
    preprocessing_kwargs:Optional[Dict[str,Any]]=None # Preprocessing kwargs
    )->torch.Tensor: # Return a tensor (0->1)
    'Reproducing anomalib read_image function from np.array'
    if preprocessing_fn is not None:
        if _use_torch_op(preprocessing_fn):
            logger.info(f'Using torch preprocessing function: {preprocessing_fn.__name__}')
            img = Image.fromarray(image).convert('RGB')
            img = preprocessing_fn(
                img, **preprocessing_kwargs)
        else:
            img = preprocessing_fn(
                image, **preprocessing_kwargs)
    else:
        img = image
    return to_dtype(to_image(img), torch.float32, scale=True) if as_tensor else np.array(image) / 255.0




# %% ../../nbs/06_inference.prediction_system.ipynb 22
def _predict_with_torch_model_(
    model_path:Path, # Path to the model file
    image_path:Path, # Path to the image file
    device:str, # Device to run the model on
    preprocessing_fn:Optional[Callable]=None, # Preprocessing function
    preprocessing_kwargs:Optional[Dict[str,Any]]=None # Preprocessing kwargs
)->Dict[str,Any]: # Return a dictionary with the prediction results
    """Predict with a PyTorch model."""
    inferencer = TorchInferencer(
        path=model_path,
        device=device
    )
    image = read_image_cv_(image_path)
    im_tnsr = _numpy_to_tensor(
        image=image,
        as_tensor=True,
        preprocessing_fn=preprocessing_fn,
        preprocessing_kwargs=preprocessing_kwargs
    )
    pred_result = inferencer.predict(im_tnsr)

    # anomaly map has is between 0 and 1 , for visualization we want it between 0 and 255
    anomaly_map = pred_result.anomaly_map if hasattr(pred_result, 'anomaly_map') else None
    if anomaly_map is not None:
        anomaly_map = (anomaly_map * 255).astype(np.uint8)

    return {
        'anomaly_score': float(pred_result.pred_score),
        'is_anomaly': bool(pred_result.pred_label),
        'prediction': 'ANOMALY' if pred_result.pred_label else 'NORMAL',
        'anomaly_map': anomaly_map,
        'heatmap': pred_result.heat_map
    }



# %% ../../nbs/06_inference.prediction_system.ipynb 26
@patch
def _load_checkpoint(self:TorchInferencer, path: str | Path) -> dict:
    if isinstance(path, str):
        path = Path(path)
    if path.suffix not in (".pt", ".pth"):
        msg = f"Unknown torch checkpoint file format {path.suffix}. Make sure you save the Torch model"
        raise ValueError(msg)

    return torch.load(path, map_location=self.device, weights_only=False)

# %% ../../nbs/06_inference.prediction_system.ipynb 39
def _predict_with_checkpoint_model_(
    model_path:Path, # Path to the model file
    image_path:Path, # Path to the image file
    device:str, # Device to run the model on
    preprocessing_fn:Optional[Callable]=None, # Preprocessing function
    preprocessing_kwargs:Optional[Dict[str,Any]]=None # Preprocessing kwargs
)->Dict[str,Any]: # Return a dictionary with the prediction results
    """Predict with a PyTorch model with checkpoint model."""

    model_class = _detect_model_class_from_filename(model_path)
    model = model_class.load_from_checkpoint(model_path)
    model.eval()
    model.to(device)
    image = read_image_cv_(image_path)
    image = _numpy_to_tensor(
        image=image,
        as_tensor=True,
        preprocessing_fn=preprocessing_fn,
        preprocessing_kwargs=preprocessing_kwargs
    )

    with torch.no_grad():
        pred_result = model(image.unsqueeze(0).to(device))


    # anomaly map has is between 0 and 1 , for visualization we want it between 0 and 255
    anomaly_map = pred_result.anomaly_map if hasattr(pred_result, 'anomaly_map') else None
    if anomaly_map is not None:
        anomaly_map = (anomaly_map * 255).astype(np.uint8)

    return {
        'anomaly_score': float(pred_result.pred_score),
        'is_anomaly': bool(pred_result.pred_label),
        'prediction': 'ANOMALY' if pred_result.pred_label.cpu() else 'NORMAL',
        'anomaly_map': anomaly_map,
        'heatmap': pred_result.heat_map
    }



# %% ../../nbs/06_inference.prediction_system.ipynb 41
def _predict_with_openvino_model_(
    model_path:Path, # Path to the model file
    image_path:Path, # Path to the image file
    device:str, # Device to run the model on
    preprocessing_fn:Optional[Callable]=None, # Preprocessing function
    preprocessing_kwargs:Optional[Dict[str,Any]]=None # Preprocessing kwargs
)->Dict[str,Any]: # Return a dictionary with the prediction results
    """Predict with a OpenVINO model."""
    inferencer = OpenVINOInferencer(
        path=model_path,
        device=device
    )
    image = read_image_cv_(image_path)
    image = _numpy_to_tensor(
        image=image,
        as_tensor=True,
        preprocessing_fn=preprocessing_fn,
        preprocessing_kwargs=preprocessing_kwargs
    )
    pred_result = inferencer.predict(image)
    anomaly_map = pred_result.anomaly_map if hasattr(pred_result, 'anomaly_map') else None
    if anomaly_map is not None:
        anomaly_map = (anomaly_map * 255).astype(np.uint8)

    return {
        'anomaly_score': float(pred_result.anomaly_score),
        'is_anomaly': bool(pred_result.pred_label),
        'prediction': 'ANOMALY' if pred_result.pred_label else 'NORMAL',
        'anomaly_map': anomaly_map,
        'heatmap': pred_result.heat_map
    }



# %% ../../nbs/06_inference.prediction_system.ipynb 42
def _create_prediction_result_dict(
    model_path: Path,  # Path of the model file
    image_path: Path,  # Path of the image file or image directory
    model_ext: str    # Extension of the model file
    ) -> Dict[str, Any]: # Result dictionary structure
    """Create initial result dictionary structure."""
    return {
        'image_path': str(image_path),
        'model_path': str(model_path),
        'model_type': model_ext,
        'anomaly_score': 0.0,
        'prediction': 'NORMAL',
        'is_anomaly': False,
        'anomaly_map': None,
        'heatmap': None,
        'saved_files': [],
		'saved_path': None
    }

# %% ../../nbs/06_inference.prediction_system.ipynb 47
def _validate_prediction_inputs(
    anomaly_score: float,# Anomaly score
    prediction: str, # Prediction
    style: str # Style of the heatmap, which can be 'heatmap_only', 'combined', or 'side_by_side'
) -> None:
    """Validate input parameters for prediction visualization."""
    if not isinstance(anomaly_score, (int, float)):
        raise TypeError("anomaly_score must be a number")
    if not isinstance(prediction, str):
        raise TypeError("prediction must be a string")
    if style not in ["heatmap_only", "image_only", "side_by_side"]:
        raise ValueError(f"Invalid style: {style}. Must be one of: heatmap_only, combined, side_by_side")

# %% ../../nbs/06_inference.prediction_system.ipynb 49
def _extract_image_from_result(
    prediction_result: ImageResult # Prediction result object ImageResult is a SimpleNamespace from Anomalib
    ) -> np.ndarray:
    """Extract image array from prediction result."""
    try:
        original_image = prediction_result.image
        if isinstance(original_image, Image.Image):
            return np.array(original_image)
        else:
            return original_image
    except AttributeError:
        raise ValueError("prediction_result must have an 'image' attribute")
    except Exception as e:
        raise ValueError(f"Failed to load image from prediction_result: {e}")

# %% ../../nbs/06_inference.prediction_system.ipynb 51
def _extract_heatmap_from_result(
    prediction_result: ImageResult # Prediction result object ImageResult is a SimpleNamespace from Anomalib
    ) -> np.ndarray: # Return a numpy array of the heatmap
    """Extract heatmap array from prediction result."""
    try:
        heatmap = prediction_result.heat_map
        if not isinstance(heatmap, np.ndarray):
            heatmap = np.array(heatmap)
        return heatmap
    except AttributeError:
        raise ValueError("prediction_result must have a 'heat_map' attribute")

# %% ../../nbs/06_inference.prediction_system.ipynb 53
def calculate_heatmap_summary(
    heatmap: np.ndarray # Heatmap array to analyze
) -> dict: # Dictionary containing min, max, and other statistics
    """Calculate summary statistics for a heatmap array."""
    if not isinstance(heatmap, np.ndarray):
        raise TypeError("heatmap must be a numpy array")

    return {
        'min': float(heatmap.min()),
        'max': float(heatmap.max()),
        'mean': float(heatmap.mean()),
        'std': float(heatmap.std()),
        'sum': float(heatmap.sum()),
        'var': float(heatmap.var())
    }


# %% ../../nbs/06_inference.prediction_system.ipynb 55
def _normalize_heatmap_for_opencv(
    heatmap: np.ndarray # Heatmap array to normalize
    ) -> np.ndarray: # Return a numpy array of the normalized heatmap
    """Normalize heatmap to 0-255 range for OpenCV processing."""
    return ((heatmap - heatmap.min()) / (heatmap.max() - heatmap.min()) * 255).astype(np.uint8)






# %% ../../nbs/06_inference.prediction_system.ipynb 58
def _apply_colormap_to_heatmap(
    heatmap_normalized: np.ndarray # Normalized heatmap to apply colormap to
    ) -> np.ndarray: # Return a numpy array of the colored heatmap
    """Apply colormap to normalized heatmap and convert to RGB."""
    heatmap_colored = cv2.applyColorMap(heatmap_normalized, cv2.COLORMAP_JET)
    return cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)



# %% ../../nbs/06_inference.prediction_system.ipynb 60
def _get_default_figsize(style: str) -> Tuple[int, int]:
    """Get default figure size based on visualization style."""
    if style == "heatmap_only":
        return (8, 8)
    elif style == "combined":
        return (10, 8)
    elif style == "side_by_side":
        return (16, 8)
    else:
        return (10, 8)

# %% ../../nbs/06_inference.prediction_system.ipynb 61
def _ensure_rgb_format(
    image_array: np.ndarray # Image array to ensure is in RGB format
    ) -> np.ndarray: # Return a numpy array of the image in RGB format
    """Ensure image array is in RGB format for display."""
    if image_array.shape[-1] == 3:
        return image_array
    else:
        return cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)

# %% ../../nbs/06_inference.prediction_system.ipynb 63
def _add_colorbar_to_plot(
    heatmap: np.ndarray, # Heatmap array to add colorbar to
    colormap: Optional[str] # Colormap to use
    ) -> None: # Return None
    """Add colorbar to the current matplotlib plot."""
    if colormap is None:
        return
    im = plt.imshow(heatmap, cmap=colormap, alpha=0)  # Invisible image for colorbar
    cbar = plt.colorbar(im, shrink=0.8)
    cbar.set_label('Anomaly Score', rotation=270, labelpad=20)

# %% ../../nbs/06_inference.prediction_system.ipynb 64
def _save_image_with_opencv(
    image_array: np.ndarray, # Image array to save
    save_path: Path, # Path to save the image including the file name+extension
    compress: bool = True, # Whether to compress the image (JPEG format)
    jpeg_quality: int = 95 # JPEG compression quality (0-100, higher is better)
    ) -> None: # Return None
    """Save image array using OpenCV with optional JPEG compression."""
    save_path.parent.mkdir(parents=True, exist_ok=True)

    if compress:
        # Force JPEG format for compression
        save_path_jpeg = save_path.with_suffix('.jpg')
        cv2.imwrite(
            str(save_path_jpeg),
            image_array,
            [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality]
        )
        logger.info(f"Image saved to: {save_path_jpeg} (JPEG quality: {jpeg_quality})")
    else:
        # Save without compression in original format
        cv2.imwrite(str(save_path), image_array)
        logger.info(f"Image saved to: {save_path}")

# %% ../../nbs/06_inference.prediction_system.ipynb 66
def _show_heatmap_only(
    heatmap: np.ndarray,
    anomaly_score: float,
    prediction: str,
    colormap: str,
    figsize: Tuple[int, int],
    show: bool=False,
    save_path: Optional[Path]=None,
    compress: bool = True,
    jpeg_quality: int = 95
) -> None:
    """Display only the heatmap visualization."""
    if not show and save_path is None:
        return

    plt.figure(figsize=figsize)
    plt.imshow(heatmap)
    plt.axis('off')
    plt.title(f"Anomaly Map\nScore: {anomaly_score:.4f} | {prediction}",
             fontsize=14, pad=20)

    #_add_colorbar_to_plot(heatmap, colormap)
    plt.tight_layout()

    if save_path is not None:
        # Convert RGB back to BGR for OpenCV saving
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        heatmap_bgr = cv2.cvtColor(heatmap, cv2.COLOR_RGB2BGR)
        _save_image_with_opencv(
            heatmap_bgr,
            save_path,
            compress,
            jpeg_quality
        )

    if show:
        plt.show()
    else:
        plt.close()


# %% ../../nbs/06_inference.prediction_system.ipynb 69
def _show_side_by_side_visualization(
    original_array: np.ndarray,
    heatmap: np.ndarray,
    image_path: Path,
    anomaly_score: float,
    prediction: str,
    colormap: str,
    figsize: Tuple[int, int],
    show: bool=False,
    save_path: Optional[Path]=None,
    compress: bool = True,
    jpeg_quality: int = 95
) -> None:
    """Display original image and heatmap side by side."""

    if not show and save_path is None:
        return

    # Resize heatmap to match original image dimensions if needed
    if heatmap.shape[:2] != original_array.shape[:2]:
        heatmap = cv2.resize(heatmap, (original_array.shape[1], original_array.shape[0]))

    original_rgb = _ensure_rgb_format(original_array)
    concat_image = np.concatenate([original_rgb, heatmap], axis=1)

    plt.figure(figsize=figsize)
    plt.imshow(concat_image)
    plt.axis('off')
    plt.title(f"Anomaly Detection Result\n{image_path.name}\nScore: {anomaly_score:.4f} | {prediction}",
             fontsize=14, pad=20)

    #_add_colorbar_to_plot(heatmap, colormap)
    plt.tight_layout()

    if save_path is not None:
        concat_bgr = cv2.cvtColor(concat_image, cv2.COLOR_RGB2BGR)

        _save_image_with_opencv(
            concat_bgr,
            save_path,
            compress,
            jpeg_quality)

    if show:
        plt.show()
    else:
        plt.close()



# %% ../../nbs/06_inference.prediction_system.ipynb 72
def _show_image_only(
    image: np.ndarray,
    image_path: Union[str, Path],
    anomaly_score: float,
    prediction: str,
    figsize: Tuple[int, int] = (8, 8),
    show: bool = True,
    save_path: Optional[Union[str, Path]] = None,
    compress: bool = True,
    jpeg_quality: int = 95
) -> None:
    """Display only the original image with prediction information."""
    # Early return if nothing to do
    if not show and save_path is None:
        return

    if image.shape[-1] == 3:
        image_rgb = _ensure_rgb_format(image)
    else:
        image_rgb = image

    # Create figure only if showing or saving
    plt.figure(figsize=figsize)
    if len(image_rgb.shape) == 2:
        plt.imshow(image_rgb, cmap='gray')
    else:
        plt.imshow(image_rgb)
    plt.axis('off')

    title = f"{Path(image_path).name}\nScore: {anomaly_score:.3f} | Prediction: {prediction}"
    plt.title(title, fontsize=14, pad=20)

    plt.tight_layout()

    if save_path is not None:
        if image_rgb.shape[-1] == 3:
            image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
        else:
            image_bgr = image_rgb
        _save_image_with_opencv(
            image_bgr,
            save_path,
            compress,
            jpeg_quality
        )

    if show:
        plt.show()
    else:
        plt.close()


# %% ../../nbs/06_inference.prediction_system.ipynb 73
def show_prediction_result(
    image_path: Union[str, Path],
    prediction_result: ImageResult,
    anomaly_score: float,
    prediction: str,
    style: str = "side_by_side",
    colormap: str = None,#"jet",
    figsize: Optional[Tuple[int, int]] = None,
    show: bool=True,
    save_path: Optional[Union[str, Path]] = None,
    compress: bool = True,
    jpeg_quality: int = 95
) -> None:
    """Display and optionally save prediction results using OpenCV and matplotlib."""
    # Validate inputs
    _validate_prediction_inputs(anomaly_score, prediction, style)

    image_path = Path(image_path)
    save_path = Path(save_path) if save_path is not None else None

    # Extract data from prediction result
    original_array = _extract_image_from_result(prediction_result)
    heatmap = _extract_heatmap_from_result(prediction_result)

    # Process heatmap for visualization
    heatmap_normalized = _normalize_heatmap_for_opencv(heatmap)
    if colormap is not None:
        heatmap_rgb = _apply_colormap_to_heatmap(heatmap_normalized)
    else:
        heatmap_rgb = heatmap_normalized

    # Set figure size
    if figsize is None:
        figsize = _get_default_figsize(style)

    # Display based on style
    if style == "image_only":
        _show_image_only(
            original_array,
            image_path,
            anomaly_score,
            prediction,
            figsize,
            show,
            save_path,
            compress,
            jpeg_quality
        )
    elif style == "heatmap_only":
        _show_heatmap_only(
            heatmap_rgb,
            anomaly_score,
            prediction,
            colormap,
            figsize,
            show,
            save_path,
            compress,
            jpeg_quality
        )
    elif style == "side_by_side":
        _show_side_by_side_visualization(
            original_array,
            heatmap_rgb,
            image_path,
            anomaly_score,
            prediction,
            colormap,
            figsize,
            show,
            save_path,
            compress,
            jpeg_quality
        )
    else:
        raise ValueError(f"Unknown style: {style}. Use 'image_only', 'heatmap_only', or 'side_by_side'")

# %% ../../nbs/06_inference.prediction_system.ipynb 79
def predict_image(
    model_path: Union[str, Path],  # Path to the model file
    image_path: Union[str, Path],  # Path to the image file
    preprocessing_fn: Optional[Callable] = None, # Preprocessing function
    preprocessing_kwargs: Optional[Dict[str, Any]] = None, # Preprocessing kwargs
    save_heatmap: bool = False,  # Save heatmap
    show_heatmap: bool = False,  # Show heatmap
    heatmap_style: str = "side_by_side", # Heatmap style
    output_dir: Optional[Union[str, Path]] = None,  # Output directory
    device: str = "auto",  # Device to use,
    compress: bool = True, # Whether to compress the image (JPEG format)
    jpeg_quality: int = 95 # JPEG compression quality (0-100, higher is better)
) -> Dict[str, Any]:
    """Universal prediction function that auto-detects model type and predicts on image."""
    model_path = Path(model_path)
    image_path = Path(image_path)

    # Validate inputs
    _validate_inputs(model_path, image_path)

    # Auto-detect device
    device = _detect_device(device)

    # Auto-detect model type from extension
    model_ext = model_path.suffix.lower()

    logger.info(f"Predicting with {model_ext} model on {image_path.name}")

    # Initialize result
    result = _create_prediction_result_dict(model_path, image_path, model_ext)

    try:
        # Route to appropriate prediction function based on model type
        if model_ext in ['.pt', '.pth']:
            pred_data = _predict_with_torch_model_(
                model_path,
                image_path,
                device,
                preprocessing_fn,
                preprocessing_kwargs
            )
        elif model_ext == '.ckpt':
            pred_data = _predict_with_checkpoint_model_(
                model_path,
                image_path,
                device,
                preprocessing_fn,
                preprocessing_kwargs
            )
        elif model_ext == '.xml':
            pred_data = _predict_with_openvino_model_(
                model_path,
                image_path,
                device,
                preprocessing_fn,
                preprocessing_kwargs
            )
        else:
            raise ValueError(f"Unsupported model format: {model_ext}")

        # Update result with prediction data
        result.update(pred_data)

        # Handle heatmap visualization if requested
        if (save_heatmap or show_heatmap) and result['anomaly_map'] is not None:
            # Create mock prediction result for visualization
            from types import SimpleNamespace
            pred_result = SimpleNamespace()
            pred_result.heat_map = result['heatmap']
            pred_result.image = Image.open(image_path)


            if show_heatmap and save_heatmap:
                if compress:
                    save_path = Path(output_dir , f'{image_path.stem}_heatmap.jpg')
                else:
                    save_path = Path(output_dir , f'{image_path.stem}_heatmap.png')
                result['saved_path'] = str(save_path)

                show_prediction_result(
                    image_path=image_path,
                    prediction_result=pred_result,
                    anomaly_score=result['anomaly_score'],
                    prediction=result['prediction'],
                    style=heatmap_style,
                    show=show_heatmap,
                    save_path=save_path,
                    compress=compress,
                    jpeg_quality=jpeg_quality
                )
            if not show_heatmap and save_heatmap:
                if compress:
                    save_path = Path(output_dir, f'{image_path.stem}_heatmap.jpg')
                else:
                    save_path = Path(output_dir, f'{image_path.stem}_heatmap.png')
                result['saved_path'] = str(save_path)
                show_prediction_result(
                    image_path=image_path,
                    prediction_result=pred_result,
                    anomaly_score=result['anomaly_score'],
                    prediction=result['prediction'],
                    style=heatmap_style,
                    show=False,
                    save_path=save_path,
                    compress=compress,
                    jpeg_quality=jpeg_quality
                )
                print(f"Heatmap saved to: {save_path}")
            if show_heatmap and not save_heatmap:
                show_prediction_result(
                    image_path=image_path,
                    prediction_result=pred_result,
                    anomaly_score=result['anomaly_score'],
                    prediction=result['prediction'],
                    compress=compress,
                    jpeg_quality=jpeg_quality
                )

        logger.info(f"Prediction: {result['prediction']} (Score: {result['anomaly_score']:.4f})")

    except Exception as e:
        logger.error(f"Prediction failed: {str(e)}")
        result['error'] = str(e)

    return result



# %% ../../nbs/06_inference.prediction_system.ipynb 88
def process_single_image(
    model_path: Path, # Path to the model file
    image_path: Path, # Path to the image file
    save_heatmap: bool, # Whether to save heatmaps
    heatmap_style: str, # Heatmap style
    show_heatmap: bool, # Whether to show heatmap
    output_dir: Path, # Output directory
    device: str, # Device to use
    batch_id: Optional[str], # Batch ID
    compress: bool = True, # Whether to compress the image (JPEG format)
    jpeg_quality: int = 95, # JPEG compression quality (0-100, higher is better)
    preprocessing_fn: Optional[Callable] = None, # Preprocessing function applieed to the images before prediction
    preprocessing_kwargs: Optional[Dict[str, Any]] = None, # Preprocessing kwargs
) -> Dict[str, Any]:
    """Process a single image and return result with batch_id."""
    result = predict_image(
        model_path=model_path,
        image_path=image_path,
        show_heatmap=show_heatmap,
        save_heatmap=save_heatmap,
        heatmap_style=heatmap_style,
        output_dir=output_dir,
        device=device,
        compress=compress,
        jpeg_quality=jpeg_quality,
        preprocessing_fn=preprocessing_fn,
        preprocessing_kwargs=preprocessing_kwargs
    )
    result['image_name'] = Path(image_path).name
    result['batch_id'] = batch_id
    return result


# %% ../../nbs/06_inference.prediction_system.ipynb 93
def validate_image_list(image_list: List[Union[str, Path]]) -> None:
    """Validate that image list is not empty."""
    if not image_list:
        raise ValueError("Image list is empty")

# %% ../../nbs/06_inference.prediction_system.ipynb 96
def filter_valid_images(
    image_list: List[Union[str, Path]] # List of image paths
    ) -> List[Path]: # List of valid image paths
    """Filter and convert image paths to valid Path objects."""
    valid_images = []
    for img_path in image_list:
        img_path = Path(img_path)
        if img_path.exists():
            valid_images.append(img_path)
        else:
            print(f"‚ö†Ô∏è  Image not found, skipping: {img_path}")
    return valid_images

# %% ../../nbs/06_inference.prediction_system.ipynb 100
def ensure_valid_images_exist(
    valid_images: List[Path] # List of valid image paths
    ) -> None:
    """Ensure at least one valid image exists."""
    if not valid_images:
        raise ValueError("No valid images found in the provided list")

# %% ../../nbs/06_inference.prediction_system.ipynb 102
def process_images_batch(
    valid_images: List[Path], # List of valid image paths
    model_path: Path, # Path to the model file
    heatmap_style: str, # Heatmap style
    output_dir: Path, # Output directory
    device: str, # Device to use
    batch_id: Optional[str], # Batch ID
    show_heatmap: bool, # Whether to show heatmap
    save_heatmap: bool, # Whether to save heatmap
    compress: bool = True, # Whether to compress the image (JPEG format)
    jpeg_quality: int = 95, # JPEG compression quality (0-100, higher is better)
    preprocessing_fn: Optional[Callable] = None, # Preprocessing function applied to the images before prediction
    preprocessing_kwargs: Optional[Dict[str, Any]] = None # Preprocessing kwargs for the preprocessing function
) -> Tuple[List[Dict[str, Any]], int, int]:
    """Process all images in batch and return results with counts."""
    results = []
    anomaly_count = 0
    failed_count = 0

    for i, image_path in enumerate(valid_images, 1):
        if i % 100 == 0:
            print(f"   Processed {i}/{len(valid_images)} images...")

        try:
            result = process_single_image(
                model_path=model_path,
                image_path=image_path,
                heatmap_style=heatmap_style,
                output_dir=output_dir,
                device=device,
                batch_id=batch_id,
                show_heatmap=show_heatmap,
                save_heatmap=save_heatmap,
                compress=compress,
                jpeg_quality=jpeg_quality,
                preprocessing_fn=preprocessing_fn,
                preprocessing_kwargs=preprocessing_kwargs
            )
            results.append(result)

            if result.get('is_anomaly', False):
                anomaly_count += 1

        except Exception as e:
            print(f"‚ùå Failed to process {Path(image_path).name}: {str(e)}")
            failed_count += 1
            continue

    return results, anomaly_count, failed_count

# %% ../../nbs/06_inference.prediction_system.ipynb 110
def calculate_batch_statistics(
    results: List[Dict[str, Any]],
    image_list: List[Union[str, Path]],
    valid_images: List[Path],
    anomaly_count: int,
    failed_count: int,
    model_path: Path,
    output_dir: Path,
    batch_id: Optional[str]
) -> Dict[str, Any]:
    """Calculate batch processing statistics."""
    total_processed = len(results)
    normal_count = total_processed - anomaly_count

    return {
        'batch_id': batch_id,
        'total_images_in_list': len(image_list),
        'valid_images_found': len(valid_images),
        'successfully_processed': total_processed,
        'failed_processing': failed_count,
        'normal_count': normal_count,
        'anomaly_count': anomaly_count,
        'anomaly_percentage': (anomaly_count / total_processed * 100) if total_processed > 0 else 0,
        'average_anomaly_score': np.mean([r['anomaly_score'] for r in results]) if results else 0,
        'model_used': str(model_path),
        'output_directory': str(output_dir)
    }

# %% ../../nbs/06_inference.prediction_system.ipynb 112
def print_batch_summary(batch_stats: Dict[str, Any]) -> None:
    """Print batch processing summary."""
    print(f"\nüìä Batch Processing Summary:")
    if batch_stats['batch_id']:
        print(f"   Batch ID: {batch_stats['batch_id']}")
    print(f"   Images in List: {batch_stats['total_images_in_list']}")
    print(f"   Valid Images: {batch_stats['valid_images_found']}")
    print(f"   Successfully Processed: {batch_stats['successfully_processed']}")
    print(f"   Failed: {batch_stats['failed_processing']}")
    print(f"   Normal: {batch_stats['normal_count']} ({100-batch_stats['anomaly_percentage']:.1f}%)")
    print(f"   Anomalies: {batch_stats['anomaly_count']} ({batch_stats['anomaly_percentage']:.1f}%)")
    print(f"   Average Score: {batch_stats['average_anomaly_score']:.4f}")

# %% ../../nbs/06_inference.prediction_system.ipynb 114
def prepare_json_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Remove non-serializable data from results for JSON export."""
    json_results = []
    for r in results:
        json_result = {k: v for k, v in r.items() if k not in ['anomaly_map', 'heatmap']}
        json_results.append(json_result)
    return json_results


def save_results_to_json(
    results: List[Dict[str, Any]],
    batch_stats: Dict[str, Any],
    output_dir: Path,
    batch_id: Optional[str]
) -> str:
    """Save results to JSON file."""
    results_filename = f"batch_results_{batch_id}.json" if batch_id else "batch_results.json"
    results_file = output_dir / results_filename

    json_results = prepare_json_results(results)

    full_results = {
        'statistics': batch_stats,
        'individual_results': json_results
    }

    with open(results_file, 'w') as f:
        json.dump(full_results, f, indent=2)

    print(f"üíæ Results saved to: {results_file}")
    return str(results_file)


# %% ../../nbs/06_inference.prediction_system.ipynb 116
def print_processing_info(
    valid_images: List[Path], # List of valid image paths
    model_path: Path, # Path to the model file
    batch_id: Optional[str] # Batch ID
    ) -> None:
    """Print processing information."""
    print(f"üîç Processing {len(valid_images)} images from list")
    if batch_id:
        print(f"üì¶ Batch ID: {batch_id}")
    print(f"üéØ Using model: {Path(model_path).name}")


def setup_output_directory(
    output_dir: Optional[Union[str, Path]], # Output directory
    batch_id: Optional[str] # Batch ID
    ) -> Path:
    """Setup and create output directory."""
    if output_dir is None:
        output_dir = Path.cwd() / "predictions"
        if batch_id:
            output_dir = output_dir / f"batch_{batch_id}"
    else:
        output_dir = Path(output_dir)
        if batch_id:
            output_dir = output_dir / f"batch_{batch_id}"

    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir

# %% ../../nbs/06_inference.prediction_system.ipynb 117
def predict_image_list(
    model_path: Union[str, Path], # Path to the model file
    image_list: List[Union[str, Path]], # List of image paths
    save_heatmap: bool = False, # Whether to save heatmaps
    heatmap_style: str = "side_by_side", # Heatmap style other options heatmap_only, image_only
    show_heatmap: bool = False, # Whether to show heatmaps
    output_dir: Optional[Union[str, Path]] = None, # Output directory
    save_results: bool = True, # Whether to save results
    batch_id: Optional[str] = None, # Batch ID
    device: str = "auto", # Device to use
    preprocessing_fn: Optional[Callable] = None, # Preprocessing function applied to the images before prediction
    preprocessing_kwargs: Optional[Dict[str, Any]] = None, # Preprocessing kwargs for the preprocessing function
    compress: bool = True, # Whether to compress the image (JPEG format)
    jpeg_quality: int = 95 # JPEG compression quality (0-100, higher is better)
) -> Dict[str, Any]:
    """Process a specific list of images using predict_image - perfect for HPC parallel processing."""
    validate_image_list(image_list)
    valid_images = filter_valid_images(image_list)
    ensure_valid_images_exist(valid_images)
    print_processing_info(valid_images, model_path, batch_id)
    output_dir = setup_output_directory(output_dir, batch_id)

    results, anomaly_count, failed_count = process_images_batch(
        valid_images=valid_images,
        model_path=model_path,
        save_heatmap=save_heatmap,
        heatmap_style=heatmap_style,
        output_dir=output_dir,
        device=device,
        show_heatmap=show_heatmap,
        batch_id=batch_id,
        compress=compress,
        jpeg_quality=jpeg_quality,
        preprocessing_fn=preprocessing_fn,
        preprocessing_kwargs=preprocessing_kwargs
    )

    batch_stats = calculate_batch_statistics(
        results=results,
        image_list=image_list,
        valid_images=valid_images,
        anomaly_count=anomaly_count,
        failed_count=failed_count,
        model_path=model_path,
        output_dir=output_dir,
        batch_id=batch_id
    )

    print_batch_summary(batch_stats)

    if save_results and results:
        print('Saving results to JSON, output directory:', output_dir)
        results_file = save_results_to_json(results, batch_stats, output_dir, batch_id)
        batch_stats['results_file'] = results_file

    return {
        'statistics': batch_stats,
        'results': results
    }

# %% ../../nbs/06_inference.prediction_system.ipynb 124
def _read_image_list_from_file(
    image_list_file: Union[str, Path] # text file with one image path per line
    ) -> List[str]: # list of image paths
    """Read image paths from a text file."""
    image_list_file = Path(image_list_file)

    if not image_list_file.exists():
        raise FileNotFoundError(f"Image list file not found: {image_list_file}")

    image_list = []
    with open(image_list_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):  # Skip empty lines and comments
                image_list.append(line)

    if not image_list:
        raise ValueError("No valid images found in file")

    return image_list

# %% ../../nbs/06_inference.prediction_system.ipynb 125
def predict_image_list_from_file_enhanced(
    model_path: Union[str, Path], # path to the model(.ckpt, .pt, .onnx, .xml)
    image_list_file: Union[str, Path], # text file with one image path per line
    batch_id: Optional[str] = None, # unique identifier for this batch (for parallel processing)
    output_dir: Optional[Union[str, Path]] = None, # directory to save the heatmap
    save_heatmap: bool = False, # whether to save heatmap visualizations
    heatmap_style: str = "side_by_side",  # "heatmap_only", "combined", "side_by_side", "cv2_side_by_side", "cv2_heatmap_only"
    device: str = "auto", # device to use for prediction("auto", "cpu", "cuda")
    save_results: bool = True, # whether to save JSON results
    show_heatmap: bool = False, # Whether to show heatmap
    compress: bool = True, # Whether to compress the image (JPEG format)
    jpeg_quality: int = 95, # JPEG compression quality (0-100, higher is better)
    preprocessing_fn: Optional[Callable] = None, # Preprocessing function applied to the images before prediction
    preprocessing_kwargs: Optional[Dict[str, Any]] = None # Preprocessing kwargs for the preprocessing function
    ) -> Dict[str, Any]: # dictionary with prediction results and file paths
    """Process images from text file """

    print(f"üöÄ ENHANCED PREDICT IMAGE LIST FROM FILE")
    print(f"üéØ Reading image list from: {image_list_file}")

    # Read image list from file (modular function - ONE responsibility)
    image_list = _read_image_list_from_file(image_list_file)

    print(f"üìÇ Loaded {len(image_list)} image paths")
    print(f"üé® Style: {heatmap_style}")

    # Delegate to predict_image_list (the core workhorse)
    return predict_image_list(
        model_path=model_path,
        image_list=image_list,
        save_heatmap=save_heatmap,
        heatmap_style=heatmap_style,
        output_dir=output_dir,
        save_results=save_results,
        batch_id=batch_id,
        device=device,
        show_heatmap=show_heatmap,
        compress=compress,
        jpeg_quality=jpeg_quality,
        preprocessing_fn=preprocessing_fn,
        preprocessing_kwargs=preprocessing_kwargs
    )


# %% ../../nbs/06_inference.prediction_system.ipynb 127
def get_images_(
    image_input: Union[str, Path] # Path to image or directory containing images
    ) -> List[Union[str, Path]]:
        """Helper to collect image paths from various input types"""
        if image_input is None:
            return []

        if isinstance(image_input, (str, Path)):
            image_path = Path(image_input)
            if image_path.is_dir():
                # Directory - collect all image files
                extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']
                images = []
                for ext in extensions:
                    images.extend(image_path.glob(f"*{ext}"))
                    images.extend(image_path.glob(f"*{ext.upper()}"))
                return sorted(images)
            elif image_path.is_file():
                # Single file
                return [image_path]
            else:
                print(f"‚ö†Ô∏è  Path not found: {image_path}")
                return []
        elif isinstance(image_input, list):
            # List of paths
            return [Path(p) for p in image_input if Path(p).exists()]
        else:
            return []

# %% ../../nbs/06_inference.prediction_system.ipynb 130
def create_poster_from_results_(
    poster_results,# List of inference results
    poster_idx,# Index of the poster
    image_size_in_poster,# Size of each image in the poster
    poster_rows,# Number of rows in the poster
    poster_cols,# Number of columns in the poster
    poster_title,# Title of the poster
    output_folder,# Path to save the poster
    heatmap_poster:bool=True, # Whether to include the heatmap in the poster
    image_poster:bool=False, # Whether to include the image in the poster
    anomaly_poster:bool=False, # Whether to include the anomaly map in the poster
    show_plot:bool=False, # Whether to show the plot
    ):
    """Create a single poster from inference results"""

    Path(output_folder).mkdir(parents=True, exist_ok=True)

    # Calculate figure size (each cell gets image_size_in_poster space)
    cell_width = image_size_in_poster[0] / 100  # Convert to inches (approx)
    cell_height = image_size_in_poster[1] / 100
    fig_width = poster_cols * cell_width * 1.2  # Add spacing
    fig_height = poster_rows * cell_height * 1.5  # Increased spacing for title

    fig, axes = plt.subplots(poster_rows, poster_cols,
                            figsize=(fig_width, fig_height))

    # Handle single row/col case
    if poster_rows == 1 and poster_cols == 1:
        axes = [[axes]]
    elif poster_rows == 1:
        axes = [axes]
    elif poster_cols == 1:
        axes = [[ax] for ax in axes]

    # Set title with more space
    if poster_title:
        title = f"{poster_title} - Poster {poster_idx + 1}"
    else:
        title = f"Anomaly Detection Results - Poster {poster_idx + 1}"

    fig.suptitle(title, fontsize=12, y=0.98, weight='bold')

    # Fill poster grid
    for row in range(poster_rows):
        for col in range(poster_cols):
            ax = axes[row][col]
            result_idx = row * poster_cols + col

            if result_idx < len(poster_results):
                result = poster_results[result_idx]
                img_path = Path(result['image_path'])

                try:
                    # Load and resize image
                    if image_poster:
                        image = Image.open(img_path)
                        image = image.convert('RGB')
                        image = image.resize(image_size_in_poster, Image.Resampling.LANCZOS)
                    elif heatmap_poster:
                        image = result['heatmap']
                    elif anomaly_poster:
                        image = result['anomaly_map']

                    # Display image
                    ax.imshow(np.array(image))

                    # Create title with prediction info
                    prediction = result['prediction']
                    score = result['anomaly_score']

                    # Color based on prediction
                    color = 'red' if prediction == 'ANOMALY' else 'green'

                    # Create detailed title
                    img_title = f"{img_path.stem}{score:.2f}"
                    ax.set_title(img_title, fontsize=8, color=color, weight='bold', pad=8)

                    # Add border color based on prediction
                    for spine in ax.spines.values():
                        spine.set_edgecolor(color)
                        spine.set_linewidth(2)

                except Exception as e:
                    # Handle error by showing error message
                    ax.text(0.5, 0.5, f"Error loading\\n{img_path.name}",
                            ha='center', va='center', transform=ax.transAxes,
                            fontsize=8, color='red')
                    ax.set_title(f"Error: {img_path.stem}", fontsize=8, color='red', pad=8)

            else:
                # Empty cell
                ax.text(0.5, 0.5, 'Empty', ha='center', va='center',
                        transform=ax.transAxes, fontsize=10, color='gray')
                ax.set_title('', fontsize=8, pad=8)

            ax.set_xticks([])
            ax.set_yticks([])

    # Adjust layout with more top space for title
    plt.tight_layout()
    plt.subplots_adjust(top=0.85, hspace=0.4, wspace=0.2)

    # Save poster
    poster_filename = f"inference_poster_{poster_idx + 1:0d}.png"
    if output_folder:
        poster_path = output_folder / poster_filename
        plt.savefig(poster_path, dpi=150, bbox_inches='tight',
                    facecolor='white', edgecolor='none')

        if not show_plot:
            plt.close()

        return str(poster_path)
    else:
        if not show_plot:
            plt.close()
        return None

# %% ../../nbs/06_inference.prediction_system.ipynb 132
def run_inference_batch(

    image_list:List[Union[str, Path]],  # list of image paths
    dataset_name:str,# name of the dataset (#train, #test, #validation)
    model_path:Union[str, Path], # path to the model inlcuding extension
    save_heatmap:bool=False, # whether to save the heatmap
    show_heatmap:bool=False, # whether to show the heatmap
    device:str='cpu', # device to run the inference on
    results:Dict[str, Any]={} # dictionary to store results
    ):
    """Run inference on a batch of images"""
    batch_results = []

    print(f"üîç Processing {len(image_list)} {dataset_name} images...")

    for i, img_path in enumerate(image_list, 1):
        if i % 50 == 0:
            print(f"   Processed {i}/{len(image_list)} images...")

        try:
            # Run inference
            result = predict_image(
                model_path=model_path,
                image_path=img_path,
                save_heatmap=False,  # We'll handle visualization ourselves
                show_heatmap=False,
                device=device
            )

            # Add dataset info
            result['dataset'] = dataset_name
            result['image_index'] = i - 1

            batch_results.append(result)

            # Update statistics
            if result.get('is_anomaly', False):
                results['statistics']['anomaly_count'] += 1
            else:
                results['statistics']['normal_count'] += 1

        except Exception as e:
            print(f"‚ùå Failed to process {img_path.name}: {e}")
            continue

    return batch_results, results

# %% ../../nbs/06_inference.prediction_system.ipynb 133
def create_inference_poster_(
    model_path: Union[str, Path], # Path to trained model (pt, ckpt, xml, etc.)
    validation_images: Optional[Union[str, Path, List[Union[str, Path]]]] = None, # Path to validation images folder or list of image paths
    test_images: Optional[Union[str, Path, List[Union[str, Path]]]] = None, # Path to test images folder or list of image paths
    output_folder: Union[str, Path] = "inference_results", # Folder to save results and posters
    poster_rows: int = 4, # Number of rows in the poster grid
    poster_cols: int = 4, # Number of columns in the poster grid
    max_images_per_poster: Optional[int] = None, # Maximum images per poster (if None, uses rows*cols)
    include_heatmap_poster: bool = True, # Whether to include anomaly heatmaps in poster
    include_anomaly_poster: bool = True, # Whether to include anomaly heatmaps in poster
    include_image_poster: bool = True, # Whether to include anomaly heatmaps in poster
    heatmap_style: str = "side_by_side",  # "heatmap_only", "combined", "side_by_side"
    image_size_in_poster: Tuple[int, int] = (256, 256), # Size to resize images in the poster
    poster_title: Optional[str] = None, # Title for the poster (auto-generated if None)
    device: str = "auto", # Device for inference ("auto", "cpu", "cuda")
    show_plot:bool=False,# Whether to show the plot
    ) -> Dict[str, Any]:
    'Perform inference on validation and test images and create poster-style combined images.'

    # Setup paths
    model_path = Path(model_path)
    output_folder = Path(output_folder)
    output_folder.mkdir(parents=True, exist_ok=True)

    # Validate model exists
    if not model_path.exists():
        raise FileNotFoundError(f"Model not found: {model_path}")

    # Auto-detect device
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"

    print(f"üöÄ Starting inference and poster creation")
    print(f"   Model: {model_path.name}")
    print(f"   Device: {device}")
    print(f"   Output: {output_folder}")
    print(f"   Poster grid: {poster_rows}x{poster_cols}")

    # Collect image paths
    # Collect validation and test images
    val_images = get_images_(validation_images)
    test_images = get_images_(test_images)

    print(f"üìä Found {len(val_images)} validation images")
    print(f"üìä Found {len(test_images)} test images")

    if not val_images and not test_images:
        raise ValueError("No validation or test images provided!")

    # Setup max images per poster
    if max_images_per_poster is None:
        max_images_per_poster = poster_rows * poster_cols


    results = {
        'model_path': str(model_path),
        'validation_results': [],
        'test_results': [],
        'posters': [],
        'statistics': {
            'total_images': len(val_images) + len(test_images),
            'validation_count': len(val_images),
            'test_count': len(test_images),
            'anomaly_count': 0,
            'normal_count': 0
        }
    }
    if val_images:
        results['validation_results'], results = run_inference_batch(
            val_images,
            'validation',
            model_path,
            save_heatmap=False,
            show_heatmap=False,
            results=results)
    if test_images:
        results['test_results'], results = run_inference_batch(
            test_images,
            'test',
            model_path,
            save_heatmap=False,
            show_heatmap=False,
            results=results)


    all_results = results['validation_results'] + results['test_results']
    if not all_results:
        print("‚ùå No successful inference results to create posters")
        return results

    print(f"‚úÖ Inference completed: {len(all_results)} successful predictions")
    print(f"   Normal: {results['statistics']['normal_count']}")
    print(f"   Anomaly: {results['statistics']['anomaly_count']}")

    # Create multiple posters if needed
    print(f"üé® Creating posters...")
    poster_paths = []

    for i in range(0, len(all_results), max_images_per_poster):
        poster_batch = all_results[i:i + max_images_per_poster]
        poster_path = create_poster_from_results_(
            poster_results=poster_batch,
            poster_idx=len(poster_paths),
            image_size_in_poster=image_size_in_poster,
            poster_rows=poster_rows,
            poster_cols=poster_cols,
            poster_title=poster_title,
            image_poster=include_image_poster,
            heatmap_poster=include_heatmap_poster,
            anomaly_poster=include_anomaly_poster,
            output_folder=output_folder,
            show_plot=show_plot
            )
        poster_paths.append(poster_path)
        print(f"   Created poster {len(poster_paths)}: {Path(poster_path).name}")

    results['posters'] = poster_paths

    # Save detailed results to JSON
    results_file = output_folder / "inference_results.json"

    # Remove numpy arrays from results to make them JSON serializable
    for result in results['validation_results']:
        result.pop('heatmap', None)
        result.pop('anomaly_map', None)

    for result in results['test_results']:
        result.pop('heatmap', None)
        result.pop('anomaly_map', None)

    # Prepare JSON-serializable results
    json_results = {
        'model_path': results['model_path'],
        'statistics': results['statistics'],
        'posters': results['posters'],
        'validation_results': results['validation_results'],
        'test_results': results['test_results'],
        'settings': {
            'poster_rows': poster_rows,
            'poster_cols': poster_cols,
            'max_images_per_poster': max_images_per_poster,
            'image_size_in_poster': image_size_in_poster,
            'poster_title': poster_title,
            'include_heatmap_poster': include_heatmap_poster,
            'include_anomaly_poster': include_anomaly_poster,
            'include_image_poster': include_image_poster,
            'device': device
        }
    }

    import json
    with open(results_file, 'w') as f:
        json.dump(json_results, f, indent=2)

    print(f"‚úÖ Inference and poster creation completed!")
    print(f"   üìä Total images processed: {results['statistics']['total_images']}")
    print(f"   üñºÔ∏è  Posters created: {len(poster_paths)}")
    print(f"   üìÅ Results saved to: {output_folder}")
    print(f"   üìÑ Detailed results: {results_file}")

    return results




# %% ../../nbs/06_inference.prediction_system.ipynb 135
def _adjust_poster_dimensions(
    num_images: int,  # Number of images to display
    default_rows: int,  # Default number of rows
    default_cols: int  # Default number of columns
    ) -> Tuple[int, int]: # Tuple of (rows, cols) adjusted for the number of images
    """
    Adjust poster dimensions based on number of images to avoid issues with small datasets.

    """
    if num_images <= 0:
        return 1, 1

    # If we have very few images, use smaller grid
    if num_images <= 4:
        if num_images == 1:
            return 1, 1
        elif num_images == 2:
            return 1, 2
        elif num_images <= 4:
            return 2, 2

    # If we have less than default grid size, find optimal dimensions
    default_capacity = default_rows * default_cols
    if num_images < default_capacity:
        # Find square-ish dimensions that can accommodate all images
        import math
        sqrt_n = math.sqrt(num_images)
        rows = int(math.ceil(sqrt_n))
        cols = int(math.ceil(num_images / rows))

        # Ensure we don't exceed the default if it's reasonable
        if rows <= default_rows and cols <= default_cols:
            return rows, cols

    # Use default dimensions if we have enough images or if calculated dimensions are too large
    return default_rows, default_cols

# %% ../../nbs/06_inference.prediction_system.ipynb 136
def train_model_and_create_posters(
    data_root: Union[str, Path],  # Root directory containing normal and abnormal subdirectories
    normal_dir: str = "good",  # Name of normal images subdirectory
    abnormal_dir: str = "bad",  # Name of abnormal images subdirectory
    model_name: str = "padim",  # Model type to train
    backbone: str = "resnet18",  # Backbone architecture
    layers: List[str] = None,  # Feature extraction layers
    n_features: int = 100,  # Number of features for models that support it
    max_epochs: int = 1,  # Maximum training epochs
    class_name: str = "anomaly_detection",  # Class name for the anomaly detection task

    # Poster creation settings
    create_good_poster: bool = False,  # Whether to create poster from good images
    good_images_percentage: float = 0.1,  # Percentage of good images to use (10% default)
    poster_rows: int = 4,  # Number of rows in poster grid
    poster_cols: int = 4,  # Number of columns in poster grid

    # Output settings
    output_folder: Optional[Union[str, Path]] = None,  # Output folder for results
    save_path: Union[str, Path] = "./models",  # Path to save trained model

    # Training settings
    train_batch_size: Optional[int] = None,  # Auto-detected if None
    eval_batch_size: Optional[int] = None,   # Auto-detected if None
    num_workers: Optional[int] = None,       # Auto-detected if None
    device: str = "auto",  # Device for training and inference

    # Advanced settings
    image_size: Tuple[int, int] = (256, 256),  # Image size for training
    seed: Optional[int] = None,  # Random seed for reproducibility
    enable_tiling: bool = False,  # Enable tiling for large images
    export_formats: List[str] = ['torch'],  # Export formats

) -> Dict[str, Any]:
    """
    Train an anomaly detection model and automatically create posters from results.

    This comprehensive function:
    1. Trains an anomaly detection model using the provided data
    2. Creates posters from all bad (abnormal) images showing predictions
    3. Optionally creates posters from a sample of good (normal) images
    4. Generates informative poster titles with model details and anomaly statistics

    Returns:
        Dictionary containing:
        - training_results: Complete training results including model paths
        - bad_poster_results: Results from bad images poster creation
        - good_poster_results: Results from good images poster (if created)
        - model_stats: Model statistics including thresholds and anomaly ranges
        - poster_info: Information about created posters including titles

    Example:
        >>> results = train_model_and_create_posters(
        ...     data_root="/path/to/data",
        ...     normal_dir="good",
        ...     abnormal_dir="defective",
        ...     model_name="padim",
        ...     backbone="resnet18",
        ...     layers=["layer1", "layer2", "layer3"],
        ...     class_name="surface_defects",
        ...     create_good_poster=True,
        ...     good_images_percentage=0.15  # Use 15% of good images
        ... )
        >>> print(f"Model trained: {results['training_results']['success']}")
        >>> print(f"Bad poster: {results['bad_poster_results']['posters'][0]}")
        >>> print(f"Poster title: {results['poster_info']['bad_poster_title']}")
    """

    print("üöÄ Starting comprehensive training and poster creation workflow")
    print("="*80)

    # Set up paths
    data_root = Path(data_root)
    save_path = Path(save_path)

    if not data_root.exists():
        raise FileNotFoundError(f"Data root directory not found: {data_root}")

    normal_path = data_root / normal_dir
    abnormal_path = data_root / abnormal_dir

    if not normal_path.exists():
        raise FileNotFoundError(f"Normal images directory not found: {normal_path}")
    if not abnormal_path.exists():
        raise FileNotFoundError(f"Abnormal images directory not found: {abnormal_path}")

    # Set default layers if not provided
    if layers is None:
        if model_name.lower() in ["padim", "stfpm"]:
            layers = ["layer1", "layer2", "layer3"]
        else:
            layers = ["layer3"]  # Default for other models

    # Set up output folder
    if output_folder is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_folder = f"training_posters_{class_name}_{model_name}_{timestamp}"
    output_folder = Path(output_folder)
    output_folder.mkdir(parents=True, exist_ok=True)

    print(f"üìÅ Data root: {data_root}")
    print(f"üìÅ Normal images: {normal_path}")
    print(f"üìÅ Abnormal images: {abnormal_path}")
    print(f"üìÅ Output folder: {output_folder}")
    print(f"ü§ñ Model: {model_name} with {backbone} backbone")
    print(f"üîß Layers: {layers}")
    print(f"üîß Features: {n_features}")

    # Step 1: Train the model
    print("\\n" + "="*80)
    print("üéØ STEP 1: Training Anomaly Detection Model")
    print("="*80)

    # Import training function
    from be_vision_ad_tools.training.flexible_trainer import (
        FlexibleTrainingConfig, train_anomaly_model, ExportType
    )

    # Create training configuration
    training_config = FlexibleTrainingConfig(
        data_root=data_root,
        normal_dir=normal_dir,
        abnormal_dir=abnormal_dir,
        class_name=class_name,
        model_name=model_name,
        backbone=backbone,
        layers=layers,
        n_features=n_features,
        max_epochs=max_epochs,
        image_size=image_size,
        train_batch_size=train_batch_size,
        eval_batch_size=eval_batch_size,
        num_workers=num_workers,
        save_path=save_path,
        seed=seed,
        enable_tiling=enable_tiling,
        export_formats=[ExportType(fmt) for fmt in export_formats],
    )

    # Train the model
    training_results = train_anomaly_model(training_config)

    if not training_results.get('success', False):
        error_msg = f"Training failed: {training_results.get('error', 'Unknown error')}"
        print(f"‚ùå {error_msg}")
        return {
            'success': False,
            'error': error_msg,
            'training_results': training_results
        }

    print(f"‚úÖ Training completed successfully!")
    print(f"üìä Model threshold: {training_results.get('image_threshold', 'N/A')}")
    print(f"üìä Anomaly score range: {training_results.get('pred_score_min', 'N/A'):.4f} - {training_results.get('pred_score_max', 'N/A'):.4f}")

    # Get model path for inference
    model_path = None
    export_paths = training_results.get('export_paths', {})
    if 'torch' in export_paths:
        model_path = export_paths['torch']
    elif training_results.get('best_model_path'):
        model_path = training_results['best_model_path']
    else:
        raise RuntimeError("No valid model path found in training results")

    print(f"üéØ Using model: {model_path}")

    # Extract model statistics for poster titles
    model_stats = {
        'model_name': model_name,
        'backbone': backbone,
        'layers': layers,
        'n_features': n_features,
        'image_threshold': training_results.get('image_threshold', 0.0),
        'pred_score_min': training_results.get('pred_score_min', 0.0),
        'pred_score_max': training_results.get('pred_score_max', 1.0),
        'anomaly_map_min': training_results.get('anomaly_map_min', 0.0),
        'anomaly_map_max': training_results.get('anomaly_map_max', 1.0),
    }

    # Step 2: Create poster from bad (abnormal) images
    print("\\n" + "="*80)
    print("üéØ STEP 2: Creating Poster from Bad Images")
    print("="*80)

    # Get all bad images to determine optimal poster dimensions
    bad_images = get_images_(abnormal_path)
    num_bad_images = len(bad_images)
    print(f"üìä Found {num_bad_images} bad images")

    # Adjust poster dimensions based on number of images
    bad_poster_rows, bad_poster_cols = _adjust_poster_dimensions(num_bad_images, poster_rows, poster_cols)
    print(f"üìê Adjusted poster dimensions for bad images: {bad_poster_rows}x{bad_poster_cols}")

    # Create poster title with model and anomaly information
    layers_str = "+".join(layers) if isinstance(layers, list) else str(layers)
    bad_poster_title = (
        f"{model_name.upper()} {backbone.upper()} L:{layers_str} F:{n_features} "
        f"T:{model_stats['image_threshold']:.3f} "
        f"A:{model_stats['pred_score_min']:.3f}-{model_stats['pred_score_max']:.3f}"
    )

    print(f"üìù Poster title: {bad_poster_title}")

    # Create poster from all bad images
    bad_poster_output = output_folder / "bad_images_poster"
    bad_poster_results = create_inference_poster_(
        model_path=model_path,
        validation_images=abnormal_path,  # Use bad images for validation
        test_images=None,
        output_folder=bad_poster_output,
        poster_rows=bad_poster_rows,
        poster_cols=bad_poster_cols,
        poster_title=bad_poster_title,
        device=device,
        include_heatmap_poster=True,  # Show heatmaps for bad images
        include_image_poster=False,    # Also show original images
        include_anomaly_poster=False, # Skip separate anomaly poster
    )

    print(f"‚úÖ Bad images poster created: {len(bad_poster_results['posters'])} poster(s)")

    # Step 3: Optionally create poster from good images sample
    good_poster_results = None
    good_poster_title = None

    if create_good_poster:
        print("\\n" + "="*80)
        print("üéØ STEP 3: Creating Poster from Good Images Sample")
        print("="*80)

        # Get all good images
        good_images = get_images_(normal_path)
        print(f"üìä Found {len(good_images)} good images")

        # Sample percentage of good images
        sample_size = max(1, int(len(good_images) * good_images_percentage))
        sample_size = min(sample_size, poster_rows * poster_cols * 5)  # Limit to reasonable amount

        if seed is not None:
            random.seed(seed)
        sampled_good_images = random.sample(good_images, sample_size)

        print(f"üìä Sampling {sample_size} good images ({good_images_percentage*100:.1f}%)")

        # Adjust poster dimensions based on number of sampled good images
        good_poster_rows, good_poster_cols = _adjust_poster_dimensions(len(sampled_good_images), poster_rows, poster_cols)
        print(f"üìê Adjusted poster dimensions for good images: {good_poster_rows}x{good_poster_cols}")

        # Create poster title for good images
        good_poster_title = (
            f"{model_name.upper()} {backbone.upper()} L:{layers_str} F:{n_features} "
            f"GOOD-{len(sampled_good_images)} "
            f"T:{model_stats['image_threshold']:.3f} "
            f"A:{model_stats['pred_score_min']:.3f}-{model_stats['pred_score_max']:.3f}"
        )

        print(f"üìù Good poster title: {good_poster_title}")

        # Create poster from sampled good images
        good_poster_output = output_folder / "good_images_poster"
        good_poster_results = create_inference_poster_(
            model_path=model_path,
            validation_images=sampled_good_images,
            test_images=None,
            output_folder=good_poster_output,
            poster_rows=good_poster_rows,
            poster_cols=good_poster_cols,
            poster_title=good_poster_title,
            device=device,
            include_heatmap_poster=True,  # Show heatmaps to verify they're low
            include_image_poster=True,    # Show original images
            include_anomaly_poster=False,
        )

        print(f"‚úÖ Good images poster created: {len(good_poster_results['posters'])} poster(s)")

    # Step 4: Compile final results
    print("\\n" + "="*80)
    print("üéØ STEP 4: Compiling Results")
    print("="*80)

    final_results = {
        'success': True,
        'training_results': training_results,
        'bad_poster_results': bad_poster_results,
        'good_poster_results': good_poster_results,
        'model_stats': model_stats,
        'poster_info': {
            'bad_poster_title': bad_poster_title,
            'good_poster_title': good_poster_title,
            'output_folder': str(output_folder),
            'model_path': str(model_path),
        },
        'settings': {
            'data_root': str(data_root),
            'normal_dir': normal_dir,
            'abnormal_dir': abnormal_dir,
            'class_name': class_name,
            'model_name': model_name,
            'backbone': backbone,
            'layers': layers,
            'n_features': n_features,
            'max_epochs': max_epochs,
            'poster_rows': poster_rows,
            'poster_cols': poster_cols,
            'create_good_poster': create_good_poster,
            'good_images_percentage': good_images_percentage,
        }
    }

    # Save summary results to JSON
    summary_file = output_folder / "training_and_poster_summary.json"

    # Create JSON-serializable version
    json_results = final_results.copy()
    # Remove non-serializable items from training results if needed
    if 'training_results' in json_results:
        training_copy = json_results['training_results'].copy()
        # Keep only serializable fields
        json_results['training_results'] = {
            k: v for k, v in training_copy.items()
            if isinstance(v, (str, int, float, bool, list, dict)) or v is None
        }

    # Remove heatmap/image arrays from poster results for JSON
    for poster_key in ['bad_poster_results', 'good_poster_results']:
        if json_results[poster_key] is not None:
            poster_copy = json_results[poster_key].copy()
            # Remove arrays from validation/test results
            for result_key in ['validation_results', 'test_results']:
                if result_key in poster_copy:
                    for result in poster_copy[result_key]:
                        result.pop('heatmap', None)
                        result.pop('anomaly_map', None)
            json_results[poster_key] = poster_copy

    def convert_paths_to_strings(obj):
        """Recursively convert PosixPath objects to strings for JSON serialization"""
        if isinstance(obj, Path):
            return str(obj)
        elif isinstance(obj, dict):
            return {k: convert_paths_to_strings(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_paths_to_strings(item) for item in obj]
        return obj

    # Convert all paths to strings for JSON serialization
    json_serializable_results = convert_paths_to_strings(
        json_results)

    with open(str(summary_file), 'w') as f:
        json.dump(json_serializable_results, f, indent=2)

    print(f"üíæ Summary saved: {summary_file}")

    # Print final summary
    print("\\n" + "="*80)
    print("üéâ WORKFLOW COMPLETED SUCCESSFULLY!")
    print("="*80)
    print(f"‚úÖ Model trained: {training_results['success']}")
    print(f"‚úÖ Bad images poster: {len(bad_poster_results['posters'])} files")
    if good_poster_results:
        print(f"‚úÖ Good images poster: {len(good_poster_results['posters'])} files")
    print(f"üìÅ All results in: {output_folder}")
    print(f"üéØ Model file: {model_path}")
    print(f"üìä Bad images processed: {bad_poster_results['statistics']['total_images']}")
    print(f"üìä Anomalies detected: {bad_poster_results['statistics']['anomaly_count']}")
    print(f"üìä Normal predictions: {bad_poster_results['statistics']['normal_count']}")
    if good_poster_results:
        print(f"üìä Good images processed: {good_poster_results['statistics']['total_images']}")
    print(f"üìù Bad poster title: {bad_poster_title}")
    if good_poster_title:
        print(f"üìù Good poster title: {good_poster_title}")

    print("\\nüöÄ Ready for analysis and deployment!")

    return final_results



# %% ../../nbs/06_inference.prediction_system.ipynb 147
def save_heatmap_visualization(
    image_path: Union[str, Path], # Path to the image file
    pred_result: ImageVisualizer, # Prediction result from anomaly detection
    anomaly_score: float, # Computed anomaly score
    prediction: str, # Prediction label (normal/abnormal)
    style: str = "combined",  # Visualization style option
    output_dir: Optional[Union[str, Path]] = None, # Directory to save output
    colormap: str = "jet", # Matplotlib colormap name
    dpi: int = 150, # Image resolution for output
    ) -> List[str]: # List of saved file paths
    """Save heatmap visualizations in different styles."""
    try:
        image_path = Path(image_path)

        # Validate input parameters
        if not image_path.exists():
            raise FileNotFoundError(f"Image file not found: {image_path}")

        valid_styles = {"heatmap_only", "combined", "side_by_side"}
        if style not in valid_styles:
            raise ValueError(f"Style must be one of {valid_styles}, got: {style}")

        # Set output directory
        if output_dir is None:
            output_dir = image_path.parent
        else:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

        # Generate base filename
        base_name = image_path.stem
        saved_files = []

        # Extract visualization data
        original_image = pred_result.image
        original_array = np.array(original_image)
        anomaly_map = pred_result.anomaly_map
        heatmap = pred_result.heat_map

        if style == "heatmap_only":
            output_path = _save_heatmap_only(
                anomaly_map, anomaly_score, prediction,
                output_dir / f"{base_name}_heatmap.png",
                colormap, dpi
            )
            saved_files.append(str(output_path))

        elif style == "combined":
            output_path = _save_combined_heatmap(
                heatmap, anomaly_score, prediction, image_path.name,
                output_dir / f"{base_name}_combined.png",
                colormap, dpi
            )
            saved_files.append(str(output_path))

        elif style == "side_by_side":
            output_path = _save_sidebyside_heatmap(
                original_array, heatmap, anomaly_score, prediction, image_path.name,
                output_dir / f"{base_name}_sidebyside.png",
                colormap, dpi
            )
            saved_files.append(str(output_path))

        print(f"üíæ Saved {style} visualization: {saved_files[-1]}")
        return saved_files

    except Exception as e:
        logging.error(f"Failed to save heatmap visualization: {e}")
        raise



# %% ../../nbs/06_inference.prediction_system.ipynb 151
def batch_predict(model_path: Union[str, Path],
                 image_folder: Union[str, Path],
                 save_heatmaps: bool = False,
                 heatmap_style: str = "combined",
                 output_dir: Optional[Union[str, Path]] = None,
                 save_results: bool = True,
                 image_extensions: List[str] = None) -> Dict[str, Any]:
    """
    Batch prediction on folder of images with automatic model detection.

    Args:
        model_path: Path to model file
        image_folder: Folder containing images
        save_heatmaps: Whether to save heatmap visualizations
        heatmap_style: Style for heatmaps
        output_dir: Output directory for results
        save_results: Whether to save JSON results
        image_extensions: List of image extensions to process

    Returns:
        Dictionary with batch results and statistics
    """
    model_path = Path(model_path)
    image_folder = Path(image_folder)

    if not model_path.exists():
        raise FileNotFoundError(f"Model not found: {model_path}")
    if not image_folder.exists():
        raise FileNotFoundError(f"Image folder not found: {image_folder}")

    # Default image extensions
    if image_extensions is None:
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']

    # Find all images
    image_files = []
    for ext in image_extensions:
        image_files.extend(image_folder.glob(f"*{ext}"))
        image_files.extend(image_folder.glob(f"*{ext.upper()}"))

    if not image_files:
        raise ValueError(f"No images found in {image_folder}")

    print(f"üîç Found {len(image_files)} images to process")
    print(f"üì¶ Using model: {model_path.name}")

    # Set output directory
    if output_dir is None:
        output_dir = image_folder / "predictions"
    else:
        output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Process images
    results = []
    anomaly_count = 0
    failed_count = 0

    for i, image_path in enumerate(image_files, 1):
        print(f"Processing {i}/{len(image_files)}: {image_path.name}")

        try:
            result = predict_image(
                model_path=model_path,
                image_path=image_path,
                save_heatmap=save_heatmaps,
                heatmap_style=heatmap_style,
                output_dir=output_dir if save_heatmaps else None
            )

            results.append(result)
            if result['is_anomaly']:
                anomaly_count += 1

        except Exception as e:
            print(f"‚ùå Failed to process {image_path.name}: {str(e)}")
            failed_count += 1
            continue

    # Calculate statistics
    total_processed = len(results)
    normal_count = total_processed - anomaly_count

    batch_stats = {
        'total_images_found': len(image_files),
        'successfully_processed': total_processed,
        'failed_processing': failed_count,
        'normal_count': normal_count,
        'anomaly_count': anomaly_count,
        'anomaly_percentage': (anomaly_count / total_processed * 100) if total_processed > 0 else 0,
        'average_anomaly_score': np.mean([r['anomaly_score'] for r in results]) if results else 0,
        'model_used': str(model_path),
        'processed_folder': str(image_folder),
        'output_directory': str(output_dir)
    }

    # Print summary
    print(f"\\nüìä Batch Processing Summary:")
    print(f"   Total Images: {batch_stats['total_images_found']}")
    print(f"   Successfully Processed: {batch_stats['successfully_processed']}")
    print(f"   Failed: {batch_stats['failed_processing']}")
    print(f"   Normal: {batch_stats['normal_count']} ({100-batch_stats['anomaly_percentage']:.1f}%)")
    print(f"   Anomalies: {batch_stats['anomaly_count']} ({batch_stats['anomaly_percentage']:.1f}%)")
    print(f"   Average Score: {batch_stats['average_anomaly_score']:.4f}")

    # Save results to JSON
    if save_results and results:
        results_file = output_dir / "batch_prediction_results.json"
        full_results = {
            'statistics': batch_stats,
            'individual_results': results
        }

        with open(results_file, 'w') as f:
            json.dump(full_results, f, indent=2)

        print(f"üíæ Results saved to: {results_file}")
        batch_stats['results_file'] = str(results_file)

    return {
        'statistics': batch_stats,
        'results': results
    }


# %% ../../nbs/06_inference.prediction_system.ipynb 157
def split_image_list(image_list: List[Union[str, Path]],
                    num_batches: int,
                    batch_strategy: str = "round_robin") -> List[List[Path]]:
    """
    Split a large image list into smaller batches for parallel processing.

    Args:
        image_list: List of image paths
        num_batches: Number of batches to create
        batch_strategy: "round_robin" (balanced) or "chunk" (consecutive)

    Returns:
        List of image path lists, one for each batch
    """
    if not image_list:
        return []

    if num_batches <= 0:
        raise ValueError("Number of batches must be positive")

    # Convert to Path objects
    paths = [Path(img) for img in image_list]

    if num_batches >= len(paths):
        # More batches than images - one image per batch
        return [[path] for path in paths]

    batches = [[] for _ in range(num_batches)]

    if batch_strategy == "round_robin":
        # Distribute images evenly across batches (balanced)
        for i, path in enumerate(paths):
            batch_idx = i % num_batches
            batches[batch_idx].append(path)

    elif batch_strategy == "chunk":
        # Split into consecutive chunks
        chunk_size = len(paths) // num_batches
        remainder = len(paths) % num_batches

        start_idx = 0
        for i in range(num_batches):
            # Add one extra image to first 'remainder' batches
            current_chunk_size = chunk_size + (1 if i < remainder else 0)
            end_idx = start_idx + current_chunk_size
            batches[i] = paths[start_idx:end_idx]
            start_idx = end_idx

    else:
        raise ValueError(f"Unknown batch strategy: {batch_strategy}")

    # Remove empty batches
    batches = [batch for batch in batches if batch]

    print(f"üì¶ Split {len(paths)} images into {len(batches)} batches:")
    for i, batch in enumerate(batches):
        print(f"   Batch {i+1}: {len(batch)} images")

    return batches


def generate_hpc_commands(model_path: Union[str, Path],
                         image_batches: List[List[Path]],
                         script_template: str = "python",
                         output_base_dir: str = "hpc_results",
                         save_heatmaps: bool = False,
                         heatmap_style: str = "combined",
                         additional_args: str = "") -> List[str]:
    """
    Generate HPC command lines for parallel execution across nodes.

    Args:
        model_path: Path to model file
        image_batches: List of image path lists (from split_image_list)
        script_template: Command template ("python", "sbatch", etc.)
        output_base_dir: Base directory for outputs
        save_heatmaps: Whether to save heatmaps
        heatmap_style: Heatmap style
        additional_args: Additional command line arguments

    Returns:
        List of command strings ready for HPC execution
    """
    model_path = Path(model_path)
    commands = []

    for i, batch in enumerate(image_batches):
        batch_id = f"hpc_batch_{i+1:04d}"

        # Create image list file for this batch
        batch_list_file = f"{output_base_dir}/batch_lists/{batch_id}_images.txt"

        # Ensure directory exists
        Path(batch_list_file).parent.mkdir(parents=True, exist_ok=True)

        # Write image paths to file
        with open(batch_list_file, 'w') as f:
            for img_path in batch:
                f.write(f"{img_path}\\n")

        # Generate command
        cmd_parts = [
            script_template,
            "-c",
            f"\"",
            f"from be_vision_ad_tools.inference.prediction_system import predict_image_list_from_file;",
            f"predict_image_list_from_file(",
            f"  model_path='{model_path}',",
            f"  image_list_file='{batch_list_file}',",
            f"  batch_id='{batch_id}',",
            f"  output_dir='{output_base_dir}',",
            f"  save_heatmaps={save_heatmaps},",
            f"  heatmap_style='{heatmap_style}'",
            f")\"",
            additional_args
        ]

        command = " ".join(cmd_parts)
        commands.append(command)

    print(f"üöÄ Generated {len(commands)} HPC commands")
    print(f"üíæ Batch list files saved in: {output_base_dir}/batch_lists/")

    return commands


# %% ../../nbs/06_inference.prediction_system.ipynb 158
def predict_image_list_from_file(
    model_path: Union[str, Path], # path to the model(.ckpt, .pt, .onnx, .xml)
    image_list_file: Union[str, Path], # text file with one image path per line
    batch_id: Optional[str] = None, # unique identifier for this batch
    output_dir: Optional[Union[str, Path]] = None, # output directory for results
    save_heatmaps: bool = False, # whether to save the heatmap
    heatmap_style: str = "combined", # "heatmap_only", "combined", "side_by_side"
    device: str = "auto" # "cpu", "cuda", "mps", "auto"
    ) -> Dict[str, Any]: # dictionary with batch results and statistics
    """
    Process images from a text file list - perfect for HPC command-line execution.

    """
    image_list_file = Path(image_list_file)

    if not image_list_file.exists():
        raise FileNotFoundError(f"Image list file not found: {image_list_file}")

    # Read image paths from file
    image_list = []
    with open(image_list_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):  # Skip empty lines and comments
                image_list.append(line)

    print(f"üìÇ Loaded {len(image_list)} image paths from {image_list_file}")

    # Use the main predict_image_list function
    return predict_image_list(
        model_path=model_path,
        image_list=image_list,
        save_heatmaps=save_heatmaps,
        heatmap_style=heatmap_style,
        output_dir=output_dir,
        batch_id=batch_id,
        device=device
    )


# %% ../../nbs/06_inference.prediction_system.ipynb 159
def merge_batch_results(results_dir: Union[str, Path],
                       output_file: Optional[Union[str, Path]] = None) -> Dict[str, Any]:
    """
    Merge results from multiple HPC batches into a single comprehensive report.

    Args:
        results_dir: Directory containing batch result JSON files
        output_file: Output file for merged results (optional)

    Returns:
        Dictionary with merged results and overall statistics
    """
    results_dir = Path(results_dir)

    if not results_dir.exists():
        raise FileNotFoundError(f"Results directory not found: {results_dir}")

    # Find all batch result files
    batch_files = list(results_dir.rglob("batch_results_*.json"))

    if not batch_files:
        print(f"‚ö†Ô∏è  No batch result files found in {results_dir}")
        return {}

    print(f"üîç Found {len(batch_files)} batch result files")

    # Merge all results
    merged_results = []
    merged_stats = {
        'total_batches': len(batch_files),
        'total_images_processed': 0,
        'total_images_failed': 0,
        'total_normal': 0,
        'total_anomalies': 0,
        'batch_details': []
    }

    for batch_file in sorted(batch_files):
        try:
            with open(batch_file, 'r') as f:
                batch_data = json.load(f)

            # Extract batch statistics
            batch_stats = batch_data.get('statistics', {})
            batch_results = batch_data.get('individual_results', [])

            # Add to merged results
            merged_results.extend(batch_results)

            # Update overall statistics
            merged_stats['total_images_processed'] += batch_stats.get('successfully_processed', 0)
            merged_stats['total_images_failed'] += batch_stats.get('failed_processing', 0)
            merged_stats['total_normal'] += batch_stats.get('normal_count', 0)
            merged_stats['total_anomalies'] += batch_stats.get('anomaly_count', 0)

            # Store batch details
            merged_stats['batch_details'].append({
                'batch_id': batch_stats.get('batch_id', batch_file.stem),
                'processed': batch_stats.get('successfully_processed', 0),
                'failed': batch_stats.get('failed_processing', 0),
                'anomalies': batch_stats.get('anomaly_count', 0),
                'avg_score': batch_stats.get('average_anomaly_score', 0),
                'file': str(batch_file)
            })

            print(f"   ‚úÖ Merged: {batch_file.name}")

        except Exception as e:
            print(f"   ‚ùå Failed to read {batch_file.name}: {str(e)}")
            continue

    # Calculate final statistics
    total_processed = merged_stats['total_images_processed']
    if total_processed > 0:
        merged_stats['overall_anomaly_percentage'] = (merged_stats['total_anomalies'] / total_processed) * 100
        merged_stats['overall_average_score'] = np.mean([r['anomaly_score'] for r in merged_results])
    else:
        merged_stats['overall_anomaly_percentage'] = 0
        merged_stats['overall_average_score'] = 0

    # Create final merged data
    final_results = {
        'merge_timestamp': datetime.now().isoformat(),
        'merged_statistics': merged_stats,
        'all_results': merged_results
    }

    # Print summary
    print(f"\\nüìä Merged Results Summary:")
    print(f"   Total Batches: {merged_stats['total_batches']}")
    print(f"   Total Images Processed: {merged_stats['total_images_processed']}")
    print(f"   Total Failed: {merged_stats['total_images_failed']}")
    print(f"   Total Normal: {merged_stats['total_normal']} ({100-merged_stats['overall_anomaly_percentage']:.1f}%)")
    print(f"   Total Anomalies: {merged_stats['total_anomalies']} ({merged_stats['overall_anomaly_percentage']:.1f}%)")
    print(f"   Overall Average Score: {merged_stats['overall_average_score']:.4f}")

    # Save merged results
    if output_file is None:
        output_file = results_dir / "merged_results.json"
    else:
        output_file = Path(output_file)

    with open(output_file, 'w') as f:
        json.dump(final_results, f, indent=2)

    print(f"üíæ Merged results saved to: {output_file}")

    return final_results

