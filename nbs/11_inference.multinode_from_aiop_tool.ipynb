{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference.multinode_from_aiop_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from multiprocessing import Value, Queue\n",
    "import subprocess\n",
    "import signal\n",
    "from threading import Thread\n",
    "import tqdm\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import uuid\n",
    "import re\n",
    "import select\n",
    "from functools import reduce\n",
    "from colorama import Fore, Style\n",
    "\n",
    "queue = multiprocessing.Queue()\n",
    "\n",
    "\n",
    "def print_status(current=None, total=None, description=None):\n",
    "    \"\"\"\n",
    "    This function can be used by a python script used in HPC_job to share its progress with\n",
    "    the DistributeHPC class. It will be used to update the progress bar.\n",
    "    \"\"\"\n",
    "    if total is None:\n",
    "        total = \"\"\n",
    "    if description is None:\n",
    "        description = \"\"\n",
    "    if current is None:\n",
    "        current = \"\"\n",
    "    print(f\"STATUS, {current}, {total}, {description}\")\n",
    "\n",
    "\n",
    "class HPC_Job:\n",
    "    \"\"\"\n",
    "    Represents a High-Performance Computing (HPC) job with attributes and methods \n",
    "    to manage its state, commands, and submission to an HPC system.\n",
    "\n",
    "    Attributes:\n",
    "        state (int): The current state of the job.\n",
    "        uuid (str): A unique identifier for the job.\n",
    "        command (list): The command to be executed by the job.\n",
    "        hpc_command (list): The full HPC command including submission parameters.\n",
    "        description (str): A description of the job shown in the progress bar.\n",
    "        status_current (str): Current Progress: n in n/N shown in the prorgress bar.\n",
    "        status_total (str): Current Progress: N in n/N shown in the prorgress bar.\n",
    "        lsf_job_id (int or None): The job ID assigned by the LSF scheduler.\n",
    "        lsf_job_queue (str or None): The queue to which the job is submitted.\n",
    "        bsub_error_msg (str or None): Error message from the `bsub` command, if any.\n",
    "\n",
    "    Constants:\n",
    "        JOB_NONE (int): Represents a job with no state.\n",
    "        JOB_SUBMITTED (int): Represents a job that has been submitted.\n",
    "        JOB_WAITING (int): Represents a job that is waiting to run.\n",
    "        JOB_RUNNING (int): Represents a job that is currently running.\n",
    "        JOB_COMPLETED (int): Represents a job that has completed successfully or failed.\n",
    "        JOB_BSUB_FAILED (int): Represents a job where the job submission failed.\n",
    "        JOB_TASK_FAILED (int): Represents a job where a task failed.\n",
    "\n",
    "        BSUB_ARGS_DEFAULT (dict): Default arguments for the `bsub` command.\n",
    "        BSUB_ARGS_DEFAULT_GPU (dict): Default arguments for GPU-based `bsub` commands.\n",
    "\n",
    "    \"\"\"\n",
    "    state = 0\n",
    "    uuid = \"\"\n",
    "    command = []\n",
    "    hpc_command = []\n",
    "    description = \"\"\n",
    "    status_current = \"\"\n",
    "    status_total = \"\"\n",
    "    lsf_job_id = None\n",
    "    lsf_job_queue = None\n",
    "    bsub_error_msg = None\n",
    "\n",
    "    JOB_NONE        = 0x0001\n",
    "    JOB_SUBMITTED   = 0x0002\n",
    "    JOB_WAITING     = 0x0004\n",
    "    JOB_RUNNING     = 0x0008\n",
    "    JOB_COMPLETED   = 0x0010\n",
    "    JOB_BSUB_FAILED = 0x1000\n",
    "    JOB_TASK_FAILED = 0x2000\n",
    "\n",
    "    BSUB_ARGS_DEFAULT = {\n",
    "        \"-q\": \"batch\",\n",
    "        \"-R\": \"ui=aiml_batch_training_dy && um=background && osrel>=70\",\n",
    "    }\n",
    "\n",
    "    BSUB_ARGS_DEFAULT_GPU = {\n",
    "        \"-q\": \"gpu\",\n",
    "        \"-gpu\": \"num=1:j_exclusive=yes\",\n",
    "        \"-R\": \"osrel>=70 && type=any\"        \n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self, cmd=[], cores=4, bsub_args=None):\n",
    "        \"\"\"\n",
    "        Initializes an HPC_Job instance with the specified command, number of cores, \n",
    "        and optional bsub arguments.\n",
    "\n",
    "        Args:\n",
    "            cmd (list, optional): The command to be executed on the HPC system. \n",
    "                                  e.g. [\"python\", \"script.py\", \"--arg1\", \"value1\"]\n",
    "            cores (int, optional): The number of cores to allocate for the job. \n",
    "                                   Defaults to 4.\n",
    "            bsub_args (dict, optional): Additional arguments for the `bsub` command. \n",
    "                                        If not provided, defaults to `HPC_Job.BSUB_ARGS_DEFAULT`.\n",
    "        \"\"\"\n",
    "        if bsub_args is None:\n",
    "            bsub_args = HPC_Job.BSUB_ARGS_DEFAULT.copy()\n",
    "        bsub_args[\"-n\"] = str(cores) \n",
    "\n",
    "        params = [[str(x),str(y)] for x,y in bsub_args.items()]\n",
    "        params = reduce(lambda x,y: x + y, params)\n",
    "\n",
    "        cmd = [str(x) for x in cmd]\n",
    "        hpc_command = [\"bsub\", \"-Is\"] + params + cmd\n",
    "\n",
    "        self.hpc_command = hpc_command\n",
    "        self.command = cmd\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "        self.state = HPC_Job.JOB_WAITING\n",
    "\n",
    "\n",
    "class DistributeHPC(object):\n",
    "    \"\"\"\n",
    "    DistributeHPC is a class designed to manage and execute distributed High-Performance Computing (HPC) jobs. \n",
    "    It provides functionality for job submission, progress tracking. The class is particularly useful for \n",
    "    running multiple computational tasks concurrently while monitoring their progress in real-time.\n",
    "    \n",
    "    Methods:\n",
    "        __init__(worker=10):\n",
    "            Initializes the DistributeHPC instance with the specified number of worker processes.\n",
    "        set_jobs_hpc(jobs, num_cpu=4):\n",
    "            Deprecated. Use `set_jobs` instead.\n",
    "        set_jobs(jobs, num_cpu=4):\n",
    "            Adds jobs to the job list. Jobs can either be instances of `HPC_Job` or lists of commands.\n",
    "        start():\n",
    "            Starts the distributed HPC job processing workflow. Manages job execution using a multiprocessing \n",
    "            pool and visualizes progress with progress bars.\n",
    "    \"\"\"\n",
    "\n",
    "    jobs = []\n",
    "    worker = 10\n",
    "    closed = None\n",
    "    \n",
    "    def __init__(self, worker=10):\n",
    "        self.worker = worker\n",
    "        self.closed = Value('i', 0)\n",
    "        \n",
    "    def _show_progress_bar(self):\n",
    "        self.thread = Thread(target=self._update, args=(self.closed, self.jobs, self.worker))\n",
    "        self.closed.value = 0\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "        \n",
    "    def _hide_progress_bar(self):\n",
    "        self.closed.value = 1\n",
    "        self.thread.join()   \n",
    "        self.thread = None\n",
    "\n",
    "    def _signal_handler(self, sig, frame):\n",
    "        self._hide_progress_bar()\n",
    "        sys.exit(0)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def _submit_hpc_job(args):\n",
    "        \"\"\"\n",
    "        Execute a job assigned to a working during Pool.imap().\n",
    "        A worker got assigned to this job and will now be executed.\n",
    "\n",
    "        (Called by start() during job submission to the processing pool)\n",
    "        \n",
    "        Args:\n",
    "            args (tuple): A tuple containing the job (HPC_Job instance) and the multiprocessing.Queue \n",
    "                          for communication between processes.\n",
    "\n",
    "        Workflow:\n",
    "            1. Adds the initial job state to the queue.\n",
    "            2. Executes the job using the `subprocess.Popen` method with the specified HPC command.\n",
    "            3. Monitors the job's output and error streams to update its state and progress:\n",
    "                - Detects job submission, waiting, and running states based on specific keywords.\n",
    "                - Captures job ID and queue information from the output.\n",
    "                - Tracks progress updates using \"STATUS\" messages.\n",
    "                - Handles errors and stack traces, marking the job as failed if necessary.\n",
    "            4. Writes output and error logs to files in the `lsf_logs` directory.\n",
    "            5. Updates the job state to indicate completion and adds it to the queue.\n",
    "\n",
    "        \"\"\"\n",
    "        job = args\n",
    "\n",
    "        # Add initial queue entry\n",
    "        err_handle = None\n",
    "        out_handle = None\n",
    "\n",
    "        try:\n",
    "\n",
    "            queue.put(job) # Announce the job to the queue\n",
    "            p = subprocess.Popen(job.hpc_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setpgrp, text=True) \n",
    "\n",
    "            # Poll results\n",
    "            while True:\n",
    "\n",
    "                # Check if the process is still running (Strange, but this is needed to avoid deadlocks)\n",
    "                if p.poll() is not None:\n",
    "                    break\n",
    "\n",
    "                # Check if there is output to read (We need select to check for both stdin and stderr, without blocking)\n",
    "                readable, _, _ = select.select([p.stdout, p.stderr], [], [], 0.01)\n",
    "\n",
    "                # Check if there is data to read from the stderr stream\n",
    "                # Try to parse it, usually bsub uses stderr for job submission status\n",
    "                if p.stderr in readable:\n",
    "                    s = p.stderr.readline()\n",
    "                    if \"waiting for dispatch\" in s.lower():\n",
    "                        job.state |= HPC_Job.JOB_WAITING\n",
    "                        queue.put(job)\n",
    "\n",
    "                    if \"starting on\" in s.lower():\n",
    "                        job.state |= HPC_Job.JOB_RUNNING\n",
    "                        queue.put(job)\n",
    "\n",
    "                    if re.match(r\"^Job <[0-9]*>.*\", s.strip(), re.IGNORECASE):\n",
    "                        job.state |= HPC_Job.JOB_SUBMITTED\n",
    "                        job_str = re.sub(r\"^[^0-9]*([0-9]*)[^<]*<([^>]*).*\", \"\\\\1|\\\\2\", s.lower().strip(), re.IGNORECASE)\n",
    "                        job.lsf_job_id, job.lsf_job_queue = job_str.split(\"|\")\n",
    "                        os.makedirs(\"lsf_logs\", exist_ok=True)\n",
    "                        err_handle = open(f'lsf_logs/lsf_{job.lsf_job_queue}_{job.lsf_job_id}.err', 'a+')\n",
    "                        out_handle = open(f'lsf_logs/lsf_{job.lsf_job_queue}_{job.lsf_job_id}.out', 'a+')\n",
    "                        queue.put(job)\n",
    "                    if err_handle is not None:\n",
    "                        err_handle.write(s)\n",
    "\n",
    "                # Check if there is data to read from the stdout stream\n",
    "                # If there is Traceback somewhere in the output, we assume the job failed\n",
    "                # If there is a STATUS line, we assume the job is running and its a progress update\n",
    "                if p.stdout in readable:\n",
    "                    s = p.stdout.readline()\n",
    "                    if s.startswith(\"Traceback\"):\n",
    "                        job.state |= HPC_Job.JOB_TASK_FAILED\n",
    "                        queue.put(job)\n",
    "                        p.terminate()\n",
    "\n",
    "                    if s.startswith(\"STATUS,\"):\n",
    "                        details = s.split(\",\")\n",
    "                        details = [x.strip() for x in details]\n",
    "                        job.status_current, job.status_total, job.description = details[-3:]\n",
    "                        queue.put(job)\n",
    "                    if out_handle is not None:\n",
    "                        out_handle.write(s)\n",
    "\n",
    "        except Exception as e:\n",
    "            job.bsub_error_msg = str(e)\n",
    "            job.state |= HPC_Job.JOB_BSUB_FAILED\n",
    "\n",
    "        if err_handle is not None:\n",
    "            err_handle.close()\n",
    "        if out_handle is not None:\n",
    "            out_handle.close()\n",
    "\n",
    "        # Add queue entry to indicate completion\n",
    "        job.state |= HPC_Job.JOB_COMPLETED\n",
    "        queue.put(job)\n",
    "\n",
    "        p.wait()\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def _clear_progress_bar(bar):\n",
    "        \"\"\"\n",
    "        Resets and clears the progress bar's state and appearance.\n",
    "\n",
    "        This is needed if more workers are there then open tasks\n",
    "\n",
    "        Args:\n",
    "            bar (tqdm.tqdm): The progress bar instance to be cleared. This is \n",
    "            an object from the `tqdm` library.\n",
    "\n",
    "        \"\"\"\n",
    "        bar.n = 0\n",
    "        bar.total = None\n",
    "        bar.bar_format = \"\"\n",
    "        bar.set_description(Style.RESET_ALL + \"\")\n",
    "        bar.update(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_main_progress_bar(bar, all_jobs):\n",
    "        \"\"\"\n",
    "        Updates the main progress bar with the current status of HPC jobs.\n",
    "\n",
    "        Args:\n",
    "            bar (tqdm.tqdm): The progress bar object to be updated.\n",
    "            all_jobs (dict): A dictionary containing all HPC jobs, where the keys are job IDs \n",
    "                             and the values are HPC_Job objects.\n",
    "\n",
    "        The function performs the following:\n",
    "            - Counts the number of jobs in different states (running, completed, task failures, bsub failures).\n",
    "            - Updates the progress bar to reflect the number of completed jobs.\n",
    "            - Sets a descriptive text for the progress bar, including the number of running, completed, \n",
    "              and failed jobs. If there are bsub failures, their error messages are also included.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract run-details\n",
    "        cnt_task_fails   = list(filter(lambda x: x.state & HPC_Job.JOB_TASK_FAILED, all_jobs.values()))\n",
    "        cnt_bsub_fails   = list(filter(lambda x: x.state & HPC_Job.JOB_BSUB_FAILED, all_jobs.values()))\n",
    "        tasks_running = list(filter(lambda x: not (x.state & HPC_Job.JOB_COMPLETED), all_jobs.values()))\n",
    "        tasks_done    = list(filter(lambda x:     (x.state & HPC_Job.JOB_COMPLETED), all_jobs.values()))\n",
    "\n",
    "        bar.update(len(tasks_done) - bar.n)\n",
    "        desc = f\"{Style.BRIGHT}{Fore.CYAN}RUNNING:{len(tasks_running)}, DONE:{len(tasks_done)}{Style.RESET_ALL}\"\n",
    "        if len(cnt_task_fails):\n",
    "            desc += f\"\\033[31m, FAILED:{len(cnt_task_fails)}\\033[0m\"\n",
    "\n",
    "        if cnt_bsub_fails:\n",
    "            err = \", \".join([x.bsub_error_msg for x in cnt_bsub_fails])\n",
    "            desc += f\"\\033[31m, FAILED:{err}\\033[0m\"\n",
    "\n",
    "        bar.set_description(desc)\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_progress_bar(bar, job):\n",
    "        \"\"\"\n",
    "        Updates a progress bar to reflect the current state and progress of a job.\n",
    "        Args:\n",
    "            bar (tqdm.tqdm): The progress bar object to be updated.\n",
    "            job (HPC_Job): The job object containing the current state, progress, and metadata.\n",
    "        Behavior:\n",
    "            - Updates the progress bar's total and current progress based on the job's status.\n",
    "            - Sets the progress bar's description to include the job's state, command, and description.\n",
    "            - Colors the description text based on the job's state for better visual feedback.\n",
    "            - Handles special cases such as task failures by appending additional information to the description.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define job states and their corresponding colors\n",
    "        job_states = {            \n",
    "            HPC_Job.JOB_SUBMITTED   : (\"submitted\", Fore.YELLOW),\n",
    "            HPC_Job.JOB_WAITING     : (\"wait\", Fore.YELLOW),\n",
    "            HPC_Job.JOB_RUNNING     : (\"run\", Fore.GREEN),\n",
    "            HPC_Job.JOB_COMPLETED   : (\"done\", Style.RESET_ALL),\n",
    "            HPC_Job.JOB_BSUB_FAILED : (\"fail\", Fore.RED),\n",
    "            HPC_Job.JOB_TASK_FAILED : (\"FAIL\",Fore.RED),\n",
    "        }\n",
    "\n",
    "        # Update the description\n",
    "        bar.bar_format = \"{l_bar}{bar}|{n_fmt}/{total_fmt} [{elapsed}<{remaining}]\"\n",
    "\n",
    "        if job.status_total != \"\": \n",
    "            bar.total = int(job.status_total)\n",
    "        \n",
    "        if job.status_current != \"\":\n",
    "            bar.update(int(job.status_current) - bar.n)\n",
    "        \n",
    "        # Determine the color and state based on the job's state\n",
    "        state_color = Style.RESET_ALL\n",
    "        state_str = \"\"\n",
    "        for state, (_str, _style) in job_states.items():\n",
    "            if job.state & state:\n",
    "                state_color = _style\n",
    "                state_str = _str\n",
    "\n",
    "        #description = f\"{state_color}{job.lsf_job_id}<{job.lsf_job_queue}>{job.description} [{job.state}]\"\n",
    "        cmd_str = \" \".join(job.command)[0:50]\n",
    "\n",
    "        description = f\"{state_color}[{state_str}]: {cmd_str} >> {job.description}\"\n",
    "        if job.state & HPC_Job.JOB_TASK_FAILED:\n",
    "            description += \" [Stacktrace!] \"\n",
    "        description += f\"{Style.RESET_ALL}\"\n",
    "        bar.set_description(description)\n",
    "\n",
    "    @staticmethod\n",
    "    def _update(closed, jobs, workers):\n",
    "        \"\"\"\n",
    "        Updates the progress of distributed jobs and manages progress bars for workers.\n",
    "        This function continuously monitors the status of jobs in a distributed HPC \n",
    "        (High-Performance Computing) environment. It updates progress bars for individual \n",
    "        workers and a main progress bar for all jobs. \n",
    "        Args:\n",
    "            closed (multiprocessing.Value): A shared value indicating whether the \n",
    "                processing has been closed. A value of 0 means processing is ongoing, \n",
    "                while a non-zero value indicates closure.\n",
    "            queue (multiprocessing.Queue): A queue containing job updates (HPC_Job object). \n",
    "            jobs (list): A list of all jobs.\n",
    "            workers (int): The number of workers processes handling the jobs.\n",
    "        Behavior:\n",
    "            - Creates and manages progress bars for the total job progress and individual workers.\n",
    "            - Updates progress bars periodically (every 0.2 seconds) to reflect the current \n",
    "              state of jobs.\n",
    "            - Cleans up progress bars for completed jobs.\n",
    "            - Closes all progress bars once processing is complete.\n",
    "        \"\"\"\n",
    "\n",
    "        all_jobs = {} # Tracking all jobs, received by the queue\n",
    "\n",
    "        # Create progress bars\n",
    "        bars  = [tqdm.tqdm(total=len(jobs), leave=True, desc=\"Total\")] # Main progress bar\n",
    "        bars += [tqdm.tqdm(leave=True) for i in range(workers)]        # Worker progress bars\n",
    "        bar_assignement = dict([(i+1, None) for i in range(workers)])  # Bar-id to job-uuid mapping, to keep order while rendering\n",
    "\n",
    "        _last_update = 0\n",
    "        _lastUUID = None\n",
    "        \n",
    "        while closed.value == 0 or not queue.empty():\n",
    "\n",
    "            # Get queue entry and add it to the status dict\n",
    "            job = None\n",
    "            if not queue.empty():\n",
    "                job = queue.get_nowait()\n",
    "            if job is None:\n",
    "                time.sleep(0.01)\n",
    "                continue\n",
    " \n",
    "            # Log job details\n",
    "            all_jobs[job.uuid] = job\n",
    "\n",
    "            # Update only every n-seconds\n",
    "            if _last_update > (time.time() - 0.5) and _lastUUID == job.uuid:\n",
    "                continue\n",
    "            _last_update = time.time()\n",
    "            _lastUUID = job.uuid\n",
    "\n",
    "            # Clean up progress bars\n",
    "            for i in range(workers):\n",
    "                if bar_assignement[i+1] is not None:\n",
    "                    uuid = bar_assignement[i+1]\n",
    "                    if uuid in all_jobs:\n",
    "                        _job = all_jobs[uuid]\n",
    "                        if _job.state & HPC_Job.JOB_COMPLETED:\n",
    "                            bar_assignement[i+1] = None\n",
    "                            DistributeHPC._clear_progress_bar(bars[i+1])\n",
    "\n",
    "            # Add this job.uuid if not already assigned\n",
    "            if job.uuid not in bar_assignement.values():\n",
    "                for i in range(workers):\n",
    "                    if bar_assignement[i+1] is None:\n",
    "                        bar_assignement[i+1] = job.uuid\n",
    "                        break\n",
    "\n",
    "            # Update task progress bar\n",
    "            barid = [x for x,y in bar_assignement.items() if y == job.uuid]\n",
    "            if len(barid):\n",
    "                DistributeHPC._update_progress_bar(bars[barid[0]], job)\n",
    "\n",
    "            # Update main progress bar\n",
    "            DistributeHPC._update_main_progress_bar(bars[0], all_jobs)\n",
    "\n",
    "        # Close all progress bars\n",
    "        DistributeHPC._update_main_progress_bar(bars[0], all_jobs)\n",
    "        _ = [DistributeHPC._clear_progress_bar(x) for x in bars[1:]]            \n",
    "        _ = [x.close() for x in bars]    \n",
    "\n",
    "    # Legacy function \n",
    "    def set_jobs_hpc(self, jobs, num_cpu=4):\n",
    "        print(\"WARNING: set_jobs_hpc is deprecated. Use set_jobs instead.\")\n",
    "        self.set_jobs(jobs)        \n",
    "\n",
    "    def set_jobs(self, jobs, num_cpu=4):\n",
    "        \"\"\"\n",
    "        Adds jobs to the job list\n",
    "\n",
    "        Args:\n",
    "            jobs (list): A list of jobs to be added. Each job can either be an instance\n",
    "                         of `HPC_Job` or a [list] of commands.\n",
    "        \"\"\"\n",
    "        for j in jobs:\n",
    "            if isinstance(j, HPC_Job):\n",
    "                self.jobs.append(j)\n",
    "            else:\n",
    "                _j = HPC_Job(cmd=j, cores=num_cpu)\n",
    "                self.jobs.append(_j)\n",
    "\n",
    "        \n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Starts the distributed HPC job processing workflow.\n",
    "        This method initializes and manages the execution of HPC jobs using a multiprocessing pool.\n",
    "        It also handles progress visualization and ensures proper cleanup of resources.\n",
    "        Workflow:\n",
    "        1. Displays a progress bar to track job execution.\n",
    "        2. Sets up a signal handler for graceful interruption handling.\n",
    "        3. Creates a multiprocessing pool to process jobs concurrently.\n",
    "        4. Submits jobs to the pool.\n",
    "        5. Ensures proper cleanup of the pool and hides the progress bar after execution.\n",
    "        Note:\n",
    "            - The number of worker processes is determined by the `self.worker` attribute.\n",
    "            - Each job is submitted along with the shared queue (`self.queue`).\n",
    "            - The `_submit_hpc_job` method is used to process individual jobs.\n",
    "        Raises:\n",
    "            Any exceptions raised during job submission or processing will propagate.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a thread to plot progress\n",
    "        self._show_progress_bar()\n",
    "        \n",
    "        # Create a pool to process multiple tasks\n",
    "        signal.signal(signal.SIGINT, self._signal_handler)\n",
    "        pool = multiprocessing.Pool(processes=self.worker, initargs=self)\n",
    "        results = pool.imap(self._submit_hpc_job, [x for x in self.jobs])\n",
    "        for r in results:\n",
    "            pass # Not interested in the result\n",
    "\n",
    "        pool.close()\n",
    "        time.sleep(0.5)\n",
    "        pool.terminate()\n",
    "\n",
    "        # Hide progress bar\n",
    "        self._hide_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#import nbdev; nbdev.nbdev_export('hpc.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
