{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094b738a",
   "metadata": {},
   "source": [
    "# Change model metadata\n",
    "> Change model metadata without training new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18595dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp postprocessing.model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1166e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a12d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#CV_TOOLS = Path(r'/home/ai_dsx.work/data/projects/cv_tools')\n",
    "#sys.path.append(str(CV_TOOLS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dotenv import load_dotenv\n",
    "from fastcore.all import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Union, List, Optional, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Anomalib imports\n",
    "import anomalib\n",
    "from anomalib.deploy import TorchInferencer, OpenVINOInferencer\n",
    "from anomalib.models import (\n",
    "    Padim, Patchcore, Cflow, Fastflow, Stfpm, \n",
    "    EfficientAd, Draem, ReverseDistillation,\n",
    "    Dfkde, Dfm, Ganomaly, Cfa, Csflow, Dsr, Fre, Rkde, Uflow\n",
    ")\n",
    "from anomalib.utils.visualization.image import ImageVisualizer, VisualizationMode\n",
    "from anomalib.data.utils import read_image\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger('anomalib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3234ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from be_vision_ad_tools.inference.prediction_system import create_inference_poster_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ModelMetadata:\n",
    "    \"\"\"Configuration class for model metadata parameters.\"\"\"\n",
    "    image_threshold: Optional[float] = None\n",
    "    pixel_threshold: Optional[float] = None\n",
    "    pred_score_min: Optional[float] = None\n",
    "    pred_score_max: Optional[float] = None\n",
    "    anomaly_map_min: Optional[float] = None\n",
    "    anomaly_map_max: Optional[float] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary, excluding None values.\"\"\"\n",
    "        return {k: v for k, v in self.__dict__.items() if v is not None}\n",
    "    \n",
    "    def update_from_dict(self, data: Dict[str, Any]) -> None:\n",
    "        \"\"\"Update metadata from dictionary.\"\"\"\n",
    "        for key, value in data.items():\n",
    "            if hasattr(self, key) and value is not None:\n",
    "                setattr(self, key, float(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be08527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _detect_model_class_from_filename(model_path: Path):\n",
    "    \"\"\"Auto-detect anomalib model class from filename.\"\"\"\n",
    "    filename_lower = model_path.name.lower()\n",
    "    \n",
    "    # Model mapping\n",
    "    model_mapping = {\n",
    "        'padim': Padim,\n",
    "        'patchcore': Patchcore,\n",
    "        'cflow': Cflow,\n",
    "        'fastflow': Fastflow,\n",
    "        'stfpm': Stfpm,\n",
    "        'efficient': EfficientAd,\n",
    "        'draem': Draem,\n",
    "        'reverse': ReverseDistillation,\n",
    "        'dfkde': Dfkde,\n",
    "        'dfm': Dfm,\n",
    "        'ganomaly': Ganomaly,\n",
    "        'cfa': Cfa,\n",
    "        'csflow': Csflow,\n",
    "        'dsr': Dsr,\n",
    "        'fre': Fre,\n",
    "        'rkde': Rkde,\n",
    "        'uflow': Uflow\n",
    "    }\n",
    "    \n",
    "    for name, model_class in model_mapping.items():\n",
    "        if name in filename_lower:\n",
    "            print(f\"ðŸŽ¯ Auto-detected model: {name.upper()}\")\n",
    "            return model_class\n",
    "    \n",
    "    # Default to PaDiM if can't detect\n",
    "    print(f\"âš ï¸  Could not auto-detect model type from filename, using PaDiM\")\n",
    "    return Padim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_im_path = Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/images/bad')\n",
    "print(f'tst im folder exists: {tst_im_path.exists()}')\n",
    "model_path= Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/models/exports/TEST_MULITNODE_task_001_padim_resnet18_18_layer1-layer2-layer3/weights/torch/model.pt')\n",
    "print(model_path.exists())\n",
    "model_path_c= Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/models/checkpoints/TEST_MULITNODE_task_001_padim_resnet18_18_layer1-layer2-layer3/last.ckpt')\n",
    "print(model_path_c.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3467c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0211d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_detect_model_class_from_filename(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_detect_model_class_from_filename(model_path_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4189128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_model_any_format(\n",
    "    model_path: Union[str, Path], # Path to model file (.pt, .ckpt, .onnx)\n",
    "    device: str = \"auto\" # Device to load model on (\"auto\", \"cpu\", \"cuda\")\n",
    ") -> Tuple[Any, str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load a model from any supported format and extract current metadata.\n",
    "    \n",
    "    \"\"\"\n",
    "    model_path = Path(model_path)\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    \n",
    "    # Auto-detect device\n",
    "    if device == \"auto\":\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    model_ext = model_path.suffix.lower()\n",
    "    current_metadata = {}\n",
    "    \n",
    "    print(f\"ðŸ“‚ Loading {model_ext} model from: {model_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        if model_ext in ['.pt', '.pth']:\n",
    "            # Load PyTorch exported model\n",
    "            model_data = torch.load(model_path, map_location=device, weights_only=False)\n",
    "            \n",
    "            # Extract metadata from model_data if available\n",
    "            if isinstance(model_data, dict):\n",
    "                # Check for metadata in the saved state\n",
    "                if 'metadata' in model_data:\n",
    "                    # Create a copy to avoid modifying the original\n",
    "                    current_metadata = dict(model_data['metadata'])\n",
    "                elif 'model_metadata' in model_data:\n",
    "                    # Create a copy to avoid modifying the original\n",
    "                    current_metadata = dict(model_data['model_metadata'])\n",
    "                else:\n",
    "                    # Try to extract from model state if it's a full model save\n",
    "                    print(\"âš ï¸  No explicit metadata found in .pt file\")\n",
    "            \n",
    "            return model_data, \"torch\", current_metadata\n",
    "            \n",
    "        elif model_ext == '.ckpt':\n",
    "            # Load Lightning checkpoint\n",
    "            model_class = _detect_model_class_from_filename(model_path)\n",
    "            model = model_class.load_from_checkpoint(model_path, map_location=device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Extract current metadata from trained model\n",
    "            if hasattr(model, 'image_threshold') and hasattr(model, 'normalization_metrics'):\n",
    "                current_metadata = {\n",
    "                    'image_threshold': float(model.image_threshold.value.item()) if hasattr(model.image_threshold.value, 'item') else float(model.image_threshold.value),\n",
    "                    'pixel_threshold': float(model.pixel_threshold.value.item()) if hasattr(model.pixel_threshold.value, 'item') else float(model.pixel_threshold.value),\n",
    "                    'pred_score_min': float(model.normalization_metrics.pred_scores.min.item()) if hasattr(model.normalization_metrics.pred_scores.min, 'item') else float(model.normalization_metrics.pred_scores.min),\n",
    "                    'pred_score_max': float(model.normalization_metrics.pred_scores.max.item()) if hasattr(model.normalization_metrics.pred_scores.max, 'item') else float(model.normalization_metrics.pred_scores.max),\n",
    "                    'anomaly_map_min': float(model.normalization_metrics.anomaly_maps.min.item()) if hasattr(model.normalization_metrics.anomaly_maps.min, 'item') else float(model.normalization_metrics.anomaly_maps.min),\n",
    "                    'anomaly_map_max': float(model.normalization_metrics.anomaly_maps.max.item()) if hasattr(model.normalization_metrics.anomaly_maps.max, 'item') else float(model.normalization_metrics.anomaly_maps.max)\n",
    "                }\n",
    "            else:\n",
    "                print(\"âš ï¸  Model doesn't have required metadata attributes\")\n",
    "            \n",
    "            return model, \"checkpoint\", current_metadata\n",
    "            \n",
    "        elif model_ext in ['.onnx', '.xml']:\n",
    "            # ONNX/OpenVINO models - metadata handling is limited\n",
    "            print(f\"ðŸ“ Loading {model_ext} model (limited metadata modification support)\")\n",
    "            \n",
    "            # For ONNX/OpenVINO, we can't easily modify internal metadata\n",
    "            # but we can create external metadata files\n",
    "            metadata_file = model_path.with_suffix('.json')\n",
    "            if metadata_file.exists():\n",
    "                with open(metadata_file, 'r') as f:\n",
    "                    current_metadata = json.load(f)\n",
    "                print(f\"ðŸ“‹ Loaded external metadata from: {metadata_file.name}\")\n",
    "            else:\n",
    "                print(\"âš ï¸  No external metadata file found for ONNX/OpenVINO model\")\n",
    "            \n",
    "            return str(model_path), model_ext[1:], current_metadata  # Return path as string\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model format: {model_ext}. Supported: .pt, .ckpt, .onnx, .xml\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load model {model_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c21ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_path)\n",
    "print(Path(model_path).exists())\n",
    "print(Path(model_path_c).exists())\n",
    "model_obj, model_f, ori_metadata = loaded_model = load_model_any_format(model_path)\n",
    "model_obj_c, model_f_c, ori_metadata_c = loaded_model = load_model_any_format(model_path_c)\n",
    "\n",
    "loaded_model_c = load_model_any_format(model_path_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fb786",
   "metadata": {},
   "source": [
    "### Step 2: Modify Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _validate_metadata_ranges(metadata_dict: Dict[str, Any]) -> None:\n",
    "    \"\"\"Validate metadata value ranges.\"\"\"\n",
    "    for key, value in metadata_dict.items():\n",
    "        if not isinstance(value, (int, float)):\n",
    "            raise ValueError(f\"Metadata value for {key} must be numeric, got {type(value)}\")\n",
    "            \n",
    "        # Basic range validation\n",
    "        if key.endswith('_threshold') and (value < 0 or value > 100):\n",
    "            print(f\"âš ï¸  Warning: {key} = {value} is outside typical range [0, 100]\")\n",
    "            \n",
    "        if key.endswith('_min') and key.endswith('_max'):\n",
    "            # This would need more sophisticated validation for min/max pairs\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2bb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def modify_model_metadata(\n",
    "    model_object: Any, # Loaded model object\n",
    "    model_format: str, # Model format (\"torch\", \"checkpoint\", \"onnx\", \"xml\")\n",
    "    new_metadata: Union[ModelMetadata, Dict[str, Any]], # New metadata values\n",
    "    validate_ranges: bool = True # Whether to validate metadata value ranges\n",
    ") -> Tuple[Any, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Modify model metadata in-place with validation.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert to dict if ModelMetadata object\n",
    "    if isinstance(new_metadata, ModelMetadata):\n",
    "        metadata_dict = new_metadata.to_dict()\n",
    "    else:\n",
    "        metadata_dict = dict(new_metadata)\n",
    "    \n",
    "    # Validate metadata ranges\n",
    "    if validate_ranges:\n",
    "        _validate_metadata_ranges(metadata_dict)\n",
    "    \n",
    "    applied_metadata = {}\n",
    "    \n",
    "    print(f\"ðŸ”§ Modifying metadata for {model_format} model...\")\n",
    "    \n",
    "    try:\n",
    "        if model_format == \"checkpoint\":\n",
    "            # Modify Lightning checkpoint model\n",
    "            for key, value in metadata_dict.items():\n",
    "                if key == 'image_threshold' and hasattr(model_object, 'image_threshold'):\n",
    "                    model_object.image_threshold.value = torch.tensor(value, device=model_object.device)\n",
    "                    applied_metadata[key] = value\n",
    "                    print(f\"  âœ… Updated image_threshold: {value}\")\n",
    "                    \n",
    "                elif key == 'pixel_threshold' and hasattr(model_object, 'pixel_threshold'):\n",
    "                    model_object.pixel_threshold.value = torch.tensor(value, device=model_object.device)\n",
    "                    applied_metadata[key] = value\n",
    "                    print(f\"  âœ… Updated pixel_threshold: {value}\")\n",
    "                    \n",
    "                elif key.startswith('pred_score_') and hasattr(model_object, 'normalization_metrics'):\n",
    "                    if key == 'pred_score_min':\n",
    "                        model_object.normalization_metrics.pred_scores.min = torch.tensor(value, device=model_object.device)\n",
    "                        applied_metadata[key] = value\n",
    "                        print(f\"  âœ… Updated pred_score_min: {value}\")\n",
    "                    elif key == 'pred_score_max':\n",
    "                        model_object.normalization_metrics.pred_scores.max = torch.tensor(value, device=model_object.device)\n",
    "                        applied_metadata[key] = value\n",
    "                        print(f\"  âœ… Updated pred_score_max: {value}\")\n",
    "                        \n",
    "                elif key.startswith('anomaly_map_') and hasattr(model_object, 'normalization_metrics'):\n",
    "                    if key == 'anomaly_map_min':\n",
    "                        model_object.normalization_metrics.anomaly_maps.min = torch.tensor(value, device=model_object.device)\n",
    "                        applied_metadata[key] = value\n",
    "                        print(f\"  âœ… Updated anomaly_map_min: {value}\")\n",
    "                    elif key == 'anomaly_map_max':\n",
    "                        model_object.normalization_metrics.anomaly_maps.max = torch.tensor(value, device=model_object.device)\n",
    "                        applied_metadata[key] = value\n",
    "                        print(f\"  âœ… Updated anomaly_map_max: {value}\")\n",
    "                else:\n",
    "                    print(f\"  âš ï¸  Skipped {key}: Not supported or attribute not found\")\n",
    "                    \n",
    "        elif model_format == \"torch\":\n",
    "            # Modify PyTorch exported model data\n",
    "            if isinstance(model_object, dict):\n",
    "                # Ensure metadata section exists\n",
    "                if 'metadata' not in model_object:\n",
    "                    model_object['metadata'] = {}\n",
    "                \n",
    "                # Update metadata\n",
    "                model_object['metadata'].update(metadata_dict)\n",
    "                applied_metadata = dict(metadata_dict)\n",
    "                \n",
    "                for key, value in metadata_dict.items():\n",
    "                    print(f\"  âœ… Updated {key}: {value}\")\n",
    "            else:\n",
    "                print(\"  âš ï¸  PyTorch model is not in expected dictionary format\")\n",
    "                \n",
    "        elif model_format in [\"onnx\", \"xml\"]:\n",
    "            # For ONNX/OpenVINO, we'll handle metadata externally\n",
    "            applied_metadata = dict(metadata_dict)\n",
    "            print(f\"  ðŸ“ Metadata will be saved externally for {model_format} model\")\n",
    "            for key, value in metadata_dict.items():\n",
    "                print(f\"  âœ… Prepared {key}: {value}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model format: {model_format}\")\n",
    "            \n",
    "        return model_object, applied_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to modify model metadata: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb49008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353cb4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata = ModelMetadata(\n",
    "    image_threshold=0.7,      # Increase detection threshold\n",
    "    pred_score_min=0.0,       # Set normalization range\n",
    "    pred_score_max=20.0,\n",
    "    anomaly_map_min=0.0,      # Set anomaly map normalization range\n",
    "    anomaly_map_max=10.0,\n",
    "    pixel_threshold=0.7      # Set pixel-level threshold\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a02662",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_model, applied_metdata = modify_model_metadata(\n",
    "    model_object=model_obj,\n",
    "    model_format='torch',#'checkpoint',\n",
    "    new_metadata=new_metadata,\n",
    "    validate_ranges=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified_model_p = Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/models/modified_model')\n",
    "#Path(modified_model_p).mkdir(parents=True, exist_ok=True)\n",
    "#torch.save(modified_model, f'{modified_model_p}/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a19570",
   "metadata": {},
   "source": [
    "### Step 3: Save modified model and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_model_with_metadata(\n",
    "    model_object: Any, # Modified model object\n",
    "    model_format: str, # Model format (\"torch\", \"checkpoint\", \"onnx\", \"xml\")\n",
    "    output_path: Union[str, Path], # Path to save the modified model\n",
    "    metadata: Dict[str, Any], # Metadata that was applied\n",
    "    create_backup: bool = True, # Whether to create backup of original\n",
    "    backup_suffix: str = \"_backup\" # Suffix for backup files\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Save model with modified metadata.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results = {\n",
    "        'saved_model_path': str(output_path),\n",
    "        'backup_path': None,\n",
    "        'metadata_file': None,\n",
    "        'format': model_format,\n",
    "        'metadata': metadata,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saving {model_format} model to: {output_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create backup if requested and file exists\n",
    "        if create_backup and output_path.exists():\n",
    "            backup_path = output_path.with_stem(f\"{output_path.stem}{backup_suffix}\")\n",
    "            backup_path.write_bytes(output_path.read_bytes())\n",
    "            results['backup_path'] = str(backup_path)\n",
    "            print(f\"ðŸ“ Created backup: {backup_path.name}\")\n",
    "        \n",
    "        if model_format == \"checkpoint\":\n",
    "            # Save Lightning checkpoint\n",
    "            torch.save(model_object.state_dict(), output_path)\n",
    "            print(f\"  âœ… Saved checkpoint model\")\n",
    "            \n",
    "        elif model_format == \"torch\":\n",
    "            # Save PyTorch model\n",
    "            torch.save(model_object, output_path)\n",
    "            print(f\"  âœ… Saved PyTorch model\")\n",
    "            \n",
    "        elif model_format in [\"onnx\", \"xml\"]:\n",
    "            # For ONNX/OpenVINO, save external metadata file\n",
    "            metadata_file = output_path.with_suffix('.json')\n",
    "            with open(metadata_file, 'w') as f:\n",
    "                json.dump({\n",
    "                    'model_metadata': metadata,\n",
    "                    'modified_timestamp': datetime.now().isoformat(),\n",
    "                    'original_model': str(output_path)\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            results['metadata_file'] = str(metadata_file)\n",
    "            print(f\"  âœ… Saved external metadata: {metadata_file.name}\")\n",
    "            \n",
    "            # Note: Original ONNX/OpenVINO model file should be copied if different location\n",
    "            if isinstance(model_object, str) and Path(model_object) != output_path:\n",
    "                # Copy original model file\n",
    "                output_path.write_bytes(Path(model_object).read_bytes())\n",
    "                print(f\"  âœ… Copied model file\")\n",
    "        \n",
    "        print(f\"ðŸŽ‰ Successfully saved model with modified metadata!\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res = save_model_with_metadata(\n",
    "    model_object=modified_model,\n",
    "    model_format='torch',\n",
    "    output_path=Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/models/modified_model/model_m.pt'),\n",
    "    metadata=applied_metdata,\n",
    "    create_backup=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4446f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d54f9a",
   "metadata": {},
   "source": [
    "### Step 4: Visualize results with modified metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d825d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_metadata_comparison_poster(\n",
    "    original_model_path: Union[str, Path], # Path to original model\n",
    "    modified_model_path: Union[str, Path], # Path to modified model  \n",
    "    test_images: List[Union[str, Path]], # Test images for comparison\n",
    "    output_dir: Union[str, Path], # Output directory\n",
    "    metadata_changes: Dict[str, Any], # Summary of metadata changes\n",
    "    poster_title_prefix: str = \"Metadata Comparison\", # Title prefix\n",
    "    poster_rows:int=3, # Number of poster rows\n",
    "    poster_cols:int=3, # Number of poster columns\n",
    "    image_size_in_poster: Tuple[int, int]=(256, 256), # Size of images in poster\n",
    "    include_heatmap: bool = True, # Whether to include heatmap poster\n",
    "    include_image_poster: bool = False, # Whether to include image poster\n",
    "    include_anomaly_poster: bool = False, # Whether to include anomaly poster\n",
    "    show_plot: bool = False # Whether to display the plot\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create model metadata comparison posters \n",
    "    \n",
    "        \n",
    "    Example:\n",
    "        ```python\n",
    "        results = create_metadata_comparison_poster(\n",
    "            original_model_path=\"models/original.pt\",\n",
    "            modified_model_path=\"models/modified.pt\", \n",
    "            test_images=[\"test1.jpg\", \"test2.jpg\"],\n",
    "            output_dir=\"comparison_results\",\n",
    "            metadata_changes={\n",
    "                \"image_threshold\": {\"old\": 0.5, \"new\": 0.7},\n",
    "                \"pred_score_max\": {\"old\": 1.0, \"new\": 20.0}\n",
    "            }\n",
    "        )\n",
    "        ```\n",
    "    \"\"\"\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"   Original model: {Path(original_model_path).name}\")\n",
    "    print(f\"   Modified model: {Path(modified_model_path).name}\")\n",
    "    print(f\"   Test images: {len(test_images)}\")\n",
    "    \n",
    "    # Create title with metadata changes summary\n",
    "    changes_summary = \", \".join([\n",
    "        f\"{k}: {v['old']}â†’{v['new']}\" for k, v in metadata_changes.items()\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Create poster for original model\n",
    "        print(\"\\\\nðŸ“Š Creating original model poster...\")\n",
    "        original_poster_results = create_inference_poster_(\n",
    "            model_path=original_model_path,\n",
    "            test_images=test_images,\n",
    "            output_folder=output_dir / \"original_model\",\n",
    "            poster_title=f\"{poster_title_prefix} - Original Model\",\n",
    "            poster_rows=poster_rows,\n",
    "            poster_cols=poster_cols,\n",
    "            include_heatmap_poster=include_heatmap,\n",
    "            include_image_poster=include_image_poster,\n",
    "            include_anomaly_poster=include_anomaly_poster,\n",
    "            image_size_in_poster=image_size_in_poster,\n",
    "            device=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Create poster for modified model  \n",
    "        print(\"\\\\nðŸ“Š Creating modified model poster...\")\n",
    "        modified_poster_results = create_inference_poster_(\n",
    "            model_path=modified_model_path,\n",
    "            test_images=test_images,\n",
    "            output_folder=output_dir / \"modified_model\",\n",
    "            poster_title=f\"{poster_title_prefix} - Modified Model ({changes_summary})\",\n",
    "            poster_rows=poster_rows,\n",
    "            poster_cols=poster_cols,\n",
    "            include_heatmap_poster=include_heatmap,\n",
    "            include_image_poster=include_image_poster,\n",
    "            include_anomaly_poster=include_anomaly_poster,\n",
    "            image_size_in_poster=image_size_in_poster,\n",
    "            device=\"auto\",\n",
    "            show_plot=True\n",
    "        )\n",
    "        \n",
    "        # Create summary comparison document\n",
    "        summary_path = output_dir / \"comparison_summary.json\"\n",
    "        summary_data = {\n",
    "            \"metadata_changes\": metadata_changes,\n",
    "            \"original_model\": {\n",
    "                \"path\": str(original_model_path),\n",
    "                \"posters\": original_poster_results.get(\"posters\", []),\n",
    "                \"statistics\": original_poster_results.get(\"statistics\", {})\n",
    "            },\n",
    "            \"modified_model\": {\n",
    "                \"path\": str(modified_model_path), \n",
    "                \"posters\": modified_poster_results.get(\"posters\", []),\n",
    "                \"statistics\": modified_poster_results.get(\"statistics\", {})\n",
    "            },\n",
    "            \"test_images\": [str(img) for img in test_images],\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        import json\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\\\nðŸŽ‰ Metadata comparison completed!\")\n",
    "        print(f\"   Original posters: {len(original_poster_results.get('posters', []))}\")\n",
    "        print(f\"   Modified posters: {len(modified_poster_results.get('posters', []))}\")\n",
    "        print(f\"   Summary: {summary_path}\")\n",
    "        print(f\"   Output directory: {output_dir}\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"original_results\": original_poster_results,\n",
    "            \"modified_results\": modified_poster_results,\n",
    "            \"summary_file\": str(summary_path),\n",
    "            \"output_directory\": str(output_dir),\n",
    "            \"metadata_changes\": metadata_changes\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to create metadata comparison: {str(e)}\"\n",
    "        print(f\"âŒ {error_msg}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": error_msg\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what keys are available in ori_metadata\n",
    "print(\"Available keys in ori_metadata:\")\n",
    "for key in ori_metadata.keys():\n",
    "    print(f\"  '{key}': {ori_metadata[key]}\")\n",
    "\n",
    "# Access the values with correct key names\n",
    "anomaly_map_min_original = ori_metadata.get('anomaly_maps.min', None)  # Note: 'anomaly_maps.min' not 'anomaly_map.min'\n",
    "anomaly_map_max_original = ori_metadata.get('anomaly_maps.max', None)\n",
    "pred_score_min_original = ori_metadata.get('pred_scores.min', None)\n",
    "pred_score_max_original = ori_metadata.get('pred_scores.max', None)\n",
    "\n",
    "# Create the metadata_changes dictionary with correct original values\n",
    "metadata_changes = {\n",
    "    \"image_threshold\": {\"old\": ori_metadata.get('image_threshold', None), \"new\": new_metadata.image_threshold},\n",
    "    \"pred_score_min\": {\"old\": pred_score_min_original, \"new\": new_metadata.pred_score_min},\n",
    "    \"pred_score_max\": {\"old\": pred_score_max_original, \"new\": new_metadata.pred_score_max},\n",
    "    \"anomaly_map_min\": {\"old\": anomaly_map_min_original, \"new\": new_metadata.anomaly_map_min},\n",
    "    \"anomaly_map_max\": {\"old\": anomaly_map_max_original, \"new\": new_metadata.anomaly_map_max},\n",
    "    \"pixel_threshold\": {\"old\": ori_metadata.get('pixel_threshold', None), \"new\": new_metadata.pixel_threshold}\n",
    "}\n",
    "\n",
    "print(f\"\\nMetadata changes:\")\n",
    "for key, change in metadata_changes.items():\n",
    "    print(f\"  {key}: {change['old']} â†’ {change['new']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_metadata.get('anomaly_map.min', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metadata_comparison_poster(\n",
    "    original_model_path=Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/models/modified_model/model.pt'),\n",
    "    modified_model_path=Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/models/modified_model/model_m.pt'),\n",
    "    test_images=[Path(tst_im_path, i) for i in os.listdir(tst_im_path)],\n",
    "\n",
    "    output_dir=Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/models/modified_model/comparison_posters'),\n",
    "    metadata_changes={\n",
    "        \"image_threshold\": {\"old\": ori_metadata.get('image_threshold', None), \"new\": new_metadata.image_threshold},\n",
    "        \"pred_score_min\": {\"old\": ori_metadata.get('pred_score_min', None), \"new\": new_metadata.pred_score_min},\n",
    "        \"pred_score_max\": {\"old\": ori_metadata.get('pred_score_max', None), \"new\": new_metadata.pred_score_max},\n",
    "        \"anomaly_map_min\": {\"old\": ori_metadata.get('anomaly_maps.min', None), \"new\": new_metadata.anomaly_map_min},\n",
    "        \"anomaly_map_max\": {\"old\": ori_metadata.get('anomaly_maps.max', None), \"new\": new_metadata.anomaly_map_max},\n",
    "        \"pixel_threshold\": {\"old\": ori_metadata.get('pixel_threshold', None), \"new\": new_metadata.pixel_threshold}\n",
    "    },\n",
    "    poster_rows=1,\n",
    "    poster_cols=2,\n",
    "    show_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ed305",
   "metadata": {},
   "source": [
    "# Step 5: all Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859589bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def modify_model_metadata_complete(\n",
    "    model_path: Union[str, Path], # Path to the input model\n",
    "    new_metadata: Union[ModelMetadata, Dict[str, Any]], # New metadata to apply\n",
    "    output_path: Optional[Union[str, Path]] = None, # Output path (default: same as input with suffix)\n",
    "    create_backup: bool = True, # Whether to create backup\n",
    "    create_poster: bool = False, # Whether to create comparison poster\n",
    "    test_images: Optional[Union[str, List[Union[str, Path]]]] = None, # Test images for poster\n",
    "    device: str = \"auto\", # Device for model operations\n",
    "    include_heatmap: bool = True, # Whether to include heatmap in poster\n",
    "    include_image_poster: bool = False, # Whether to include image poster\n",
    "    include_anomaly_poster: bool = False, # Whether to include anomaly poster\n",
    "    image_size_in_poster: Tuple[int, int] = (256, 256), # Size of images in poster\n",
    "    poster_rows: int = 3, # Number of poster rows\n",
    "    poster_cols: int = 3, # Number of poster columns\n",
    "    poster_title_prefix: str = \"Metadata Comparison\" # Title prefix for the poster\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete workflow to modify model metadata with optional poster creation.\n",
    "    \n",
    "    This is the main function that combines all operations:\n",
    "    1. Load model from any format (.pt, .ckpt, .onnx)\n",
    "    2. Extract current metadata\n",
    "    3. Apply new metadata values\n",
    "    4. Save modified model\n",
    "    5. Optionally create comparison poster\n",
    "    \n",
    "    Example:\n",
    "        ```python\n",
    "        # Modify threshold values\n",
    "        new_meta = ModelMetadata(\n",
    "            image_threshold=0.7,\n",
    "            pixel_threshold=0.5,\n",
    "            pred_score_min=0.0,\n",
    "            pred_score_max=1.0\n",
    "        )\n",
    "        \n",
    "        results = modify_model_metadata_complete(\n",
    "            model_path=\"model.pt\",\n",
    "            new_metadata=new_meta,\n",
    "            create_poster=True,\n",
    "            test_images=[\"test1.jpg\", \"test2.jpg\"] or \"test_image_folder\"\n",
    "        )\n",
    "        ```\n",
    "    \"\"\"\n",
    "    model_path = Path(model_path)\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    \n",
    "    if create_poster and not test_images:\n",
    "        raise ValueError(\"test_images required when create_poster=True\")\n",
    "    \n",
    "    # Generate output path if not provided\n",
    "    if output_path is None:\n",
    "        output_path = model_path.with_stem(f\"{model_path.stem}_modified\")\n",
    "    else:\n",
    "        output_path = Path(output_path)\n",
    "    \n",
    "    print(f\"ðŸš€ Starting complete metadata modification workflow...\")\n",
    "    print(f\"ðŸ“‚ Input model: {model_path.name}\")\n",
    "    print(f\"ðŸ’¾ Output model: {output_path.name}\")\n",
    "    \n",
    "    results = {\n",
    "        'input_model': str(model_path),\n",
    "        'output_model': str(output_path),\n",
    "        'success': False,\n",
    "        'operations': [],\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load model and extract current metadata\n",
    "        print(\"\\nðŸ“– Step 1: Loading model...\")\n",
    "        model_object, model_format, original_metadata = load_model_any_format(model_path, device)\n",
    "        results['original_metadata'] = original_metadata\n",
    "        results['model_format'] = model_format\n",
    "        results['operations'].append(\"model_loaded\")\n",
    "        \n",
    "        # Step 2: Modify metadata\n",
    "        print(\"\\\\nðŸ”§ Step 2: Modifying metadata...\")\n",
    "        modified_model, applied_metadata = modify_model_metadata(\n",
    "            model_object, model_format, new_metadata, validate_ranges=True\n",
    "        )\n",
    "        results['applied_metadata'] = applied_metadata\n",
    "        results['operations'].append(\"metadata_modified\")\n",
    "        \n",
    "        # Step 3: Save modified model\n",
    "        print(\"\\\\nðŸ’¾ Step 3: Saving modified model...\")\n",
    "        save_results = save_model_with_metadata(\n",
    "            modified_model, model_format, output_path, applied_metadata, create_backup\n",
    "        )\n",
    "        results.update(save_results)\n",
    "        results['operations'].append(\"model_saved\")\n",
    "        \n",
    "        print('Before step 4 check')\n",
    "        print(f'create_poster: {create_poster}, test_images: {test_images}')\n",
    "        # Step 4: Create poster if requested\n",
    "        if create_poster and test_images:\n",
    "            if isinstance(test_images, list):\n",
    "                test_images = [str(img) for img in test_images]\n",
    "            if isinstance(test_images, L):\n",
    "                test_images = [Path(img).as_posix() for img in test_images]\n",
    "            if isinstance(test_images, str):\n",
    "                test_images = [Path(i).as_posix() for i in Path(test_images).ls()]\n",
    "            print(\"\\\\nðŸŽ¨ Step 4: Creating comparison poster...\")\n",
    "            poster_dir = output_path.parent / \"posters\"\n",
    "            print(f\"{'#' * 10} poster dir: {poster_dir}\")\n",
    "            print(applied_metadata)\n",
    "            print(f\"{'#' * 10} poster dir: {poster_dir}\")\n",
    "            \n",
    "            # Build metadata changes dictionary using helper function\n",
    "            # This handles partial updates properly by falling back to original values\n",
    "            metadata_changes = build_metadata_changes_dict(\n",
    "                applied_metadata=applied_metadata,\n",
    "                original_metadata=original_metadata,\n",
    "                include_unchanged=True  # Include all values for complete comparison\n",
    "            )\n",
    "            \n",
    "            # Validate the metadata changes dictionary\n",
    "            validation_results = validate_metadata_changes_dict(\n",
    "                metadata_changes=metadata_changes,\n",
    "                allow_none_values=True  # Allow None values for missing metadata\n",
    "            )\n",
    "            \n",
    "            if not validation_results['valid']:\n",
    "                print(f\"âš ï¸  Metadata changes validation issues: {validation_results['issues']}\")\n",
    "            else:\n",
    "                print(f\"âœ… Metadata changes built successfully:\")\n",
    "                print(f\"   - Total parameters: {validation_results['total_changes']}\")\n",
    "                print(f\"   - Modified: {validation_results['modified_count']}\")\n",
    "                print(f\"   - Unchanged: {validation_results['unchanged_count']}\")\n",
    "            \n",
    "            poster_results = create_metadata_comparison_poster(\n",
    "                original_model_path=model_path,\n",
    "                modified_model_path=save_results['saved_model_path'],\n",
    "                test_images=test_images,\n",
    "                output_dir=poster_dir,\n",
    "                metadata_changes=metadata_changes,\n",
    "                poster_rows=poster_rows,\n",
    "                poster_cols=poster_cols,\n",
    "                show_plot=False,\n",
    "                image_size_in_poster=image_size_in_poster,\n",
    "                include_heatmap=include_heatmap,\n",
    "                include_image_poster=include_image_poster,\n",
    "                include_anomaly_poster=include_anomaly_poster,\n",
    "                poster_title_prefix=\"Metadata Comparison\"\n",
    "            )\n",
    "            results['poster_results'] = poster_results\n",
    "            results['operations'].append(\"poster_created\")\n",
    "        \n",
    "        results['success'] = True\n",
    "        print(f\"\\\\nðŸŽ‰ Complete workflow finished successfully!\")\n",
    "        print(f\"âœ… Operations completed: {', '.join(results['operations'])}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        results['success'] = False\n",
    "        print(f\"\\\\nâŒ Workflow failed: {str(e)}\")\n",
    "        raise RuntimeError(f\"Complete metadata modification failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d0438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b23d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d128ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export  \n",
    "def build_metadata_changes_dict(\n",
    "    applied_metadata: Union[ModelMetadata, Dict[str, Any]], # Applied/modified metadata\n",
    "    original_metadata: Dict[str, Any], # Original metadata from model\n",
    "    include_unchanged: bool = True # Whether to include unchanged values\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build a metadata changes dictionary that properly handles partial updates.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def get_metadata_value(\n",
    "        applied_meta, \n",
    "        original_meta, \n",
    "        key_mapping):\n",
    "        \"\"\"\n",
    "        Get metadata value from applied_metadata with fallback to original_metadata.\n",
    "        \n",
    "        \"\"\"\n",
    "        applied_key = key_mapping['applied_key']\n",
    "        original_key = key_mapping['original_key']\n",
    "        \n",
    "        # Try to get from applied metadata (handle both dict and object)\n",
    "        if hasattr(applied_meta, applied_key):\n",
    "            # Object-style access (for ModelMetadata objects)\n",
    "            new_value = getattr(applied_meta, applied_key, None)\n",
    "        elif isinstance(applied_meta, dict) and applied_key in applied_meta:\n",
    "            # Dict-style access\n",
    "            new_value = applied_meta[applied_key]\n",
    "        else:\n",
    "            # Fallback to original metadata value\n",
    "            new_value = original_meta.get(original_key, None)\n",
    "        \n",
    "        return new_value\n",
    "    \n",
    "    # Define mappings between applied metadata keys and original metadata keys\n",
    "    metadata_mappings = {\n",
    "        \"image_threshold\": {\n",
    "            'applied_key': 'image_threshold',\n",
    "            'original_key': 'image_threshold'\n",
    "        },\n",
    "        \"pred_score_min\": {\n",
    "            'applied_key': 'pred_score_min', \n",
    "            'original_key': 'pred_scores.min'\n",
    "        },\n",
    "        \"pred_score_max\": {\n",
    "            'applied_key': 'pred_score_max',\n",
    "            'original_key': 'pred_scores.max'\n",
    "        },\n",
    "        \"anomaly_map_min\": {\n",
    "            'applied_key': 'anomaly_map_min',\n",
    "            'original_key': 'anomaly_maps.min'\n",
    "        },\n",
    "        \"anomaly_map_max\": {\n",
    "            'applied_key': 'anomaly_map_max',\n",
    "            'original_key': 'anomaly_maps.max'\n",
    "        },\n",
    "        \"pixel_threshold\": {\n",
    "            'applied_key': 'pixel_threshold',\n",
    "            'original_key': 'pixel_threshold'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_changes = {}\n",
    "    \n",
    "    for change_key, mapping in metadata_mappings.items():\n",
    "        old_value = original_metadata.get(mapping['original_key'], None)\n",
    "        new_value = get_metadata_value(applied_metadata, original_metadata, mapping)\n",
    "        \n",
    "        # Include in result if changed or if include_unchanged is True\n",
    "        if include_unchanged or old_value != new_value:\n",
    "            metadata_changes[change_key] = {\n",
    "                \"old\": old_value,\n",
    "                \"new\": new_value\n",
    "            }\n",
    "    \n",
    "    return metadata_changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a53708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. test build_metadata_changes_dict with include_unchanged=False\n",
    "# 2. test validate_metadata_changes_dict with allow_none_values=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b05f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f62e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f35f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_metadata_changes_dict(\n",
    "    metadata_changes: Dict[str, Dict[str, Any]], # Metadata changes dictionary\n",
    "    allow_none_values: bool = False # Whether None values are acceptable\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate a metadata changes dictionary and return validation results.\n",
    "    \n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        'valid': True,\n",
    "        'total_changes': len(metadata_changes),\n",
    "        'modified_count': 0,\n",
    "        'unchanged_count': 0,\n",
    "        'none_value_count': 0,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    for param_name, change_data in metadata_changes.items():\n",
    "        # Check structure\n",
    "        if not isinstance(change_data, dict) or 'old' not in change_data or 'new' not in change_data:\n",
    "            validation_results['valid'] = False\n",
    "            validation_results['issues'].append(f\"Invalid structure for {param_name}\")\n",
    "            continue\n",
    "        \n",
    "        old_val = change_data['old']\n",
    "        new_val = change_data['new']\n",
    "        \n",
    "        # Count None values\n",
    "        if old_val is None or new_val is None:\n",
    "            validation_results['none_value_count'] += 1\n",
    "            if not allow_none_values:\n",
    "                validation_results['valid'] = False\n",
    "                validation_results['issues'].append(f\"None value found in {param_name}\")\n",
    "        \n",
    "        # Count changes\n",
    "        if old_val != new_val:\n",
    "            validation_results['modified_count'] += 1\n",
    "        else:\n",
    "            validation_results['unchanged_count'] += 1\n",
    "    \n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66e383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b388bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def modify_model_metadata_cli(\n",
    "    model_path: Union[str, Path]=None, # Path to the input model\n",
    "    output_path: Optional[Union[str, Path]] = None, # Output path (default: same as input with suffix)\n",
    "    config_file: str = None, # Optional config file (JSON/YAML) with metadata parameters\n",
    "    # Individual metadata parameters (CLI-friendly) - ignored if config_file is provided\n",
    "    image_threshold: Optional[float] = None, # Image-level anomaly threshold\n",
    "    pixel_threshold: Optional[float] = None, # Pixel-level anomaly threshold\n",
    "    pred_score_min: Optional[float] = None, # Prediction score minimum for normalization\n",
    "    pred_score_max: Optional[float] = None, # Prediction score maximum for normalization\n",
    "    anomaly_map_min: Optional[float] = None, # Anomaly map minimum for normalization\n",
    "    anomaly_map_max: Optional[float] = None, # Anomaly map maximum for normalization\n",
    "    # General options\n",
    "    create_backup: bool = False, # Whether to create backup\n",
    "    create_poster: bool = True, # Whether to create comparison poster\n",
    "    test_images_folder: Optional[Union[str, Path]] = None, # Folder containing test images\n",
    "    image_extensions: List[str] = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff'], # Valid image extensions\n",
    "    max_test_images: int = 20, # Maximum number of test images to use\n",
    "    device: str = \"auto\", # Device for model operations\n",
    "    include_heatmap: bool = True, # Whether to include heatmap in poster\n",
    "    include_image_poster: bool = False, # Whether to include image poster\n",
    "    include_anomaly_poster: bool = False, # Whether to include anomaly poster\n",
    "    image_size_in_poster_width: int = 256, # Width of images in poster\n",
    "    image_size_in_poster_height: int = 256, # Height of images in poster\n",
    "    poster_rows: int = 3, # Number of poster rows\n",
    "    poster_cols: int = 3, # Number of poster columns\n",
    "    poster_title_prefix: str = \"Metadata Comparison\" # Title prefix for the poster\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Command-line interface wrapper for model metadata modification.\n",
    "    Example:\n",
    "        ```python\n",
    "        # Method 1: CLI-style usage with individual parameters\n",
    "        results = modify_model_metadata_cli(\n",
    "            model_path=\"model.pt\",\n",
    "            image_threshold=0.7,\n",
    "            pixel_threshold=0.5,\n",
    "            pred_score_max=2.0,\n",
    "            create_poster=True,\n",
    "            test_images_folder=\"test_images\"\n",
    "        )\n",
    "        \n",
    "        # Method 2: Using config file\n",
    "        results = modify_model_metadata_cli(\n",
    "            model_path=\"model.pt\",\n",
    "            config_file=\"metadata_config.json\",\n",
    "            create_poster=True,\n",
    "            test_images_folder=\"test_images\"\n",
    "        )\n",
    "        ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle config file loading if provided\n",
    "    if config_file:\n",
    "        config_file = Path(config_file)\n",
    "        if not config_file.exists():\n",
    "            raise FileNotFoundError(f\"Config file not found: {config_file}\")\n",
    "        \n",
    "        print(f\"ðŸ“„ Loading configuration from: {config_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load config based on file extension\n",
    "            if config_file.suffix.lower() == '.json':\n",
    "                import json\n",
    "                with open(config_file, 'r') as f:\n",
    "                    config_data = json.load(f)\n",
    "            elif config_file.suffix.lower() in ['.yaml', '.yml']:\n",
    "                try:\n",
    "                    import yaml\n",
    "                    with open(config_file, 'r') as f:\n",
    "                        config_data = yaml.safe_load(f)\n",
    "                except ImportError:\n",
    "                    raise ImportError(\"PyYAML is required to load YAML config files. Install with: pip install pyyaml\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported config file format: {config_file.suffix}. Use .json or .yaml/.yml\")\n",
    "\n",
    "\n",
    "            #Creating first default and then overwriting with cli values\n",
    "            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "\n",
    "\n",
    "             # Extract model_path from config (REQUIRED)\n",
    "            if 'model_path' not in config_data or config_data['model_path'] is None:\n",
    "                raise ValueError(\"model_path must be specified in the config file\")\n",
    "            model_path = config_data['model_path']\n",
    "            \n",
    "            # Extract output_path from config (optional)\n",
    "            output_path = config_data.get('output_path', None)\n",
    "            \n",
    "            # Extract metadata from config\n",
    "            metadata_section = config_data.get('metadata', config_data)  # Allow 'metadata' section or root level\n",
    "            \n",
    "            # Build metadata dictionary from config\n",
    "            new_metadata = {}\n",
    "            metadata_keys = ['image_threshold', 'pixel_threshold', 'pred_score_min', \n",
    "                           'pred_score_max', 'anomaly_map_min', 'anomaly_map_max']\n",
    "            \n",
    "            for key in metadata_keys:\n",
    "                if key in metadata_section and metadata_section[key] is not None:\n",
    "                    new_metadata[key] = float(metadata_section[key])\n",
    "            \n",
    "            # Override other settings from config if present\n",
    "            if 'create_backup' in config_data:\n",
    "                create_backup = bool(config_data['create_backup'])\n",
    "            if 'create_poster' in config_data:\n",
    "                create_poster = bool(config_data['create_poster'])\n",
    "            if 'test_images_folder' in config_data:\n",
    "                test_images_folder = config_data['test_images_folder']\n",
    "            if 'max_test_images' in config_data:\n",
    "                max_test_images = int(config_data['max_test_images'])\n",
    "            if 'device' in config_data:\n",
    "                device = str(config_data['device'])\n",
    "\n",
    "            if 'image_extensions' in config_data:\n",
    "                image_extensions = list(config_data['image_extensions'])\n",
    "            \n",
    "            # Poster settings from config\n",
    "            if 'poster_settings' in config_data:\n",
    "                poster_cfg = config_data['poster_settings']\n",
    "                if 'include_heatmap' in poster_cfg:\n",
    "                    include_heatmap = bool(poster_cfg['include_heatmap'])\n",
    "                if 'include_image_poster' in poster_cfg:\n",
    "                    include_image_poster = bool(poster_cfg['include_image_poster'])\n",
    "                if 'include_anomaly_poster' in poster_cfg:\n",
    "                    include_anomaly_poster = bool(poster_cfg['include_anomaly_poster'])\n",
    "                if 'image_size_width' in poster_cfg:\n",
    "                    image_size_in_poster_width = int(poster_cfg['image_size_width'])\n",
    "                if 'image_size_height' in poster_cfg:\n",
    "                    image_size_in_poster_height = int(poster_cfg['image_size_height'])\n",
    "                if 'rows' in poster_cfg:\n",
    "                    poster_rows = int(poster_cfg['rows'])\n",
    "                if 'cols' in poster_cfg:\n",
    "                    poster_cols = int(poster_cfg['cols'])\n",
    "                if 'title_prefix' in poster_cfg:\n",
    "                    poster_title_prefix = str(poster_cfg['title_prefix'])\n",
    "            \n",
    "            print(f\"âœ… Config loaded: {len(new_metadata)} metadata parameters\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load config file {config_file}: {str(e)}\")\n",
    "    \n",
    "    else:\n",
    "        # Build metadata dictionary from individual parameters\n",
    "        metadata_params = {\n",
    "            'image_threshold': image_threshold,\n",
    "            'pixel_threshold': pixel_threshold,\n",
    "            'pred_score_min': pred_score_min,\n",
    "            'pred_score_max': pred_score_max,\n",
    "            'anomaly_map_min': anomaly_map_min,\n",
    "            'anomaly_map_max': anomaly_map_max\n",
    "        }\n",
    "        \n",
    "        # Filter out None values to create final metadata\n",
    "        new_metadata = {k: v for k, v in metadata_params.items() if v is not None}\n",
    "        print(f\"ðŸ”§ CLI metadata parameters: {new_metadata}\")\n",
    "    \n",
    "    # Validate that at least one metadata parameter was provided\n",
    "    if not new_metadata:\n",
    "        raise ValueError(\n",
    "            \"At least one metadata parameter must be specified either as individual parameters or in config file: \"\n",
    "            \"image_threshold, pixel_threshold, pred_score_min, pred_score_max, \"\n",
    "            \"anomaly_map_min, or anomaly_map_max\"\n",
    "        )\n",
    "    \n",
    "    # Validate inputs\n",
    "    if create_poster and not test_images_folder:\n",
    "        raise ValueError(\"test_images_folder is required when create_poster=True\")\n",
    "    \n",
    "    test_images = None\n",
    "    \n",
    "    # Process test images folder if provided\n",
    "    if test_images_folder:\n",
    "        test_images_folder = Path(test_images_folder)\n",
    "        \n",
    "        if not test_images_folder.exists():\n",
    "            raise FileNotFoundError(f\"Test images folder not found: {test_images_folder}\")\n",
    "        \n",
    "        if not test_images_folder.is_dir():\n",
    "            raise ValueError(f\"test_images_folder must be a directory: {test_images_folder}\")\n",
    "        \n",
    "        print(f\"ðŸ” Scanning test images folder: {test_images_folder}\")\n",
    "        \n",
    "        # Find all image files in the folder\n",
    "        test_images = []\n",
    "        for ext in image_extensions:\n",
    "            # Case-insensitive search\n",
    "            pattern_lower = f\"*{ext.lower()}\"\n",
    "            pattern_upper = f\"*{ext.upper()}\"\n",
    "            \n",
    "            test_images.extend(test_images_folder.glob(pattern_lower))\n",
    "            test_images.extend(test_images_folder.glob(pattern_upper))\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        test_images = sorted(list(set(test_images)))\n",
    "        \n",
    "        # Limit number of images\n",
    "        if len(test_images) > max_test_images:\n",
    "            print(f\"âš ï¸  Found {len(test_images)} images, limiting to {max_test_images}\")\n",
    "            test_images = test_images[:max_test_images]\n",
    "        \n",
    "        if not test_images:\n",
    "            print(f\"âš ï¸  No valid images found in {test_images_folder}\")\n",
    "            print(f\"   Supported extensions: {image_extensions}\")\n",
    "            if create_poster:\n",
    "                raise ValueError(f\"No test images found in {test_images_folder}\")\n",
    "        else:\n",
    "            print(f\"âœ… Found {len(test_images)} test images\")\n",
    "    \n",
    "    # Convert separate width/height to tuple\n",
    "    image_size_in_poster = (image_size_in_poster_width, image_size_in_poster_height)\n",
    "    \n",
    "    # Call the main function with processed parameters\n",
    "    print(f\"ðŸš€ Calling modify_model_metadata_complete with processed parameters...\")\n",
    "    \n",
    "    return modify_model_metadata_complete(\n",
    "        model_path=model_path,\n",
    "        new_metadata=new_metadata,\n",
    "        output_path=output_path,\n",
    "        create_backup=create_backup,\n",
    "        create_poster=create_poster,\n",
    "        test_images=test_images,\n",
    "        device=device,\n",
    "        include_heatmap=include_heatmap,\n",
    "        include_image_poster=include_image_poster,\n",
    "        include_anomaly_poster=include_anomaly_poster,\n",
    "        image_size_in_poster=image_size_in_poster,\n",
    "        poster_rows=poster_rows,\n",
    "        poster_cols=poster_cols,\n",
    "        poster_title_prefix=poster_title_prefix\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71ce97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/models/exports/TEST_MULITNODE_task_000_padim_resnet18_18_layer1/weights/torch/model.pt')\n",
    "test_images = Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/images/bad')\n",
    "modify_model_metadata_cli(\n",
    "    model_path=model_path,\n",
    "    image_threshold=10,\n",
    "    pixel_threshold=10,\n",
    "    pred_score_max=10.0,\n",
    "    anomaly_map_min=0,\n",
    "    anomaly_map_max=12.0,\n",
    "    create_poster=True,\n",
    "    test_images_folder=test_images,\n",
    "    poster_rows=1,\n",
    "    poster_cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c2a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path= Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/model_metadata_change.yaml')\n",
    "yaml_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(yaml_path, 'r') as f:\n",
    "    yaml_data = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705884f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res= modify_model_metadata_cli(\n",
    "    config_file=yaml_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d05e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_metadata_config_template(\n",
    "    output_path: Union[str, Path], # Path to save the config template\n",
    "    format_type: str = \"json\", # Config format (\"json\" or \"yaml\")\n",
    "    include_example_values: bool = True # Whether to include example values\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a template configuration file for metadata modification.\n",
    "    \n",
    "    Args:\n",
    "        output_path: Path where to save the config template\n",
    "        format_type: Format of config file (\"json\" or \"yaml\")  \n",
    "        include_example_values: Whether to include example values or null/None\n",
    "        \n",
    "    Example:\n",
    "        ```python\n",
    "        # Create JSON template\n",
    "        create_metadata_config_template(\n",
    "            \"metadata_config.json\", \n",
    "            format_type=\"json\",\n",
    "            include_example_values=True\n",
    "        )\n",
    "        \n",
    "        # Create YAML template\n",
    "        create_metadata_config_template(\n",
    "            \"metadata_config.yaml\",\n",
    "            format_type=\"yaml\", \n",
    "            include_example_values=False\n",
    "        )\n",
    "        ```\n",
    "    \"\"\"\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    # Template configuration data\n",
    "    if include_example_values:\n",
    "        config_data = {\n",
    "            \"metadata\": {\n",
    "                \"image_threshold\": 0.7,\n",
    "                \"pixel_threshold\": 0.5,\n",
    "                \"pred_score_min\": 0.0,\n",
    "                \"pred_score_max\": 2.0,\n",
    "                \"anomaly_map_min\": 0.0,\n",
    "                \"anomaly_map_max\": 10.0\n",
    "            },\n",
    "            \"create_backup\": True,\n",
    "            \"create_poster\": True,\n",
    "            \"test_images_folder\": \"test_images\",\n",
    "            \"max_test_images\": 20,\n",
    "            \"device\": \"auto\",\n",
    "            \"poster_settings\": {\n",
    "                \"include_heatmap\": True,\n",
    "                \"include_image_poster\": False,\n",
    "                \"include_anomaly_poster\": False,\n",
    "                \"image_size_width\": 256,\n",
    "                \"image_size_height\": 256,\n",
    "                \"rows\": 3,\n",
    "                \"cols\": 3,\n",
    "                \"title_prefix\": \"Metadata Comparison\"\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        config_data = {\n",
    "            \"metadata\": {\n",
    "                \"image_threshold\": None,\n",
    "                \"pixel_threshold\": None,\n",
    "                \"pred_score_min\": None,\n",
    "                \"pred_score_max\": None,\n",
    "                \"anomaly_map_min\": None,\n",
    "                \"anomaly_map_max\": None\n",
    "            },\n",
    "            \"create_backup\": True,\n",
    "            \"create_poster\": False,\n",
    "            \"test_images_folder\": None,\n",
    "            \"max_test_images\": 20,\n",
    "            \"device\": \"auto\",\n",
    "            \"poster_settings\": {\n",
    "                \"include_heatmap\": True,\n",
    "                \"include_image_poster\": False,\n",
    "                \"include_anomaly_poster\": False,\n",
    "                \"image_size_width\": 256,\n",
    "                \"image_size_height\": 256,\n",
    "                \"rows\": 3,\n",
    "                \"cols\": 3,\n",
    "                \"title_prefix\": \"Metadata Comparison\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Save config file\n",
    "    try:\n",
    "        if format_type.lower() == \"json\":\n",
    "            import json\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(config_data, f, indent=2)\n",
    "            print(f\"âœ… JSON config template saved to: {output_path}\")\n",
    "            \n",
    "        elif format_type.lower() in [\"yaml\", \"yml\"]:\n",
    "            try:\n",
    "                import yaml\n",
    "                with open(output_path, 'w') as f:\n",
    "                    yaml.dump(config_data, f, default_flow_style=False, indent=2)\n",
    "                print(f\"âœ… YAML config template saved to: {output_path}\")\n",
    "            except ImportError:\n",
    "                raise ImportError(\"PyYAML is required to create YAML config files. Install with: pip install pyyaml\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format_type}. Use 'json' or 'yaml'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to create config template: {str(e)}\")\n",
    "\n",
    "\n",
    "def load_and_validate_config(config_path: Union[str, Path]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load and validate a configuration file.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the configuration file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validated configuration data\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If config file doesn't exist\n",
    "        ValueError: If config format is invalid\n",
    "    \"\"\"\n",
    "    config_path = Path(config_path)\n",
    "    \n",
    "    if not config_path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "    \n",
    "    print(f\"ðŸ“„ Loading and validating config: {config_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load based on extension\n",
    "        if config_path.suffix.lower() == '.json':\n",
    "            import json\n",
    "            with open(config_path, 'r') as f:\n",
    "                config_data = json.load(f)\n",
    "        elif config_path.suffix.lower() in ['.yaml', '.yml']:\n",
    "            try:\n",
    "                import yaml\n",
    "                with open(config_path, 'r') as f:\n",
    "                    config_data = yaml.safe_load(f)\n",
    "            except ImportError:\n",
    "                raise ImportError(\"PyYAML required for YAML files. Install with: pip install pyyaml\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported config format: {config_path.suffix}\")\n",
    "        \n",
    "        # Validate required structure\n",
    "        metadata_section = config_data.get('metadata', {})\n",
    "        valid_metadata_keys = {\n",
    "            'image_threshold', 'pixel_threshold', 'pred_score_min',\n",
    "            'pred_score_max', 'anomaly_map_min', 'anomaly_map_max'\n",
    "        }\n",
    "        \n",
    "        # Check if at least one metadata parameter is provided\n",
    "        provided_metadata = {k: v for k, v in metadata_section.items() \n",
    "                           if k in valid_metadata_keys and v is not None}\n",
    "        \n",
    "        if not provided_metadata:\n",
    "            raise ValueError(\"Config must contain at least one valid metadata parameter\")\n",
    "        \n",
    "        print(f\"âœ… Config validation passed: {len(provided_metadata)} metadata parameters found\")\n",
    "        \n",
    "        return config_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load/validate config: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32645b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir('/home/ai_dsx.work/data/projects/be-vision-ad-tools/nbs')\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f4cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f944aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('09_postprocessing.model_metadata.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177db567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
