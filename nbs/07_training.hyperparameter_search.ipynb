{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2eff5d7",
   "metadata": {},
   "source": [
    "# Heatmap comparison\n",
    "> Compare heatmap for different training parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3922ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.hyperparameter_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59042f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea411dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f12f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "custom_lib_path = Path(r'/home/ai_warstein/homes/goni/custom_libs')\n",
    "sys.path.append(str(custom_lib_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from be_vision_ad_tools.inference.prediction_system import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984466cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cv_tools.imports import *\n",
    "#from cv_tools.core import *\n",
    "#from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_dotenv(dotenv_path=f'/home/ai_dsx.work/data/projects/be-vision-ad-tools/be-vision-ad-tools/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20d33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "CURRETNT_NB='/home/ai_dsx.work/data/projects/be-vision-ad-tools/nbs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e9222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f94ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Import our training functions\n",
    "from be_vision_ad_tools.training.flexible_trainer import (\n",
    "    FlexibleTrainingConfig, train_anomaly_model, ModelType, BackboneType\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde844bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Dict, Any, Union, Optional, Tuple\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1\n",
    "data_root = \"/home/ai_dsx.work/data/projects/AD_tool_test/images\"\n",
    "normal_dir = \"good\"\n",
    "abnormal_dir = \"bad\"\n",
    "class_name = \"hyperparam_search\"\n",
    "save_path = Path(data_root) / 'hyperparameter_models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['padim']\n",
    "backbones = ['resnet18', 'resnet50']\n",
    "n_features_list = [100]\n",
    "layers = [['layer1'], ['layer2'], ['layer1', 'layer2']]  # Different layer combinations\n",
    "param_combinations = list(product(model_names, backbones, n_features_list, layers))\n",
    "for i, (model_name, backbone, n_features, layer) in enumerate(param_combinations, 1):\n",
    "    #print(f\"\\nüîÑ Testing combination {i}/{len(param_combinations)}\")\n",
    "    #print(f\"   Model: {model_name}\")\n",
    "    #print(f\"   Backbone: {backbone}\")\n",
    "    #print(f\"   Features: {n_features}\")\n",
    "    #print(f\"   Layer: {layer}\")\n",
    "    #print(\"-\" * 50)\n",
    "    \n",
    "    # Create unique class name based on layer configuration\n",
    "    if len(layer) == 1:\n",
    "        layer_str = layer[0]\n",
    "    else:\n",
    "        layer_str = '+'.join(layer)\n",
    "    \n",
    "    unique_class_name = f\"{class_name}_{model_name}_{backbone}_{n_features}feat_{layer_str}\"\n",
    "    print(unique_class_name)\n",
    "\n",
    "    #config = FlexibleTrainingConfig(\n",
    "            #data_root=data_root,\n",
    "            #normal_dir=normal_dir,\n",
    "            #abnormal_dir=abnormal_dir,\n",
    "            #class_name=unique_class_name,\n",
    "            #model_name=model_name,\n",
    "            #backbone=backbone,\n",
    "            #n_features=n_features,\n",
    "            #layers=layer,  # Pass as list\n",
    "            #max_epochs=max_epochs,\n",
    "            #save_path=save_path / unique_class_name,\n",
    "            #train_batch_size=8,  # Small batch for quick testing\n",
    "            #eval_batch_size=8,\n",
    "            #num_workers=0,  # Safe for notebooks\n",
    "        #)\n",
    "            \n",
    "    ## Train the model\n",
    "    #training_result = train_anomaly_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7737e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def simple_hyperparameter_search(\n",
    "    data_root: Union[str, Path],  # Root directory containing normal and abnormal subdirectories\n",
    "    normal_dir: str = \"good\",     # Name of normal images subdirectory  \n",
    "    abnormal_dir: str = \"bad\",    # Name of abnormal images subdirectory\n",
    "    class_name: str = \"hyperparam_search\",  # Base class name for experiments\n",
    "    \n",
    "    # Parameter combinations to test\n",
    "    model_names: List[str] = None,        # List of models to test ['padim', 'patchcore']\n",
    "    backbones: List[str] = None,          # List of backbones to test ['resnet18', 'resnet50']\n",
    "    n_features_list: List[int] = None,    # List of n_features to test [100, 200]\n",
    "    layers: List[str] = None,             # List of layers to test ['layer1', 'layer2', 'all']\n",
    "    \n",
    "    # Training settings\n",
    "    max_epochs: int = 1,                  # Keep low for quick testing\n",
    "    save_path: Union[str, Path] = None,  # Base path for model saves\n",
    "    \n",
    "    # Output settings\n",
    "    output_folder: Union[str, Path] = None,  # Results folder\n",
    "    \n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simple hyperparameter grid search that tests different parameter combinations.\n",
    "    \n",
    "    Starts very simple - tests a few combinations and reports results.\n",
    "    Perfect for building up to more complex comparison systems.\n",
    "    \"\"\"\n",
    "\n",
    "    if output_folder is None:\n",
    "        output_folder = Path(data_root) / 'hyperparameter_results'\n",
    "        Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    if save_path is None:\n",
    "        save_path = Path(data_root) / 'hyperparameter_models'\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"üîç HYPERPARAMETER SEARCH - STEP 1: SIMPLE GRID SEARCH\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Set defaults if none provided\n",
    "    if model_names is None:\n",
    "        model_names = ['padim']  # Start with padim\n",
    "    if backbones is None:\n",
    "        backbones = ['resnet18', 'resnet50']  # Start with 2 backbones\n",
    "    if n_features_list is None:\n",
    "        n_features_list = [64]  # Keep simple for now\n",
    "    if layers is None:\n",
    "        layers = [['layer1']]  # Start with layer1\n",
    "        \n",
    "    # Create output folders\n",
    "    output_folder = Path(output_folder)\n",
    "    save_path = Path(save_path)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Data: {data_root}\")\n",
    "    print(f\"üìÅ Output: {output_folder}\")\n",
    "    print(f\"üíæ Models: {save_path}\")\n",
    "    print(f\"üéØ Testing: {len(model_names)} models √ó {len(backbones)} backbones √ó {len(n_features_list)} features √ó {len(layers)} layers\")\n",
    "    \n",
    "    # Generate all parameter combinations including layers\n",
    "    param_combinations = list(product(model_names, backbones, n_features_list, layers))\n",
    "    total_combinations = len(param_combinations)\n",
    "    \n",
    "    print(f\"üß™ Total combinations to test: {total_combinations}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Store results for each combination\n",
    "    all_results = []\n",
    "    successful_trainings = 0\n",
    "    failed_trainings = 0\n",
    "    \n",
    "    # Test each combination\n",
    "    for i, (model_name, backbone, n_features, layer) in enumerate(param_combinations, 1):\n",
    "        \n",
    "        print(f\"\\nüîÑ Testing combination {i}/{total_combinations}\")\n",
    "        print(f\"   Model: {model_name}\")\n",
    "        print(f\"   Backbone: {backbone}\")  \n",
    "        print(f\"   Features: {n_features}\")\n",
    "        print(f\"   Layer: {layer}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create unique class name for this combination\n",
    "        # also for num_layers remove ['layer1'] -> layer1\n",
    "\n",
    "        if len(layer) == 1:\n",
    "            layer_str = layer[0]\n",
    "        else:\n",
    "            layer_str = '+'.join(layer)\n",
    "    \n",
    "        unique_class_name = f\"{class_name}_{model_name}_{backbone}_{n_features}feat_{layer_str}\"\n",
    "\n",
    "        \n",
    "        # Create training config\n",
    "        try:\n",
    "            config = FlexibleTrainingConfig(\n",
    "                data_root=data_root,\n",
    "                normal_dir=normal_dir,\n",
    "                abnormal_dir=abnormal_dir,\n",
    "                class_name=unique_class_name,\n",
    "                model_name=model_name,\n",
    "                backbone=backbone,\n",
    "                n_features=n_features,\n",
    "                layers=layer,  # Add layer parameter\n",
    "                max_epochs=max_epochs,\n",
    "                save_path=save_path / unique_class_name,\n",
    "                train_batch_size=8,  # Small batch for quick testing\n",
    "                eval_batch_size=8,\n",
    "                num_workers=0,  # Safe for notebooks\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            training_result = train_anomaly_model(config)\n",
    "            \n",
    "            # Calculate training time\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Store results\n",
    "            result_entry = {\n",
    "                'combination_id': i,\n",
    "                'model_name': model_name,\n",
    "                'backbone': backbone,\n",
    "                'n_features': n_features,\n",
    "                'layers': layer,\n",
    "                'class_name': unique_class_name,\n",
    "                'training_time_seconds': round(training_time, 2),\n",
    "                'success': training_result.get('success', False),\n",
    "                'model_path': training_result.get('export_paths', {}),\n",
    "                'exported_models': training_result.get('export_paths', {}),\n",
    "                'config_dict': config.to_dict(),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Add training metrics if available\n",
    "            if 'training_metrics' in training_result:\n",
    "                result_entry['training_metrics'] = training_result['training_metrics']\n",
    "                \n",
    "            if training_result.get('success', False):\n",
    "                successful_trainings += 1\n",
    "                print(f\"   ‚úÖ SUCCESS in {training_time:.1f}s\")\n",
    "                if 'model_path' in training_result:\n",
    "                    print(f\"   üíæ Model saved: {training_result['model_path']}\")\n",
    "            else:\n",
    "                failed_trainings += 1\n",
    "                result_entry['error'] = training_result.get('error', 'Unknown error')\n",
    "                print(f\"   ‚ùå FAILED: {training_result.get('error', 'Unknown error')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_trainings += 1\n",
    "            training_time = time.time() - start_time\n",
    "            result_entry = {\n",
    "                'combination_id': i,\n",
    "                'model_name': model_name,\n",
    "                'backbone': backbone,\n",
    "                'n_features': n_features,\n",
    "                'layers': layer,\n",
    "                'class_name': unique_class_name,\n",
    "                'training_time_seconds': round(training_time, 2),\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            print(f\"   ‚ùå EXCEPTION: {str(e)}\")\n",
    "            \n",
    "        all_results.append(result_entry)\n",
    "    \n",
    "    # Create summary results\n",
    "    summary = {\n",
    "        'total_combinations': total_combinations,\n",
    "        'successful_trainings': successful_trainings,\n",
    "        'failed_trainings': failed_trainings,\n",
    "        'success_rate': round(successful_trainings / total_combinations * 100, 1) if total_combinations > 0 else 0,\n",
    "        'search_completed_at': datetime.now().isoformat(),\n",
    "        'results': all_results\n",
    "    }\n",
    "    \n",
    "    # Save results to JSON\n",
    "    results_file = output_folder / f\"hyperparameter_search_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä HYPERPARAMETER SEARCH COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"‚úÖ Successful trainings: {successful_trainings}/{total_combinations}\")\n",
    "    print(f\"‚ùå Failed trainings: {failed_trainings}/{total_combinations}\")\n",
    "    print(f\"üìà Success rate: {summary['success_rate']}%\")\n",
    "    print(f\"üíæ Results saved: {results_file}\")\n",
    "    \n",
    "    # Show successful models\n",
    "    successful_models = [r for r in all_results if r['success']]\n",
    "    if successful_models:\n",
    "        print(f\"\\nüéØ Successfully trained models:\")\n",
    "        for result in successful_models:\n",
    "            time_str = f\"{result['training_time_seconds']}s\"\n",
    "            print(f\"   ‚Ä¢ {result['model_name']} + {result['backbone']} ({result['n_features']} feat, {result['layers']}) - {time_str}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Ready for Step 2: Creating posters for each model!\")\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/home/ai_dsx.work/data/projects/AD_tool_test/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12053462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Step 1: Simple Hyperparameter Search\n",
    "\n",
    "print(\"üß™ TESTING STEP 1: Simple Hyperparameter Search\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# For quick testing, let's use just 2 simple combinations\n",
    "test_results = simple_hyperparameter_search(\n",
    "    data_root=\"/home/ai_dsx.work/data/projects/AD_tool_test/images\",  # Replace with your data path\n",
    "    normal_dir=\"good\",\n",
    "    abnormal_dir=\"bad\", \n",
    "    class_name=\"test_hyperparam\",\n",
    "    \n",
    "    # Start simple - just test 2 combinations\n",
    "    model_names=['padim'],           # Just 1 model for quick test\n",
    "    backbones=['resnet18'],          # Just 1 backbone for quick test\n",
    "    n_features_list=[10],           # Just 1 feature count\n",
    "    layers=[['layer1'], ['layer2']],\n",
    "    \n",
    "    max_epochs=1,                    # Very quick training\n",
    "    output_folder=f\"{DATA_ROOT}/test_hyperparameter_results\",\n",
    "    save_path=f\"{DATA_ROOT}/test_hyperparameter_models\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Step 1 Test Complete!\")\n",
    "print(f\"Success rate: {test_results['success_rate']}%\")\n",
    "print(f\"Total models tested: {test_results['total_combinations']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e381d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = \"/home/ai_dsx.work/data/projects/AD_tool_test/images/bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51252e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_search_results(\n",
    "    search_results: Dict[str, Any]  # Results from simple_hyperparameter_search\n",
    ") -> bool:\n",
    "    \"\"\"Validate that search results contain required data.\"\"\"\n",
    "    if not isinstance(search_results, dict):\n",
    "        return False\n",
    "    if 'results' not in search_results:\n",
    "        return False\n",
    "    if not isinstance(search_results['results'], list):\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_successful_models(\n",
    "    search_results: Dict[str, Any],  # Results from simple_hyperparameter_search\n",
    "    max_models: int  # Maximum number of models to extract\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract successful models from search results.\"\"\"\n",
    "    if not validate_search_results(search_results):\n",
    "        return []\n",
    "    \n",
    "    successful_models = [\n",
    "        r for r in search_results['results'] \n",
    "        if r.get('success', False)\n",
    "    ][:max_models]\n",
    "    \n",
    "    print(f\"üìä Extracted {len(successful_models)} successful models (max: {max_models})\")\n",
    "    return successful_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ce9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "s_md = extract_successful_models(\n",
    "    test_results,\n",
    "    max_models=10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca987e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def collect_image_paths(\n",
    "    test_images: Union[str, Path, List]  # Test images (directory, single file, or list)\n",
    ") -> List[Path]:\n",
    "    \"\"\"Collect and validate image paths from various input formats.\"\"\"\n",
    "    if isinstance(test_images, (str, Path)):\n",
    "        test_images_path = Path(test_images)\n",
    "        if test_images_path.is_dir():\n",
    "            # Get common image extensions\n",
    "            image_paths = []\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']:\n",
    "                image_paths.extend(list(test_images_path.glob(ext)))\n",
    "                image_paths.extend(list(test_images_path.glob(ext.upper())))\n",
    "            return sorted(image_paths)  # Sort for consistent ordering\n",
    "        else:\n",
    "            return [test_images_path] if test_images_path.exists() else []\n",
    "    elif isinstance(test_images, list):\n",
    "        return [Path(p) for p in test_images if Path(p).exists()]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff930325",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_paths = collect_image_paths(test_image_path)\n",
    "im_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945731ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def limit_test_images(\n",
    "    image_paths: List[Path],  # List of image paths\n",
    "    max_test_images: int  # Maximum number to keep\n",
    ") -> List[Path]:\n",
    "    \"\"\"Limit the number of test images and provide feedback.\"\"\"\n",
    "    if len(image_paths) > max_test_images:\n",
    "        print(f\"‚ö†Ô∏è  Found {len(image_paths)} test images, limiting to first {max_test_images}\")\n",
    "        return image_paths[:max_test_images]\n",
    "    \n",
    "    print(f\"üñºÔ∏è  Using {len(image_paths)} test images\")\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "image_paths = limit_test_images(im_paths, max_test_images=10)\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_torch_model_file(\n",
    "    model_result: Dict[str, Any]  # Model result from search results\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"Find the torch model file path using multiple search strategies.\"\"\"\n",
    "    torch_model_path = None\n",
    "    \n",
    "    # Strategy 1: Try exported models first\n",
    "    if 'exported_models' in model_result and model_result['exported_models']:\n",
    "        exported = model_result['exported_models']\n",
    "        if 'torch' in exported:\n",
    "            torch_model_path = Path(exported['torch'])\n",
    "        elif 'pytorch' in exported:\n",
    "            torch_model_path = Path(exported['pytorch'])\n",
    "    \n",
    "    # Strategy 2: Fallback to standard model directory structure\n",
    "    if not torch_model_path or not torch_model_path.exists():\n",
    "        model_dir = Path(model_result.get('model_path', ''))\n",
    "        search_paths = [\n",
    "            model_dir / \"weights\" / \"torch\" / \"model.pt\",\n",
    "            model_dir / \"weights\" / \"pytorch\" / \"model.pt\", \n",
    "            model_dir / \"model.pt\",\n",
    "            model_dir / \"model.pth\",\n",
    "        ]\n",
    "        for path in search_paths:\n",
    "            if path.exists():\n",
    "                torch_model_path = path\n",
    "                break\n",
    "    \n",
    "    return torch_model_path if torch_model_path and torch_model_path.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ceaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_torch_model_file(\n",
    "    model_result: Dict[str, Any]  # Model result from search results\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"Find the torch model file path using multiple search strategies.\"\"\"\n",
    "    torch_model_path = None\n",
    "    \n",
    "    # Strategy 1: Try exported models first\n",
    "    if 'exported_models' in model_result and model_result['exported_models']:\n",
    "        exported = model_result['exported_models']\n",
    "        if 'torch' in exported:\n",
    "            torch_model_path = Path(exported['torch'])\n",
    "        elif 'pytorch' in exported:\n",
    "            torch_model_path = Path(exported['pytorch'])\n",
    "    \n",
    "    # Strategy 2: Handle multinode results where model_path might be None\n",
    "    if not torch_model_path or not torch_model_path.exists():\n",
    "        model_path = model_result.get('model_path')\n",
    "        \n",
    "        # For multinode results, construct path from config_used or task information\n",
    "        if model_path is None and 'config_used' in model_result:\n",
    "            config = model_result['config_used']\n",
    "            if 'save_path' in config:\n",
    "                model_path_ = config['save_path']\n",
    "                class_name = config['class_name']\n",
    "                model_path = Path(model_path_, 'exports', class_name)\n",
    "            elif 'task_output_base' in config:\n",
    "                model_path = str(Path(config['task_output_base']) / 'model')\n",
    "        \n",
    "        # Skip if still no model path\n",
    "        if model_path is None:\n",
    "            print(f\"‚ö†Ô∏è  No model path found for {model_result.get('task_id', 'unknown task')}\")\n",
    "            return None\n",
    "            \n",
    "        model_dir = Path(model_path)\n",
    "        search_paths = [\n",
    "            model_dir / \"weights\" / \"torch\" / \"model.pt\",\n",
    "            model_dir / \"weights\" / \"pytorch\" / \"model.pt\", \n",
    "            model_dir / \"model.pt\",\n",
    "            model_dir / \"model.pth\",\n",
    "        ]\n",
    "        for path in search_paths:\n",
    "            if path.exists():\n",
    "                torch_model_path = path\n",
    "                break\n",
    "    \n",
    "    return torch_model_path if torch_model_path and torch_model_path.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_path = find_torch_model_file(s_md[0])\n",
    "m_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_model_descriptor(\n",
    "    model_result: Dict[str, Any]  # Model result from search results\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Create a comprehensive model descriptor with all needed information.\"\"\"\n",
    "    torch_model_path = find_torch_model_file(\n",
    "        model_result)\n",
    "    \n",
    "    if not torch_model_path:\n",
    "        return None\n",
    "    \n",
    "    # Create standard model combination identifier\n",
    "    model_combo = f\"{model_result['model_name']}_{model_result['backbone']}_{model_result['n_features']}feat\"\n",
    "    \n",
    "    # Handle layers information if present\n",
    "    layers_info = \"\"\n",
    "    if 'layers' in model_result and model_result['layers']:\n",
    "        layers = model_result['layers']\n",
    "        if isinstance(layers, list):\n",
    "            layers_str = \"+\".join(layers)\n",
    "        else:\n",
    "            layers_str = str(layers)\n",
    "        layers_info = f\"_{layers_str}\"\n",
    "        model_combo += layers_info\n",
    "    \n",
    "    return {\n",
    "        'combo': model_combo,\n",
    "        'name': model_result['model_name'],\n",
    "        'backbone': model_result['backbone'],\n",
    "        'n_features': model_result['n_features'],\n",
    "        'layers': model_result.get('layers', []),\n",
    "        'path': torch_model_path,\n",
    "        'training_time': model_result.get('training_time_seconds', 0),\n",
    "        'result': model_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rest =create_model_descriptor(s_md[0])\n",
    "m_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82252c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_model_descriptors(\n",
    "    successful_models: List[Dict[str, Any]]  # List of successful models\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create model descriptors for all successful models.\"\"\"\n",
    "    model_descriptors = []\n",
    "    \n",
    "    for model_result in successful_models:\n",
    "        try:\n",
    "            descriptor = create_model_descriptor(model_result)\n",
    "            if descriptor:\n",
    "                model_descriptors.append(descriptor)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Skipping {model_result.get('model_name', 'unknown')} - model file not found\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in prepare_model_descriptors: {type(e).__name__}: {str(e)}\")\n",
    "            print(f\"   Failed model: {model_result.get('model_name', 'unknown')}\")\n",
    "    \n",
    "    print(f\"üìÇ Prepared {len(model_descriptors)} valid model descriptors\")\n",
    "    return model_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce9437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92218ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_descriptors = _prepare_model_descriptors(s_md)\n",
    "model_descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_single_model_inference(\n",
    "    model_descriptor: Dict[str, Any],  # Model descriptor with path and info\n",
    "    image_path: Path,  # Single image path\n",
    "    device: str  # Device for inference\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run inference on a single image with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        prediction_result = predict_image(\n",
    "            model_path=model_descriptor['path'],\n",
    "            image_path=image_path,\n",
    "            device=device,\n",
    "            save_heatmap=False,  # Don't save files for batch processing\n",
    "            show_heatmap=False\n",
    "        )\n",
    "        \n",
    "        # Add timing and model info\n",
    "        prediction_result['inference_time'] = time.time() - start_time\n",
    "        prediction_result['model_combo'] = model_descriptor['combo']\n",
    "        \n",
    "        return prediction_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return standardized error result with detailed info\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'error_type': type(e).__name__,\n",
    "            'image_path': str(image_path),\n",
    "            'model_combo': model_descriptor['combo'],\n",
    "            'anomaly_score': 0.0,\n",
    "            'prediction': 'ERROR',\n",
    "            'is_anomaly': False,\n",
    "            'heatmap': None,\n",
    "            'inference_time': 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c006742",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_result_ = _run_single_model_inference(\n",
    "    model_descriptors[0], \n",
    "    image_paths[0], \n",
    "    device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_model_image_batch(\n",
    "    model_descriptor: Dict[str, Any],  # Model descriptor\n",
    "    image_paths: List[Path],  # List of image paths\n",
    "    device: str,  # Device for inference\n",
    "    progress_callback: Optional[callable] = None  # Optional progress callback\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Process all images for a single model - true batch processing per model.\"\"\"\n",
    "    model_combo = model_descriptor['combo']\n",
    "    print(f\"\\nüìä Processing model: {model_combo}\")\n",
    "    \n",
    "    batch_results = {}\n",
    "    total_inference_time = 0.0\n",
    "    successful_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for img_idx, image_path in enumerate(image_paths, 1):\n",
    "        # Progress reporting\n",
    "        if img_idx % 5 == 0 or img_idx == len(image_paths):\n",
    "            print(f\"   üñºÔ∏è  Processing image {img_idx}/{len(image_paths)}\")\n",
    "        \n",
    "        # Run inference\n",
    "        result = run_single_model_inference(\n",
    "            model_descriptor, \n",
    "            image_path, \n",
    "            device)\n",
    "        \n",
    "        # Store result\n",
    "        batch_results[str(image_path)] = result\n",
    "        \n",
    "        # Update statistics\n",
    "        if 'error' in result:\n",
    "            error_count += 1\n",
    "            print(f\"   ‚ùå Error on {image_path.name}: {result['error']}\")\n",
    "        else:\n",
    "            successful_count += 1\n",
    "            total_inference_time += result.get('inference_time', 0.0)\n",
    "        \n",
    "        # Call progress callback if provided\n",
    "        if progress_callback:\n",
    "            progress_callback(img_idx, len(image_paths), model_combo)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    avg_time_per_image = total_inference_time / max(successful_count, 1)\n",
    "    \n",
    "    print(f\"   ‚úÖ Completed: {successful_count} successful, {error_count} errors\")\n",
    "    print(f\"   ‚ö° Timing: {total_time:.1f}s total, {avg_time_per_image:.3f}s avg per image\")\n",
    "    \n",
    "    return {\n",
    "        'results': batch_results,\n",
    "        'statistics': {\n",
    "            'total_images': len(image_paths),\n",
    "            'successful_count': successful_count,\n",
    "            'error_count': error_count,\n",
    "            'total_time': total_time,\n",
    "            'inference_time': total_inference_time,\n",
    "            'avg_time_per_image': avg_time_per_image\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_result_ = process_model_image_batch(\n",
    "    model_descriptors[0], \n",
    "    image_paths, \n",
    "    device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def execute_full_batch_inference(\n",
    "    model_descriptors: List[Dict[str, Any]],  # List of model descriptors\n",
    "    image_paths: List[Path],  # List of image paths\n",
    "    device: str  # Device for inference\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute inference for all models on all images with comprehensive tracking.\"\"\"\n",
    "    total_operations = len(model_descriptors) * len(image_paths)\n",
    "    print(f\"üöÄ Starting true batch inference:\")\n",
    "    print(f\"   üìä {len(model_descriptors)} models √ó {len(image_paths)} images = {total_operations} operations\")\n",
    "    \n",
    "    batch_results = {}\n",
    "    global_stats = {\n",
    "        'total_models': len(model_descriptors),\n",
    "        'total_images': len(image_paths),\n",
    "        'total_operations': total_operations,\n",
    "        'successful_operations': 0,\n",
    "        'failed_operations': 0,\n",
    "        'total_inference_time': 0.0,\n",
    "        'model_results': {}\n",
    "    }\n",
    "    \n",
    "    # Process each model\n",
    "    for model_idx, model_descriptor in enumerate(model_descriptors, 1):\n",
    "        print(f\"\\nüéØ Model {model_idx}/{len(model_descriptors)}\")\n",
    "        \n",
    "        # Process this model on all images\n",
    "        model_batch_result = process_model_image_batch(\n",
    "            model_descriptor, \n",
    "            image_paths, \n",
    "            device\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        model_combo = model_descriptor['combo']\n",
    "        batch_results[model_combo] = model_batch_result['results']\n",
    "        \n",
    "        # Update global statistics\n",
    "        stats = model_batch_result['statistics']\n",
    "        global_stats['successful_operations'] += stats['successful_count']\n",
    "        global_stats['failed_operations'] += stats['error_count']\n",
    "        global_stats['total_inference_time'] += stats['inference_time']\n",
    "        global_stats['model_results'][model_combo] = stats\n",
    "    \n",
    "    print(f\"\\nüéâ Batch inference complete!\")\n",
    "    print(f\"   ‚úÖ {global_stats['successful_operations']}/{total_operations} operations successful\")\n",
    "    print(f\"   ‚ö° Total inference time: {global_stats['total_inference_time']:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        'batch_results': batch_results,\n",
    "        'statistics': global_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547996d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_inference_results = _execute_full_batch_inference(\n",
    "    model_descriptors,\n",
    "    image_paths,\n",
    "    device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abc928",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results = batch_inference_results['batch_results']\n",
    "batch_stats = batch_inference_results['statistics']\n",
    "batch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c98bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# ===================================================================\n",
    "# STEP 3: POSTER VISUALIZATION HELPERS\n",
    "# ===================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_poster_dimensions(\n",
    "    num_models: int,  # Number of models\n",
    "    num_images: int,  # Number of test images\n",
    "    show_original: bool  # Whether to show original column\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"Calculate optimal poster grid dimensions.\"\"\"\n",
    "    num_cols = num_models + (1 if show_original else 0)\n",
    "    num_rows = num_images\n",
    "    return num_rows, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_poster_figure(\n",
    "    num_rows: int,  # Number of rows\n",
    "    num_cols: int,  # Number of columns\n",
    "    figsize: Tuple[int, int]  # Figure size\n",
    ") -> Tuple[plt.Figure, np.ndarray]:\n",
    "    \"\"\"Setup matplotlib figure and handle edge cases for axes.\"\"\"\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    \n",
    "    # Normalize axes to 2D array for consistent access\n",
    "    if num_rows == 1 and num_cols == 1:\n",
    "        axes = [[axes]]\n",
    "    elif num_rows == 1:\n",
    "        axes = [axes]\n",
    "    elif num_cols == 1:\n",
    "        axes = [[ax] for ax in axes]\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff046ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def set_poster_main_title(\n",
    "    fig:plt.Figure,  # The figure object\n",
    "    model_count:int,  # Number of models\n",
    "    image_count:int,  # Number of images\n",
    "    evaluation_results:Dict[str, Any]=None  # Evaluation results\n",
    "):\n",
    "    \"\"\"Set the main title of the poster.\"\"\"\n",
    "    title_parts = [\n",
    "        \"Different Model Comparison\",\n",
    "        f\"{model_count} Models x {image_count} Images\",\n",
    "    ]\n",
    "\n",
    "    if evaluation_results and 'models_evaluated' in evaluation_results:\n",
    "        title_parts.append(\n",
    "            f\"{evaluation_results['models_evaluated']}  evaluated\"\n",
    "        )\n",
    "    main_title = \" ‚ö° \".join(title_parts)\n",
    "    fig.suptitle(main_title, fontsize=16, fontweight=\"bold\", y=0.97)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7420251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_column_headers(\n",
    "    axes: np.ndarray,  # Axes array\n",
    "    model_descriptors: List[Dict[str, Any]],  # Model descriptors\n",
    "    show_original: bool  # Whether original column is shown\n",
    "):\n",
    "    \"\"\"Create informative column headers for each model.\"\"\"\n",
    "    col_idx = 0\n",
    "    \n",
    "    # Original images column header\n",
    "    if show_original:\n",
    "        axes[0][col_idx].set_title(\n",
    "            \"Original\" + chr(10) + \"Test Images\", \n",
    "            fontsize=12, fontweight='bold', \n",
    "            pad=15, \n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.7)\n",
    "        )\n",
    "        col_idx += 1\n",
    "    \n",
    "    # Model column headers\n",
    "    for model in model_descriptors:\n",
    "        header_text = f\"{model['name'].upper()}\" + chr(10) + f\"{model['backbone']}\"\n",
    "        header_text += f\"{chr(10)}({model['n_features']} feat)\"\n",
    "\n",
    "        # Add layer info if available\n",
    "        if model.get('layers'):\n",
    "            layers = model['layers']\n",
    "            if isinstance(layers, list):\n",
    "                layer_str = \"+\".join(layers)\n",
    "            else:\n",
    "                layer_str = str(layers)\n",
    "            header_text += f\"{chr(10)}[{layer_str}]\"\n",
    "\n",
    "        axes[0][col_idx].set_title(\n",
    "            header_text, \n",
    "            fontsize=10, fontweight='bold',\n",
    "            pad=15, \n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.7)\n",
    "        )\n",
    "        col_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def display_original_image_with_label(\n",
    "    ax: plt.Axes,  # Axes object\n",
    "    image_path: Path,  # Image path\n",
    "    image_size: Tuple[int, int]  # Display size\n",
    "):\n",
    "    \"\"\"Display original image with row label.\"\"\"\n",
    "    try:\n",
    "        original_img = Image.open(image_path).convert('RGB')\n",
    "        original_img = original_img.resize(image_size)\n",
    "        \n",
    "        ax.imshow(original_img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add image name as row label\n",
    "        ax.text(-0.15, 0.5, image_path.stem, rotation=90, \n",
    "                verticalalignment='center', horizontalalignment='right',\n",
    "                transform=ax.transAxes, \n",
    "                fontsize=9, fontweight='bold')\n",
    "    except Exception as e:\n",
    "        ax.text(0.5, 0.5, f\"ERROR {chr(10)}Loading {chr(10)}{image_path.name}\", \n",
    "                ha='center', va='center', fontsize=8, color='red')\n",
    "        ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b67f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def display_error_placeholder(\n",
    "    ax: plt.Axes,  # Axes object\n",
    "    error_info: str  # Error information\n",
    "):\n",
    "    \"\"\"Display styled error placeholder.\"\"\"\n",
    "    ax.text(0.5, 0.5, f\"ERROR\\\\n{error_info}\", \n",
    "            ha='center', va='center', fontsize=9, color='red',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='mistyrose', alpha=0.8))\n",
    "    ax.axis('off')\n",
    "\n",
    "#| export\n",
    "def prepare_prediction_display_image(\n",
    "    prediction_result: Dict[str, Any],  # Prediction result\n",
    "    original_image_path: Path,  # Original image path\n",
    "    image_size: Tuple[int, int]  # Display size\n",
    ") -> Image.Image:\n",
    "    \"\"\"Prepare the best available image for display (heatmap or original).\"\"\"\n",
    "    # Priority 1: Use heatmap if available\n",
    "    if 'heatmap' in prediction_result and prediction_result['heatmap'] is not None:\n",
    "        heatmap = prediction_result['heatmap']\n",
    "        if isinstance(heatmap, np.ndarray):\n",
    "            # Ensure proper format for PIL\n",
    "            if heatmap.dtype != np.uint8:\n",
    "                heatmap = (heatmap * 255).astype(np.uint8)\n",
    "            display_img = Image.fromarray(heatmap)\n",
    "        else:\n",
    "            display_img = heatmap\n",
    "    else:\n",
    "        # Priority 2: Use original image\n",
    "        display_img = Image.open(original_image_path).convert('RGB')\n",
    "    \n",
    "    return display_img.resize(image_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674031f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def display_model_prediction_result(\n",
    "    ax: plt.Axes,  # Axes object\n",
    "    model_descriptor: Dict[str, Any],  # Model descriptor\n",
    "    image_path: Path,  # Image path\n",
    "    batch_results: Dict[str, Dict[str, Any]],  # All batch results\n",
    "    image_size: Tuple[int, int],  # Display size\n",
    "    show_colors: bool  # Whether to use color coding\n",
    "):\n",
    "    \"\"\"Display the prediction result for one model-image combination.\"\"\"\n",
    "    # Get pre-computed result\n",
    "    model_combo = model_descriptor['combo']\n",
    "    model_results = batch_results.get(model_combo, {})\n",
    "    prediction_result = model_results.get(str(image_path), {})\n",
    "    \n",
    "    if 'error' in prediction_result:\n",
    "        # Display error\n",
    "        error_msg = prediction_result.get('error_type', 'Unknown')\n",
    "        display_error_placeholder(ax, error_msg)\n",
    "    else:\n",
    "        # Display successful result\n",
    "        try:\n",
    "            display_img = prepare_prediction_display_image(\n",
    "                prediction_result, image_path, image_size\n",
    "            )\n",
    "            ax.imshow(display_img)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Add prediction overlay\n",
    "            add_prediction_info_overlay(ax, prediction_result, show_colors)\n",
    "            \n",
    "        except Exception as e:\n",
    "            display_error_placeholder(ax, f\"Display\\\\nError\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3577fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def populate_poster_grid(\n",
    "    axes: np.ndarray,  # Axes array\n",
    "    image_paths: List[Path],  # Image paths\n",
    "    model_descriptors: List[Dict[str, Any]],  # Model descriptors\n",
    "    batch_results: Dict[str, Dict[str, Any]],  # Batch results\n",
    "    show_original: bool,  # Show original column\n",
    "    image_size: Tuple[int, int],  # Image display size\n",
    "    show_colors: bool  # Color code predictions\n",
    "):\n",
    "    \"\"\"Populate the entire poster grid with images and predictions.\"\"\"\n",
    "    for row_idx, image_path in enumerate(image_paths):\n",
    "        print(f\"üñºÔ∏è  Populating poster row {row_idx+1}/{len(image_paths)}: {image_path.name}\")\n",
    "        \n",
    "        col_idx = 0\n",
    "        \n",
    "        # Show original image\n",
    "        if show_original:\n",
    "            display_original_image_with_label(\n",
    "                axes[row_idx][col_idx], \n",
    "                image_path, \n",
    "                image_size)\n",
    "            col_idx += 1\n",
    "        \n",
    "        # Display prediction results for each model\n",
    "        for model_descriptor in model_descriptors:\n",
    "            display_model_prediction_result(\n",
    "                axes[row_idx][col_idx], \n",
    "                model_descriptor, \n",
    "                image_path, \n",
    "                batch_results,\n",
    "                image_size,\n",
    "                show_colors\n",
    "            )\n",
    "            col_idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf53f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_poster_with_metadata(\n",
    "    fig: plt.Figure,  # Figure object\n",
    "    output_file: Union[str, Path],  # Output file path\n",
    "    batch_statistics: Dict[str, Any]  # Batch statistics for metadata\n",
    ") -> Path:\n",
    "    \"\"\"Save the poster with proper layout and return the path.\"\"\"\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Saving poster to: {output_path}\")\n",
    "    \n",
    "    # Save with high quality\n",
    "    plt.savefig(\n",
    "        output_path, \n",
    "        dpi=150, \n",
    "        bbox_inches='tight', \n",
    "        facecolor='white', \n",
    "        edgecolor='none')\n",
    "    plt.show()\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd67e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# ===================================================================\n",
    "# STEP: TESTING AND VALIDATION HELPERS\n",
    "# ===================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3190d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_search_results(\n",
    "    search_results: Dict[str, Any]  # Results from simple_hyperparameter_search\n",
    ") -> bool:\n",
    "    \"\"\"Validate that search results contain required data.\"\"\"\n",
    "    if not isinstance(search_results, dict):\n",
    "        return False\n",
    "    if 'results' not in search_results:\n",
    "        return False\n",
    "    if not isinstance(search_results['results'], list):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_test_images_input(\n",
    "    test_images: Union[str, Path, List]  # Test images input\n",
    ") -> bool:\n",
    "    \"\"\"Validate test images input format.\"\"\"\n",
    "    if isinstance(test_images, (str, Path)):\n",
    "        path = Path(test_images)\n",
    "        return path.exists()\n",
    "    elif isinstance(test_images, list):\n",
    "        return len(test_images) > 0\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_model_descriptors(\n",
    "    model_descriptors: List[Dict[str, Any]]  # Model descriptors to test\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Test model descriptors for completeness and validity.\"\"\"\n",
    "    test_results = {\n",
    "        'total_models': len(model_descriptors),\n",
    "        'valid_models': 0,\n",
    "        'invalid_models': 0,\n",
    "        'missing_files': [],\n",
    "        'valid_combos': []\n",
    "    }\n",
    "    \n",
    "    print(\"üß™ Testing model descriptors...\")\n",
    "    \n",
    "    for descriptor in model_descriptors:\n",
    "        if descriptor['path'].exists():\n",
    "            test_results['valid_models'] += 1\n",
    "            test_results['valid_combos'].append(descriptor['combo'])\n",
    "        else:\n",
    "            test_results['invalid_models'] += 1\n",
    "            test_results['missing_files'].append(str(descriptor['path']))\n",
    "    \n",
    "    print(f\"   ‚úÖ Valid models: {test_results['valid_models']}\")\n",
    "    print(f\"   ‚ùå Invalid models: {test_results['invalid_models']}\")\n",
    "    \n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_image_accessibility(\n",
    "    image_paths: List[Path]  # Image paths to test\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Test image accessibility and format validation.\"\"\"\n",
    "    test_results = {\n",
    "        'total_images': len(image_paths),\n",
    "        'accessible_images': 0,\n",
    "        'inaccessible_images': 0,\n",
    "        'invalid_files': []\n",
    "    }\n",
    "    \n",
    "    print(\"üß™ Testing image accessibility...\")\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img.verify()  # Verify image integrity\n",
    "            test_results['accessible_images'] += 1\n",
    "        except Exception as e:\n",
    "            test_results['inaccessible_images'] += 1\n",
    "            test_results['invalid_files'].append(str(image_path))\n",
    "    \n",
    "    print(f\"   ‚úÖ Accessible images: {test_results['accessible_images']}\")\n",
    "    print(f\"   ‚ùå Inaccessible images: {test_results['inaccessible_images']}\")\n",
    "    \n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2ae2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_modular_validation_tests(\n",
    "    search_results: Dict[str, Any],  # Search results\n",
    "    test_images: Union[str, Path, List],  # Test images\n",
    "    max_models: int,  # Max models\n",
    "    max_test_images: int,  # Max test images\n",
    "    device: str  # Device\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run comprehensive validation tests before processing.\"\"\"\n",
    "    print(\"üß™ RUNNING MODULAR VALIDATION TESTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    validation_results = {\n",
    "        'overall_status': 'pending',\n",
    "        'tests_passed': 0,\n",
    "        'tests_failed': 0,\n",
    "        'details': {}\n",
    "    }\n",
    "    \n",
    "    # Test 1: Search results validation\n",
    "    print(\"\\\\nüîç Test 1: Search Results Validation\")\n",
    "    if validate_search_results(search_results):\n",
    "        print(\"   ‚úÖ Search results format is valid\")\n",
    "        validation_results['tests_passed'] += 1\n",
    "    else:\n",
    "        print(\"   ‚ùå Search results format is invalid\")\n",
    "        validation_results['tests_failed'] += 1\n",
    "    \n",
    "    # Test 2: Test images validation\n",
    "    print(\"\\\\nüîç Test 2: Test Images Validation\")\n",
    "    if validate_test_images_input(test_images):\n",
    "        print(\"   ‚úÖ Test images input is valid\")\n",
    "        validation_results['tests_passed'] += 1\n",
    "    else:\n",
    "        print(\"   ‚ùå Test images input is invalid\")\n",
    "        validation_results['tests_failed'] += 1\n",
    "    \n",
    "    # Test 3: Extract and test models\n",
    "    print(\"\\\\nüîç Test 3: Model Extraction and Validation\")\n",
    "    successful_models = extract_successful_models(search_results, max_models)\n",
    "    if successful_models:\n",
    "        model_descriptors = prepare_model_descriptors(\n",
    "            successful_models)\n",
    "        model_test = test_model_descriptors(model_descriptors)\n",
    "        validation_results['details']['model_test'] = model_test\n",
    "        \n",
    "        if model_test['valid_models'] > 0:\n",
    "            print(\"   ‚úÖ At least one valid model found\")\n",
    "            validation_results['tests_passed'] += 1\n",
    "        else:\n",
    "            print(\"   ‚ùå No valid models found\")\n",
    "            validation_results['tests_failed'] += 1\n",
    "    else:\n",
    "        print(\"   ‚ùå No successful models in search results\")\n",
    "        validation_results['tests_failed'] += 1\n",
    "    \n",
    "    # Test 4: Image accessibility\n",
    "    print(\"\\\\nüîç Test 4: Image Accessibility\")\n",
    "    image_paths = collect_image_paths(test_images)\n",
    "    if image_paths:\n",
    "        limited_images = limit_test_images(image_paths, max_test_images)\n",
    "        image_test = test_image_accessibility(limited_images)\n",
    "        validation_results['details']['image_test'] = image_test\n",
    "        \n",
    "        if image_test['accessible_images'] > 0:\n",
    "            print(\"   ‚úÖ At least one accessible image found\")\n",
    "            validation_results['tests_passed'] += 1\n",
    "        else:\n",
    "            print(\"   ‚ùå No accessible images found\")\n",
    "            validation_results['tests_failed'] += 1\n",
    "    else:\n",
    "        print(\"   ‚ùå No image paths found\")\n",
    "        validation_results['tests_failed'] += 1\n",
    "    \n",
    "    # Test 5: Device availability\n",
    "    print(\"\\\\nüîç Test 5: Device Availability\")\n",
    "    device_test = test_device_availability(device)\n",
    "    validation_results['details']['device_test'] = device_test\n",
    "    print(\"   ‚úÖ Device test completed\")\n",
    "    validation_results['tests_passed'] += 1\n",
    "    \n",
    "    # Overall status\n",
    "    if validation_results['tests_failed'] == 0:\n",
    "        validation_results['overall_status'] = 'passed'\n",
    "        print(f\"\\\\nüéâ ALL TESTS PASSED ({validation_results['tests_passed']}/5)\")\n",
    "    else:\n",
    "        validation_results['overall_status'] = 'failed'\n",
    "        print(f\"\\\\n‚ö†Ô∏è  TESTS INCOMPLETE: {validation_results['tests_passed']} passed, {validation_results['tests_failed']} failed\")\n",
    "    \n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34198ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_device_availability(\n",
    "    device: str  # Device specification\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Test device availability and configuration.\"\"\"\n",
    "    test_results = {\n",
    "        'requested_device': device,\n",
    "        'available_device': 'cpu',\n",
    "        'cuda_available': False,\n",
    "        'cuda_device_count': 0\n",
    "    }\n",
    "    \n",
    "    print(\"üß™ Testing device availability...\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        test_results['cuda_available'] = True\n",
    "        test_results['cuda_device_count'] = torch.cuda.device_count()\n",
    "        \n",
    "        if device == \"auto\":\n",
    "            test_results['available_device'] = 'cuda'\n",
    "        elif device == \"cuda\":\n",
    "            test_results['available_device'] = 'cuda'\n",
    "    \n",
    "    print(f\"   üñ•Ô∏è  Requested: {device}\")\n",
    "    print(f\"   ‚úÖ Available: {test_results['available_device']}\")\n",
    "    print(f\"   üöÄ CUDA: {test_results['cuda_available']}\")\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_modular_batch_comparison_poster(\n",
    "    search_results: Dict[str, Any],        # Results from simple_hyperparameter_search\n",
    "    test_images: Union[str, Path, List],   # Test images (directory, single file, or list)\n",
    "    \n",
    "    # Comparison settings\n",
    "    max_models: int = 4,                   # Maximum models to compare (columns)\n",
    "    max_test_images: int = 6,              # Maximum test images to show (rows)\n",
    "    \n",
    "    # Poster settings  \n",
    "    image_size_in_poster: Tuple[int, int] = (200, 200),  # Size of each image in poster\n",
    "    show_original: bool = True,            # Show original image column\n",
    "    show_prediction_colors: bool = True,   # Color code predictions\n",
    "    \n",
    "    # Output settings\n",
    "    output_file: Union[str, Path] = \"./modular_batch_comparison.png\",\n",
    "    figsize: Tuple[int, int] = (20, 12),   # Figure size in inches\n",
    "    device: str = \"auto\",                  # Device for inference\n",
    "    \n",
    "    # Advanced settings\n",
    "    run_validation_tests: bool = True,     # Run pre-processing validation\n",
    "    evaluation_results: Optional[Dict[str, Any]] = None  # Optional evaluation results\n",
    "    \n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create comparison poster using true modular batch processing.\n",
    "    \n",
    "    This function implements true batch processing with comprehensive modular design,\n",
    "    testing, and validation - without changing the existing inference system.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üéØ MODULAR BATCH COMPARISON POSTER - V2\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üèóÔ∏è  Architecture: True modular batch processing\")\n",
    "    print(\"üîß Inference: Uses existing predict_image() without modification\")\n",
    "    print(\"üß™ Testing: Comprehensive validation and error handling\")\n",
    "    \n",
    "    # === STEP 1: VALIDATION AND TESTING ===\n",
    "    if run_validation_tests:\n",
    "        validation_results = run_modular_validation_tests(\n",
    "            search_results, test_images, max_models, max_test_images, device\n",
    "        )\n",
    "        \n",
    "        if validation_results['overall_status'] == 'failed':\n",
    "            return {\n",
    "                'success': False, \n",
    "                'error': 'Validation tests failed',\n",
    "                'validation_results': validation_results\n",
    "            }\n",
    "    \n",
    "    # === STEP 2: DATA PREPARATION ===\n",
    "    print(f\"\\\\nüîß STEP 2: Data Preparation\")\n",
    "    \n",
    "    successful_models = extract_successful_models(\n",
    "        search_results, max_models)\n",
    "    if not successful_models:\n",
    "        return {'success': False, 'error': 'No successful models found'}\n",
    "    \n",
    "    image_paths = collect_image_paths(\n",
    "        test_images)\n",
    "    if not image_paths:\n",
    "        return {'success': False, 'error': 'No valid image paths found'}\n",
    "    \n",
    "    image_paths = limit_test_images(image_paths, max_test_images)\n",
    "    model_descriptors = prepare_model_descriptors(successful_models)\n",
    "    \n",
    "    if not model_descriptors:\n",
    "        return {'success': False, 'error': 'No valid model files found'}\n",
    "    \n",
    "    print(f\"‚úÖ Prepared: {len(model_descriptors)} models, {len(image_paths)} images\")\n",
    "    \n",
    "    # === STEP 3: TRUE BATCH INFERENCE ===\n",
    "    print(f\"\\\\nüöÄ STEP 3: True Batch Inference\")\n",
    "    \n",
    "    batch_inference_result = execute_full_batch_inference(\n",
    "        model_descriptors, image_paths, device\n",
    "    )\n",
    "    \n",
    "    batch_results = batch_inference_result['batch_results']\n",
    "    batch_statistics = batch_inference_result['statistics']\n",
    "    \n",
    "    # === STEP 4: POSTER CREATION ===\n",
    "    print(f\"\\\\nüé® STEP 4: Poster Creation\")\n",
    "    \n",
    "    num_rows, num_cols = calculate_poster_dimensions(\n",
    "        len(model_descriptors), len(image_paths), show_original\n",
    "    )\n",
    "    \n",
    "    fig, axes = setup_poster_figure(num_rows, num_cols, figsize)\n",
    "    \n",
    "    set_poster_main_title(fig, len(model_descriptors), len(image_paths), evaluation_results)\n",
    "    create_column_headers(axes, model_descriptors, show_original)\n",
    "    \n",
    "    populate_poster_grid(\n",
    "        axes, image_paths, model_descriptors, batch_results,\n",
    "        show_original, image_size_in_poster, show_prediction_colors\n",
    "    )\n",
    "    \n",
    "    output_path = save_poster_with_metadata(fig, output_file, batch_statistics)\n",
    "    \n",
    "    # === STEP 5: FINAL SUMMARY ===\n",
    "    print(f\"\\\\nüìä MODULAR BATCH PROCESSING COMPLETE!\")\n",
    "    print(f\"   üéØ Models processed: {len(model_descriptors)}\")\n",
    "    print(f\"   üñºÔ∏è  Images processed: {len(image_paths)}\")\n",
    "    print(f\"   ‚úÖ Successful operations: {batch_statistics['successful_operations']}\")\n",
    "    print(f\"   ‚ùå Failed operations: {batch_statistics['failed_operations']}\")\n",
    "    print(f\"   ‚ö° Total inference time: {batch_statistics['total_inference_time']:.1f}s\")\n",
    "    print(f\"   üíæ Poster saved: {output_path}\")\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'output_file': str(output_path),\n",
    "        'batch_results': batch_results,\n",
    "        'batch_statistics': batch_statistics,\n",
    "        'model_descriptors': model_descriptors,\n",
    "        'image_paths': [str(p) for p in image_paths],\n",
    "        'validation_results': validation_results if run_validation_tests else None,\n",
    "        'poster_dimensions': (num_rows, num_cols)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7775b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ac25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_prediction_display_image(\n",
    "    prediction_result: Dict[str, Any],  # Prediction result\n",
    "    original_image_path: Path,  # Original image path\n",
    "    image_size: Tuple[int, int]  # Display size\n",
    ") -> Image.Image:\n",
    "    \"\"\"Prepare the best available image for display (heatmap or original).\"\"\"\n",
    "    # Priority 1: Use heatmap if available\n",
    "    if 'heatmap' in prediction_result and prediction_result['heatmap'] is not None:\n",
    "        heatmap = prediction_result['heatmap']\n",
    "        if isinstance(heatmap, np.ndarray):\n",
    "            # Ensure proper format for PIL\n",
    "            if heatmap.dtype != np.uint8:\n",
    "                heatmap = (heatmap * 255).astype(np.uint8)\n",
    "            display_img = Image.fromarray(heatmap)\n",
    "        else:\n",
    "            display_img = heatmap\n",
    "    else:\n",
    "        # Priority 2: Use original image\n",
    "        display_img = Image.open(original_image_path).convert('RGB')\n",
    "    \n",
    "    return display_img.resize(image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_prediction_info_overlay(\n",
    "    ax: plt.Axes,  # Axes object\n",
    "    prediction_result: Dict[str, Any],  # Prediction result\n",
    "    show_colors: bool  # Whether to use color coding\n",
    "):\n",
    "    \"\"\"Add prediction information overlay with score and timing.\"\"\"\n",
    "    prediction_text = prediction_result.get('prediction', 'UNKNOWN')\n",
    "    score = prediction_result.get('anomaly_score', 0.0)\n",
    "    timing = prediction_result.get('inference_time', 0.0)\n",
    "    \n",
    "    # Color coding based on prediction\n",
    "    if show_colors:\n",
    "        text_color = 'red' if prediction_text == 'ANOMALY' else 'green'\n",
    "        bg_alpha = 0.9\n",
    "    else:\n",
    "        text_color = 'black'\n",
    "        bg_alpha = 0.8\n",
    "    \n",
    "    # Create info text\n",
    "    info_text = f\"{prediction_text}: {score:.3f}\"\n",
    "    #if timing > 0:\n",
    "        #info_text += f\"\\\\n{timing*1000:.0f}ms\"\n",
    "    \n",
    "    ax.text(0.02, 0.98, info_text, \n",
    "            transform=ax.transAxes,\n",
    "            verticalalignment='top', fontsize=8,\n",
    "            color=text_color, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=bg_alpha))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e6e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def populate_poster_grid(\n",
    "    axes: np.ndarray,  # Axes array\n",
    "    image_paths: List[Path],  # Image paths\n",
    "    model_descriptors: List[Dict[str, Any]],  # Model descriptors\n",
    "    batch_results: Dict[str, Dict[str, Any]],  # Batch results\n",
    "    show_original: bool,  # Show original column\n",
    "    image_size: Tuple[int, int],  # Image display size\n",
    "    show_colors: bool  # Color code predictions\n",
    "):\n",
    "    \"\"\"Populate the entire poster grid with images and predictions.\"\"\"\n",
    "    for row_idx, image_path in enumerate(image_paths):\n",
    "        #print(f\"üñºÔ∏è  Populating poster row {row_idx+1}/{len(image_paths)}: {image_path.name}\")\n",
    "        \n",
    "        col_idx = 0\n",
    "        \n",
    "        # Show original image\n",
    "        if show_original:\n",
    "            display_original_image_with_label(\n",
    "                axes[row_idx][col_idx], \n",
    "                image_path, \n",
    "                image_size)\n",
    "            col_idx += 1\n",
    "        \n",
    "        # Display prediction results for each model\n",
    "        for model_descriptor in model_descriptors:\n",
    "            display_model_prediction_result(\n",
    "                axes[row_idx][col_idx], \n",
    "                model_descriptor, \n",
    "                image_path, \n",
    "                batch_results,\n",
    "                image_size,\n",
    "                show_colors\n",
    "            )\n",
    "            col_idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a364e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_poster_with_metadata(\n",
    "    fig: plt.Figure,  # Figure object\n",
    "    output_file: Union[str, Path],  # Output file path\n",
    "    batch_statistics: Dict[str, Any]  # Batch statistics for metadata\n",
    ") -> Path:\n",
    "    \"\"\"Save the poster with proper layout and return the path.\"\"\"\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Saving poster to: {output_path}\")\n",
    "    \n",
    "    # Save with high quality\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e82e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_res = create_modular_batch_comparison_poster(\n",
    "    search_results=test_results,\n",
    "    test_images=\"/home/ai_dsx.work/data/projects/AD_tool_test/images/bad\",\n",
    "    max_models=4,\n",
    "    max_test_images=6,\n",
    "    run_validation_tests=False,\n",
    "    show_original=False,\n",
    "    device=\"auto\",\n",
    "    output_file=f\"{data_root}/modular_batch_comparison_poster.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d87467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rich results with detailed statistics\n",
    "if comparison_res['success']:\n",
    "    stats = comparison_res['batch_statistics']\n",
    "    print(f\"‚ö° {stats['total_inference_time']:.1f}s total inference time\")\n",
    "    \n",
    "    # Per-model performance analysis\n",
    "    for model, model_stats in stats['model_results'].items():\n",
    "        print(f\"{model}: {model_stats['avg_time_per_image']:.3f}s/image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186301d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad39f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def diff_parameter_and_save_poster(\n",
    "    data_root:str, # data root path\n",
    "    normal_dir:str, # normal directory name\n",
    "    abnormal_dir:str, # abnormal directory name\n",
    "    class_name:str, # class name\n",
    "    test_images:Union[str,Path,List], # test images path\n",
    "    model_names:List[str]=['padim'], # model names\n",
    "    backbones:List[str]=['wide_resnet50'], # backbones\n",
    "    n_features_list:List[int]=[64], # number of features\n",
    "    layers:List[List[str]]=[['layer1', 'layer2', 'layer3']], # layers\n",
    "    max_epochs:int=10, # max epochs\n",
    "    max_models:int=4, # max models\n",
    "    max_test_images:int=10, # max test images\n",
    "    run_validation_tests:bool=False, # run validation tests\n",
    "    show_original:bool=False, # show original\n",
    "    device:str=\"auto\", # device\n",
    "    figsize:Tuple[int,int]=(20,12), # figure size\n",
    "    output_file:str=f\"modular_batch_comparison_poster.png\", # output file\n",
    "    model_path_name:str=None, # model path\n",
    "    result_path_name:str=None, # model name\n",
    "    ):\n",
    "    '''\n",
    "    This function is used to train a model and save the poster.\n",
    "    '''\n",
    "    if model_path_name is None:\n",
    "        model_path_name = \"test_hyperparameter_results\"\n",
    "    if result_path_name is None:\n",
    "        result_path_name = \"test_hyperparameter_models\"\n",
    "\n",
    "    if model_names is None:\n",
    "        model_names = ['padim']\n",
    "    if backbones is None:\n",
    "        backbones = ['resnet18']\n",
    "    if n_features_list is None:\n",
    "        n_features_list = [10]\n",
    "    if layers is None:\n",
    "        layers = [['layer1', 'layer2', 'layer3']]   \n",
    "\n",
    "    \n",
    "    test_results = simple_hyperparameter_search(\n",
    "        data_root=data_root,  # Replace with your data path\n",
    "        normal_dir=normal_dir,\n",
    "        abnormal_dir=abnormal_dir,\n",
    "        class_name=class_name,\n",
    "        model_names=model_names,\n",
    "        backbones=backbones,\n",
    "        n_features_list=n_features_list,\n",
    "        layers=layers,\n",
    "        max_epochs=1,                    # Very quick training\n",
    "        output_folder=f\"{data_root}/{model_path_name}\",\n",
    "        save_path=f\"{data_root}/{result_path_name}\"\n",
    "    )\n",
    "\n",
    "    comparison_res = create_modular_batch_comparison_poster(\n",
    "        search_results=test_results,\n",
    "        test_images=test_images,\n",
    "        max_models=max_models,\n",
    "        max_test_images=max_test_images,\n",
    "        run_validation_tests=run_validation_tests,\n",
    "        show_original=show_original,\n",
    "        device=device,\n",
    "        figsize=figsize,\n",
    "        output_file=output_file\n",
    "    )\n",
    "    # Rich results with detailed statistics\n",
    "    if comparison_res['success']:\n",
    "        stats = comparison_res['batch_statistics']\n",
    "        print(f\"‚ö° {stats['total_inference_time']:.1f}s total inference time\")\n",
    "        \n",
    "    # Per-model performance analysis\n",
    "    for model, model_stats in stats['model_results'].items():\n",
    "        print(f\"{model}: {model_stats['avg_time_per_image']:.3f}s/image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3007cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_parameter_and_save_poster(\n",
    "    data_root=\"/home/ai_dsx.work/data/projects/AD_tool_test/images\",\n",
    "    normal_dir=\"good\",\n",
    "    abnormal_dir=\"bad\",\n",
    "    class_name=\"test_hyperparam\",\n",
    "    test_images=\"/home/ai_dsx.work/data/projects/AD_tool_test/images/bad\",\n",
    "    model_names=['padim'],\n",
    "    backbones=['resnet18'],\n",
    "    n_features_list=[10, 64, 128],\n",
    "    layers=[['layer1'], ['layer1', 'layer2', 'layer3']],\n",
    "    max_epochs=1,\n",
    "    max_models=4,\n",
    "    max_test_images=6,\n",
    "    run_validation_tests=False,\n",
    "    show_original=False,\n",
    "    device=\"auto\",\n",
    "    output_file=f\"{data_root}/modular_batch_comparison_poster.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = Path(r'/home/ai_dsx.work/data/projects/be-vision-ad-tools/nbs')\n",
    "os.chdir(cur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('07_training.hyperparameter_search.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17434a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
