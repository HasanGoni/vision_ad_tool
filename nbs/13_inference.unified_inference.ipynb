{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Inference System\n",
    "\n",
    "> Intelligent inference system that automatically selects between serial, parallel, and HPC multinode execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference.unified_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook provides a unified inference system that intelligently chooses between three execution strategies:\n",
    "\n",
    "1. **Jupyter Notebook Mode**: Simple for-loop execution for interactive development\n",
    "2. **HPC Multinode Mode**: Distributed execution across cluster nodes using `bsub`\n",
    "3. **Parallel Mode**: Local parallel execution using multiprocessing.Pool\n",
    "\n",
    "The system automatically detects the execution environment and selects the optimal strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Dict, Any, Optional, Set,Callable\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from platform import system\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastcore.test import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if system() == 'Linux':\n",
    "    #os.chdir(r'/home/ai_dsx.work/data/projects/be-vision-ad-tools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Import from existing notebooks\n",
    "from be_vision_ad_tools.inference.prediction_system import (\n",
    "    predict_image_list_from_file_enhanced\n",
    ")\n",
    "\n",
    "from be_vision_ad_tools.inference.multinode_inference import *\n",
    "\n",
    "from be_vision_ad_tools.inference.multinode_from_aiop_tool import (\n",
    "    HPC_Job,\n",
    "    DistributeHPC,\n",
    "    print_status\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "def username() -> str:\n",
    "    \"Return the username of the current user. Example: username()\"\n",
    "    import getpass\n",
    "    return getpass.getuser()\n",
    "\n",
    "# Test immediately\n",
    "print(f\"username(): {username()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if system()== 'Linux' and username() == 'hasan':\n",
    "\tDATA_ROOT = os.getenv('DATA_PATH')\n",
    "\tDATA_ROOT = Path(DATA_ROOT)\n",
    "good_im_path = Path(DATA_ROOT, 'malacca','g_imgs')\n",
    "\n",
    "\n",
    "bad_im_path = Path(DATA_ROOT, 'malacca','b_imgs')\n",
    "MODEL_PATH=Path(DATA_ROOT, 'malacca','models','model.pt')\n",
    "print(f\"good_im_path: {good_im_path}\")\n",
    "print(f\"bad_im_path: {bad_im_path}\")\n",
    "print(f\"MODEL_PATH: {MODEL_PATH}\")\n",
    "sm_im_path = Path(good_im_path).ls()[0]\n",
    "OUTPUT_DIR = Path(DATA_ROOT,'malacca', 'output')\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CORE = \"/home/ai_dsx.work/data/projects/AD_tool_test/images/\"\n",
    "\n",
    "MODEL_PATH= Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/models/exports/TEST_MULITNODE_task_000_padim_resnet18_18_layer1/weights/torch/model.pt')\n",
    "GOOD_IM_PATH= Path(DATA_CORE, 'good')\n",
    "BAD_IM_PATH= Path(DATA_CORE, 'bad')\n",
    "OUTPUT_DIR = Path(r'/home/ai_dsx.work/data/projects/AD_tool_test/inference_results20251009')\n",
    "good_im_list = GOOD_IM_PATH.ls()\n",
    "bad_im_list = BAD_IM_PATH.ls()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Folder Handling\n",
    "\n",
    "Functions to handle both flat and nested folder structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def resolve_test_folders_smart(\n",
    "    test_folders: Union[str, Path, List[Union[str, Path]]]  # Folder(s), file(s), or mixed\n",
    ") -> List[Path]:  # Returns list of image paths\n",
    "    \"\"\"Resolve test_folders to image paths - handles lists, flat folders, and nested folders.\"\"\"\n",
    "\n",
    "    if not isinstance(test_folders, list):\n",
    "        test_folders = [test_folders]\n",
    "\n",
    "    image_paths = []\n",
    "\n",
    "    for folder_or_file in test_folders:\n",
    "        path = Path(folder_or_file)\n",
    "\n",
    "        if not path.exists():\n",
    "            print(f\"âš ï¸  Warning: '{folder_or_file}' does not exist\")\n",
    "            continue\n",
    "\n",
    "        if path.is_file() and is_image_file(path):\n",
    "            # It's an image file\n",
    "            image_paths.append(path)\n",
    "\n",
    "        elif path.is_dir():\n",
    "            # It's a directory - use smart folder scanning\n",
    "            try:\n",
    "                folder_info = scan_folder_structure(path)\n",
    "                image_paths.extend(folder_info['all_image_paths'])\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Warning: Failed to scan '{folder_or_file}': {e}\")\n",
    "                # Fallback to old behavior\n",
    "                for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif']:\n",
    "                    image_paths.extend(path.glob(ext))\n",
    "                    image_paths.extend(path.glob(ext.upper()))\n",
    "        else:\n",
    "            print(f\"âš ï¸  Warning: '{folder_or_file}' is not a valid file or directory\")\n",
    "\n",
    "    # Remove duplicates and sort\n",
    "    unique_paths = sorted(set(image_paths))\n",
    "\n",
    "    print(f\"ðŸ“ Resolved {len(unique_paths)} images from {len(test_folders)} input path(s)\")\n",
    "    return unique_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(resolve_test_folders_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = resolve_test_folders_smart(\n",
    "    test_folders=list(good_im_list),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_ = resolve_test_folders_smart(\n",
    "    test_folders=DATA_CORE\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Detection\n",
    "\n",
    "These functions detect the execution environment to choose the optimal inference strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def in_jupyter_notebook() -> bool:\n",
    "    \"\"\"Check if code is running in a Jupyter notebook.\"\"\"\n",
    "    try:\n",
    "        # Check for IPython shell\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True  # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type\n",
    "    except NameError:\n",
    "        return False  # Probably standard Python interpreter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_jupyter_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def has_bsub_command() -> bool:\n",
    "    \"\"\"Check if bsub command is available (HPC environment).\"\"\"\n",
    "    return shutil.which(\"bsub\") is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_bsub_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def detect_execution_environment() -> str:\n",
    "    \"\"\"Detect execution environment and return appropriate mode.\"\"\"\n",
    "    if in_jupyter_notebook():\n",
    "        return \"jupyter\"\n",
    "    elif has_bsub_command():\n",
    "        return \"hpc\"\n",
    "    else:\n",
    "        return \"parallel\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_execution_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment detection\n",
    "def test_environment_detection():\n",
    "    \"\"\"Test environment detection functions.\"\"\"\n",
    "\n",
    "    # Test Jupyter detection\n",
    "    jupyter_result = in_jupyter_notebook()\n",
    "    print(f\"ðŸ““ In Jupyter: {jupyter_result}\")\n",
    "\n",
    "    # Test HPC detection\n",
    "    hpc_result = has_bsub_command()\n",
    "    print(f\"ðŸ–¥ï¸  Has bsub: {hpc_result}\")\n",
    "\n",
    "    # Test overall detection\n",
    "    mode = detect_execution_environment()\n",
    "    print(f\"ðŸŽ¯ Detected mode: {mode}\")\n",
    "\n",
    "    assert mode in [\"jupyter\", \"hpc\", \"parallel\"], f\"Invalid mode: {mode}\"\n",
    "    print(\"âœ… Environment detection tests passed\")\n",
    "\n",
    "test_environment_detection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Strategies\n",
    "\n",
    "Three different execution strategies for different environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Serial Execution (Jupyter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(predict_image_list_from_file_enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_jupyter_inference(\n",
    "    model_path: Union[str, Path], # Path to the model file\n",
    "    image_path: Union[str, Path, List[Path]], # List of image paths to process\n",
    "    output_dir: Union[str, Path] = \"inference_results\", # Output directory\n",
    "    save_heatmaps: bool = False, # Whether to save heatmaps\n",
    "    heatmap_style: str = \"side_by_side\", # Style of heatmap to save\n",
    "    jpeg_quality: int = 95,\n",
    "    compress: bool = True,\n",
    "    preprocessing_fn=None, # Preprocessing function\n",
    "    preprocessing_kwargs=None, # Preprocessing kwargs\n",
    "    **kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute inference serially using simple for-loop (Jupyter mode).\"\"\"\n",
    "\n",
    "    image_paths = resolve_test_folders_smart(image_path)\n",
    "\n",
    "    print(f\"ðŸ““ Running in Jupyter mode (serial for-loop)\")\n",
    "    print(f\"   Processing {len(image_paths)} images sequentially...\")\n",
    "\n",
    "    model_path = Path(model_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create temporary batch file with all images\n",
    "    batch_file = output_dir / \"jupyter_batch_images.txt\"\n",
    "    create_batch_list_file(image_paths, batch_file)\n",
    "\n",
    "    # Use existing prediction system\n",
    "    results = predict_image_list_from_file_enhanced(\n",
    "        model_path=model_path,\n",
    "        image_list_file=batch_file,\n",
    "        batch_id=\"jupyter_batch\",\n",
    "        output_dir=output_dir,\n",
    "        save_results=save_heatmaps,\n",
    "        heatmap_style=heatmap_style,\n",
    "        compress=compress,\n",
    "        jpeg_quality=jpeg_quality,\n",
    "        preprocessing_fn=preprocessing_fn,\n",
    "        preprocessing_kwargs=preprocessing_kwargs,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Jupyter inference complete: {len(image_paths)} images processed\")\n",
    "\n",
    "    return {\n",
    "        \"mode\": \"jupyter\",\n",
    "        \"total_images\": len(image_paths),\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from be_vision_ad_tools.inference.prediction_system import (\n",
    "    predict_image_list_from_file_enhanced,\n",
    "    predict_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_half_black_image(\n",
    "    image: np.ndarray, # Input image as numpy array\n",
    "    side: str = \"left\" # Side to make black: \"left\", \"right\", \"top\", or \"bottom\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Create an image with half of it blacked out.\"\"\"\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        raise TypeError(\"image must be a numpy array\")\n",
    "\n",
    "    if side not in [\"left\", \"right\", \"top\", \"bottom\"]:\n",
    "        raise ValueError(f\"Invalid side: {side}. Must be one of: left, right, top, bottom\")\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = image.copy()\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    if side == \"left\":\n",
    "        result[:, :width//2] = 0\n",
    "    elif side == \"right\":\n",
    "        result[:, width//2:] = 0\n",
    "    elif side == \"top\":\n",
    "        result[:height//2, :] = 0\n",
    "    elif side == \"bottom\":\n",
    "        result[height//2:, :] = 0\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = run_jupyter_inference(\n",
    "    model_path=MODEL_PATH,\n",
    "    image_paths=bad_im_list,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_heatmaps=True,\n",
    "    heatmap_style=\"side_by_side\",\n",
    "    #preprocessing_fn=create_half_black_image,\n",
    "    #preprocessing_kwargs={\"side\": \"left\"}\n",
    "    jpeg_quality=95,\n",
    "    compress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs['results'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: HPC Multinode Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hpc_inference(\n",
    "    model_path: Union[str, Path],\n",
    "    image_path: Union[str, Path, List[Path]],\n",
    "    output_dir: Union[str, Path] = \"inference_results\",\n",
    "    save_heatmaps: bool = False,\n",
    "    heatmap_style: str = \"side_by_side\",\n",
    "    compress: bool = True,\n",
    "    jpeg_quality: int = 95,\n",
    "    batch_size: int = 10,\n",
    "    num_nodes: int = 10,\n",
    "    preprocessing_fn=None,\n",
    "    preprocessing_kwargs=None,\n",
    "    **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"Execute inference using HPC multinode execution.\"\"\"\n",
    "\n",
    "    print(f\"ðŸ–¥ï¸  Running HPC multinode inference\")\n",
    "    bsub_rs = distribute_folder_inference(\n",
    "                                        root_path=image_path,\n",
    "                                        model_path=model_path,\n",
    "                                        output_dir=output_dir,\n",
    "                                        save_heatmaps=save_heatmaps,\n",
    "                                        heatmap_style=heatmap_style,\n",
    "                                        batch_size=batch_size,\n",
    "                                        num_nodes=num_nodes,\n",
    "                                        compress=compress,\n",
    "                                        jpeg_quality=jpeg_quality,\n",
    "                                        preprocessing_fn=preprocessing_fn,\n",
    "                                        preprocessing_kwargs=preprocessing_kwargs,\n",
    "                                        **kwargs\n",
    "                                    )\n",
    "\n",
    "    print(f\"âœ… HPC inference complete: {len(Path(image_path).ls())} images processed\")\n",
    "\n",
    "    return {\n",
    "        \"mode\": \"hpc\",\n",
    "        \"total_images\": len(Path(image_path).ls()),\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"num_nodes\": num_nodes,\n",
    "        \"results\": bsub_rs\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_ = run_hpc_inference(\n",
    "    model_path=MODEL_PATH,\n",
    "    image_path=BAD_IM_PATH,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_heatmaps=True,\n",
    "    heatmap_style=\"side_by_side\",\n",
    "    batch_size=10,\n",
    "    num_nodes=10,\n",
    "    compress=True,\n",
    "    jpeg_quality=95,\n",
    "    preprocessing_fn=None,\n",
    "    preprocessing_kwargs=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test done "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. run_jupyter_inference\n",
    "2. distribute_folder_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Parallel Execution (Python Script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _process_batch_worker(args: tuple) -> Dict[str, Any]:\n",
    "    \"\"\"Worker function for parallel batch processing.\"\"\"\n",
    "    model_path, batch_images, batch_id, output_dir, save_heatmaps, heatmap_style, compress, jpeg_quality, preprocessing_fn, preprocessing_kwargs, kwargs = args\n",
    "\n",
    "    # Create batch file\n",
    "    batch_list_file = Path(output_dir) / \"batch_lists\" / f\"{batch_id}_images.txt\"\n",
    "    create_batch_list_file(batch_images, batch_list_file)\n",
    "\n",
    "    # Process batch\n",
    "    results = predict_image_list_from_file(\n",
    "        model_path=model_path,\n",
    "        image_list_file=batch_list_file,\n",
    "        batch_id=batch_id,\n",
    "        output_dir=output_dir,\n",
    "        save_results=save_heatmaps,\n",
    "        heatmap_style=heatmap_style,\n",
    "        compress=compress,\n",
    "        jpeg_quality=jpeg_quality,\n",
    "        preprocessing_fn=preprocessing_fn,\n",
    "        preprocessing_kwargs=preprocessing_kwargs,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"num_images\": len(batch_images),\n",
    "        \"results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_parallel_inference(\n",
    "    model_path: Union[str, Path],\n",
    "    image_path: Union[str, Path, List[Path]],\n",
    "    batch_size: int = 100,\n",
    "    num_workers: Optional[int] = None,\n",
    "    output_dir: Union[str, Path] = \"parallel_results\",\n",
    "    save_heatmaps: bool = False,\n",
    "    heatmap_style: str = \"side_by_side\",\n",
    "    compress: bool = True,\n",
    "    jpeg_quality: int = 95,\n",
    "    num_nodes: int = 10,\n",
    "    preprocessing_fn=None,\n",
    "    preprocessing_kwargs=None,\n",
    "    **kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute inference in parallel using multiprocessing.Pool.\"\"\"\n",
    "\n",
    "    print(\"ðŸš€ SMART FOLDER INFERENCE DISTRIBUTION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Step 1: Validate inputs (fail fast)\n",
    "    print(\"\\nðŸ“‹ Step 1: Validating inputs...\")\n",
    "    model_path, root_path = validate_inference_inputs(\n",
    "        model_path=model_path,\n",
    "        root_path=image_path,\n",
    "        batch_size=batch_size,\n",
    "        num_nodes=num_nodes\n",
    "    )\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âœ… Output directory: {output_dir}\")\n",
    "\n",
    "    # Step 2: Scan folder structure (auto-detect flat vs nested)\n",
    "    print(f\"\\nðŸ“¡ Step 2: Scanning folder structure...\")\n",
    "    folder_info = scan_folder_structure(root_path)\n",
    "\n",
    "    # Step 3: Create smart batches\n",
    "    print(f\"\\nðŸ”¨ Step 3: Creating smart batches...\")\n",
    "    batches = create_smart_batches(folder_info, batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    # Split into batches\n",
    "    image_batches = create_smart_batches(\n",
    "        Path(image_path), batch_size=batch_size)\n",
    "\n",
    "    if num_workers is None:\n",
    "        num_workers = cpu_count()\n",
    "\n",
    "    print(f\"âš¡ Running in parallel mode with {num_workers} workers\")\n",
    "\n",
    "    model_path = Path(model_path)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Prepare worker arguments\n",
    "    worker_args = []\n",
    "    for i, batch in enumerate(image_batches):\n",
    "        batch_id = f\"batch_{i+1:04d}\"\n",
    "        batch_kwargs = {\n",
    "            \"save_heatmaps\": save_heatmaps,\n",
    "            \"heatmap_style\": heatmap_style,\n",
    "            \"compress\": compress,\n",
    "            \"jpeg_quality\": jpeg_quality,\n",
    "            \"preprocessing_fn\": preprocessing_fn,\n",
    "            \"preprocessing_kwargs\": preprocessing_kwargs,\n",
    "            **kwargs\n",
    "        }\n",
    "        worker_args.append((model_path, batch, batch_id, output_path, batch_kwargs))\n",
    "\n",
    "    # Execute in parallel\n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        batch_results = list(tqdm(\n",
    "            pool.imap(_process_batch_worker, worker_args),\n",
    "            total=len(worker_args),\n",
    "            desc=\"Processing batches\"\n",
    "        ))\n",
    "\n",
    "    print(f\"âœ… Parallel inference complete: {len(image_batches)} batches processed\")\n",
    "\n",
    "    return {\n",
    "        \"mode\": \"parallel\",\n",
    "        \"total_images\": len(image_paths),\n",
    "        \"num_batches\": len(image_batches),\n",
    "        \"num_workers\": num_workers,\n",
    "        \"output_dir\": str(output_path),\n",
    "        \"results\": batch_results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_ = run_parallel_inference(\n",
    "    model_path=MODEL_PATH,\n",
    "    image_path=BAD_IM_PATH,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_heatmaps=True,\n",
    "    heatmap_style=\"side_by_side\",\n",
    "    compress=True,\n",
    "    jpeg_quality=95,\n",
    "    preprocessing_fn=None,\n",
    "    preprocessing_kwargs=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Interface\n",
    "\n",
    "The main function that intelligently routes to the appropriate execution strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def unified_inference(\n",
    "    model_path: Union[str, Path],\n",
    "    test_folders: Union[str, Path, List[Union[str, Path]]],\n",
    "    batch_size: int = 100,\n",
    "    execution_mode: str = \"auto\",\n",
    "    num_nodes: int = 4,\n",
    "    num_workers: Optional[int] = None,\n",
    "    output_dir: Optional[Union[str, Path]] = None,\n",
    "    save_heatmaps: bool = False,\n",
    "    heatmap_style: str = \"cv2_side_by_side\",\n",
    "    **kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Unified inference function that automatically selects execution strategy.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to trained model\n",
    "        test_folders: Image folder(s) or file path(s)\n",
    "        batch_size: Maximum images per batch\n",
    "        execution_mode: \"auto\", \"jupyter\", \"hpc\", or \"parallel\"\n",
    "        num_nodes: Number of HPC nodes (for HPC mode)\n",
    "        num_workers: Number of parallel workers (for parallel mode, default: cpu_count())\n",
    "        output_dir: Output directory for results\n",
    "        save_heatmaps: Whether to save visualization heatmaps\n",
    "        heatmap_style: Visualization style\n",
    "        **kwargs: Additional arguments passed to inference functions\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with inference results and metadata\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"ðŸš€ Starting Unified Inference System\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Auto-detect or validate execution mode\n",
    "    if execution_mode == \"auto\":\n",
    "        execution_mode = detect_execution_environment()\n",
    "        print(f\"ðŸŽ¯ Auto-detected mode: {execution_mode}\")\n",
    "    else:\n",
    "        if execution_mode not in [\"jupyter\", \"hpc\", \"parallel\"]:\n",
    "            raise ValueError(f\"Invalid execution_mode: {execution_mode}\")\n",
    "        print(f\"ðŸŽ¯ Using specified mode: {execution_mode}\")\n",
    "\n",
    "    # Resolve image paths (handles lists, flat folders, and nested folders)\n",
    "    image_paths = resolve_test_folders_smart(test_folders)\n",
    "\n",
    "    if not image_paths:\n",
    "        raise ValueError(\"No valid images found in test_folders\")\n",
    "\n",
    "    # Set default output directory based on mode\n",
    "    if output_dir is None:\n",
    "        output_dir = f\"{execution_mode}_inference_results\"\n",
    "\n",
    "    # Execute based on detected/specified mode\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if execution_mode == \"jupyter\":\n",
    "        results = run_jupyter_inference(\n",
    "            model_path=model_path,\n",
    "            image_paths=image_paths,\n",
    "            output_dir=output_dir,\n",
    "            save_heatmaps=save_heatmaps,\n",
    "            heatmap_style=heatmap_style,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    elif execution_mode == \"hpc\":\n",
    "        results = run_hpc_multinode_inference(\n",
    "            model_path=model_path,\n",
    "            image_paths=image_paths,\n",
    "            batch_size=batch_size,\n",
    "            num_nodes=num_nodes,\n",
    "            output_dir=output_dir,\n",
    "            save_heatmaps=save_heatmaps,\n",
    "            heatmap_style=heatmap_style,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    elif execution_mode == \"parallel\":\n",
    "        results = run_parallel_inference(\n",
    "            model_path=model_path,\n",
    "            image_paths=image_paths,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            output_dir=output_dir,\n",
    "            save_heatmaps=save_heatmaps,\n",
    "            heatmap_style=heatmap_style,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ðŸŽ‰ Unified inference complete!\")\n",
    "    print(f\"   Mode: {results['mode']}\")\n",
    "    print(f\"   Images: {results['total_images']}\")\n",
    "    print(f\"   Output: {results['output_dir']}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Auto-detect and run with flat folder\n",
    "# results = unified_inference(\n",
    "#     model_path=\"path/to/model.pt\",\n",
    "#     test_folders=\"test_images/\",  # Flat folder - all images in one directory\n",
    "#     batch_size=50,\n",
    "#     save_heatmaps=True\n",
    "# )\n",
    "\n",
    "# Example 1b: Auto-detect and run with nested folder\n",
    "# results = unified_inference(\n",
    "#     model_path=\"path/to/model.pt\",\n",
    "#     test_folders=\"production_images/\",  # Nested folder - images in subfolders\n",
    "#     batch_size=50,\n",
    "#     save_heatmaps=True\n",
    "# )\n",
    "\n",
    "# Example 1c: Auto-detect and run with list of images\n",
    "# results = unified_inference(\n",
    "#     model_path=\"path/to/model.pt\",\n",
    "#     test_folders=[\"img1.jpg\", \"img2.png\", \"folder1/\"],  # Mixed: files and folders\n",
    "#     batch_size=50,\n",
    "#     save_heatmaps=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Force Jupyter mode\n",
    "# results = unified_inference(\n",
    "#     model_path=\"path/to/model.pt\",\n",
    "#     test_folders=[\"folder1/\", \"folder2/\"],\n",
    "#     execution_mode=\"jupyter\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Force HPC multinode mode\n",
    "# results = unified_inference(\n",
    "#     model_path=\"path/to/model.pt\",\n",
    "#     test_folders=\"large_dataset/\",\n",
    "#     execution_mode=\"hpc\",\n",
    "#     batch_size=100,\n",
    "#     num_nodes=8\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Force parallel mode with custom workers\n",
    "# results = unified_inference(\n",
    "#     model_path=\"path/to/model.pt\",\n",
    "#     test_folders=\"test_images/\",\n",
    "#     execution_mode=\"parallel\",\n",
    "#     batch_size=50,\n",
    "#     num_workers=8\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests and Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unified_inference_modes():\n",
    "    \"\"\"Test that unified inference can route to different modes.\"\"\"\n",
    "\n",
    "    # Test mode validation\n",
    "    try:\n",
    "        unified_inference(\n",
    "            model_path=\"fake.pt\",\n",
    "            test_folders=[],\n",
    "            execution_mode=\"invalid_mode\"\n",
    "        )\n",
    "        assert False, \"Should raise ValueError for invalid mode\"\n",
    "    except ValueError as e:\n",
    "        assert \"Invalid execution_mode\" in str(e)\n",
    "        print(\"âœ… Mode validation works\")\n",
    "\n",
    "    # Test empty image paths\n",
    "    try:\n",
    "        unified_inference(\n",
    "            model_path=\"fake.pt\",\n",
    "            test_folders=[],\n",
    "            execution_mode=\"jupyter\"\n",
    "        )\n",
    "        assert False, \"Should raise ValueError for empty images\"\n",
    "    except ValueError as e:\n",
    "        assert \"No valid images\" in str(e)\n",
    "        print(\"âœ… Empty images validation works\")\n",
    "\n",
    "    print(\"âœ… All unified inference tests passed\")\n",
    "\n",
    "def test_resolve_test_folders_smart():\n",
    "    \"\"\"Test smart folder resolution with different input types.\"\"\"\n",
    "\n",
    "    # Test with non-existent path (should warn but not fail)\n",
    "    result = resolve_test_folders_smart([\"non_existent_folder\"])\n",
    "    test_eq(len(result), 0)\n",
    "    print(\"âœ… Handles non-existent paths gracefully\")\n",
    "\n",
    "    # Test with empty list\n",
    "    result = resolve_test_folders_smart([])\n",
    "    test_eq(len(result), 0)\n",
    "    print(\"âœ… Handles empty list\")\n",
    "\n",
    "    print(\"âœ… Smart folder resolution tests passed\")\n",
    "\n",
    "# Run tests\n",
    "test_unified_inference_modes()\n",
    "test_resolve_test_folders_smart()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
