"""A comprehensive, production-ready anomaly detection training function with full anomalib flexibility"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/04_training.flexible_anomaly_trainer.ipynb.

# %% auto 0
__all__ = ['detect_environment', 'reset_environment_cache', 'get_smart_defaults', 'ModelType', 'BackboneType', 'ThresholdMethod',
           'FlexibleTrainingConfig', 'train_anomaly_model', 'validate_model_name', 'validate_backbone_name', 'main_',
           'run_inference_after_training']

# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 4
import sys
import platform
import psutil
import os
import logging
import warnings
from pathlib import Path
from typing import Tuple, List, Optional, Union, Dict, Any, Literal
from datetime import datetime
import json
import yaml
from dataclasses import dataclass, field, asdict
from enum import Enum

# Core scientific libraries
import numpy as np
import pandas as pd

# Matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# PIL for image processing and cv2 for image processing
from PIL import Image, ImageDraw, ImageFont
import cv2


# FastCore for CLI and utilities
from fastcore.all import *
from fastcore.script import *


# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 6
import torch
import torchvision.transforms.v2 as v2
from torch.utils.data import DataLoader
from torchvision.transforms.v2 import Compose, Resize, ToTensor, Normalize


# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 7
# Anomalib imports - CORRECTED for v1.2.0
import anomalib
from anomalib import TaskType, LearningType
from anomalib.data.image.folder import Folder
from anomalib.engine import Engine
from anomalib.models import (
    Padim, Patchcore, Cflow, Fastflow, Stfpm, 
    EfficientAd, Draem, ReverseDistillation,
    Dfkde, Dfm, Ganomaly, Cfa, Csflow, Dsr, Fre, Rkde, Uflow
)
from anomalib.deploy import ExportType, TorchInferencer
from anomalib.utils.normalization import NormalizationMethod  # Only MIN_MAX and NONE available
from anomalib.metrics import ManualThreshold, F1AdaptiveThreshold  # Correct threshold classes
from anomalib.callbacks import TilerConfigurationCallback
from anomalib.utils.visualization.image import ImageVisualizer, VisualizationMode

# Lightning imports
from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, RichModelSummary
from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger
from PIL import Image, ImageFile
import PIL

# Enable loading of truncated images - fixes PIL truncated image errors
ImageFile.LOAD_TRUNCATED_IMAGES = True

# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 8
# Suppress warnings for cleaner output
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
logging.getLogger('lightning.pytorch').setLevel(logging.WARNING)

# Environment detection utilities
import psutil
import platform
import multiprocessing as mp


# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 11
# Global cache for environment detection to avoid duplicate detection and messages
_env_cache = None

def detect_environment() -> Dict[str, Any]:
    """
    Intelligent environment detection for optimal anomalib configuration.
    Includes detection for HPC systems with NFS storage to prevent multiprocessing issues.
    """
    global _env_cache
    
    # Return cached result if available
    if _env_cache is not None:
        return _env_cache
    
    env_info = {
        'is_jupyter': False,
        'is_colab': False,
        'is_kaggle': False,
        'is_hpc': False,
        'is_nfs': False,
        'platform': platform.system(),
        'cpu_count': mp.cpu_count(),
        'available_memory_gb': psutil.virtual_memory().total / (1024**3),
        'recommended_num_workers': 4,
        'recommended_batch_size': 16,
        'recommended_accelerator': 'auto'
    }
    
    # Detect Jupyter environments
    try:
        # Check if IPython is available and we're in a notebook
        from IPython import get_ipython
        if get_ipython() is not None:
            env_info['is_jupyter'] = True
            # Check for specific notebook types
            if 'google.colab' in str(get_ipython()):
                env_info['is_colab'] = True
            elif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:
                env_info['is_kaggle'] = True
    except ImportError:
        pass
    
    # Detect HPC environment indicators
    hpc_indicators = [
        'SLURM_JOB_ID', 'PBS_JOBID', 'LSB_JOBID',  # Job schedulers
        'SLURM_CLUSTER_NAME', 'PBS_QUEUE', 'LSF_QUEUE',  # Queue systems
        'MODULEPATH', 'LMOD_DIR',  # Module systems common in HPC
    ]
    
    if any(indicator in os.environ for indicator in hpc_indicators):
        env_info['is_hpc'] = True
    
    # Detect NFS filesystem (common in HPC environments)
    try:
        import subprocess
        # Check if current working directory is on NFS
        result = subprocess.run(['df', '-T', '.'], capture_output=True, text=True, timeout=5)
        if 'nfs' in result.stdout.lower():
            env_info['is_nfs'] = True
    except (subprocess.TimeoutExpired, subprocess.SubprocessError, FileNotFoundError):
        # If df command fails, try alternative detection
        try:
            # Check if /proc/mounts exists and contains NFS info
            with open('/proc/mounts', 'r') as f:
                mounts = f.read()
                if 'nfs' in mounts and os.getcwd() in mounts:
                    env_info['is_nfs'] = True
        except (FileNotFoundError, PermissionError):
            # Final fallback: check for common NFS patterns in hostname or environment
            hostname = os.environ.get('HOSTNAME', '').lower()
            if any(pattern in hostname for pattern in ['nfs', 'shared', 'cluster']):
                env_info['is_nfs'] = True
    
    # Auto-configure based on environment
    if env_info['is_jupyter'] or env_info['is_colab'] or env_info['is_kaggle']:
        # Jupyter/Colab/Kaggle: Use single-threaded to avoid multiprocessing issues
        env_info['recommended_num_workers'] = 0
        env_info['recommended_accelerator'] = 'cpu' if env_info['platform'] == 'Windows' else 'auto'
    elif env_info['is_hpc'] or env_info['is_nfs']:
        # HPC/NFS environments: Use single-threaded to avoid "Device or resource busy" errors
        env_info['recommended_num_workers'] = 0
        env_info['recommended_accelerator'] = 'auto'
        print(f"üñ•Ô∏è  HPC/NFS environment detected - Using num_workers=0 to prevent multiprocessing issues")
    else:
        # Script execution on local systems: Can use multiple workers
        env_info['recommended_num_workers'] = min(4, max(1, env_info['cpu_count'] // 2))
    
    # Memory-aware batch size recommendations
    memory_gb = env_info['available_memory_gb']
    if memory_gb < 4:
        env_info['recommended_batch_size'] = 4
    elif memory_gb < 8:
        env_info['recommended_batch_size'] = 8
    elif memory_gb < 16:
        env_info['recommended_batch_size'] = 16
    else:
        env_info['recommended_batch_size'] = 32
    
    # Cache the result for future calls
    _env_cache = env_info
    return env_info



# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 12
def reset_environment_cache():
    """Reset the environment detection cache. Useful for testing."""
    global _env_cache
    _env_cache = None

def get_smart_defaults() -> Dict[str, Any]:
    """Get intelligent defaults based on current environment."""
    env = detect_environment()
    
    return {
        'num_workers': env['recommended_num_workers'],
        'train_batch_size': env['recommended_batch_size'],
        'eval_batch_size': env['recommended_batch_size'],
        'accelerator': env['recommended_accelerator'],
        'enable_progress_bar': not env['is_jupyter'],  # Disable in Jupyter for cleaner output
        'num_sanity_val_steps': 0 if env['is_jupyter'] else 2,  # Reduce in Jupyter
    }


# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 17
class ModelType(str, Enum):
    """Available anomaly detection models in anomalib."""
    PADIM = "padim"
    PATCHCORE = "patchcore"
    CFLOW = "cflow"
    FASTFLOW = "fastflow"
    STFPM = "stfpm"
    EFFICIENT_AD = "efficient_ad"
    DRAEM = "draem"
    REVERSE_DISTILLATION = "reverse_distillation"
    DFKDE = "dfkde"
    DFM = "dfm"
    GANOMALY = "ganomaly"
    CFA = "cfa"
    CSFLOW = "csflow"
    DSR = "dsr"
    FRE = "fre"
    RKDE = "rkde"
    UFLOW = "uflow"


# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 18
class BackboneType(str, Enum):
    """Available backbone architectures."""
    RESNET18 = "resnet18"
    RESNET34 = "resnet34"
    RESNET50 = "resnet50"
    RESNET101 = "resnet101"
    WIDE_RESNET50 = "wide_resnet50_2"
    EFFICIENTNET_B0 = "efficientnet_b0"
    EFFICIENTNET_B1 = "efficientnet_b1"
    EFFICIENTNET_B2 = "efficientnet_b2"
    EFFICIENTNET_B3 = "efficientnet_b3"
    EFFICIENTNET_B4 = "efficientnet_b4"
    EFFICIENTNET_B5 = "efficientnet_b5"
    EFFICIENTNET_B6 = "efficientnet_b6"
    EFFICIENTNET_B7 = "efficientnet_b7"
    VIT_B_16 = "vit_b_16"
    VIT_L_16 = "vit_l_16"
    


# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 19
class ThresholdMethod(str, Enum):
    """Threshold computation methods."""
    ADAPTIVE = "adaptive"
    MANUAL = "manual"


# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 20
@dataclass
class FlexibleTrainingConfig:
    """Comprehensive configuration for flexible anomaly detection training."""
    
    # Data configuration
    data_root: Union[str, Path] = field(default_factory=lambda: Path.cwd())
    normal_dir: str = "normal"
    abnormal_dir: str = "abnormal"
    class_name: str = "default_class"
    
    # Model configuration  
    model_name: Union[str, ModelType] = ModelType.PADIM
    backbone: Union[str, BackboneType] = BackboneType.RESNET18
    layers: List[str] = field(default_factory=lambda: ["layer1", "layer2", "layer3"])
    n_features: int = 100
    model_file_name: str = "model.pth"



    # Image preprocessing - CORRECTED for anomalib v1.2.0 with anomalib defaults
    image_size: Tuple[int, int] = (256, 256)  # Anomalib standard default (not 224)
    normalization_method: NormalizationMethod = NormalizationMethod.MIN_MAX  # Only MIN_MAX or NONE available
    center_crop: Optional[Tuple[int, int]] = None
    
    # Training configuration - Will be auto-adjusted based on environment
    max_epochs: int = 100
    train_batch_size: Optional[int] = None  # Auto-detected if None
    eval_batch_size: Optional[int] = None   # Auto-detected if None  
    num_workers: Optional[int] = None       # Auto-detected if None
    accelerator: str = "auto"
    devices: Union[int, List[int], str] = "auto"
    
    # Engine configuration - Auto-adjusted for environment
    enable_progress_bar: Optional[bool] = None      # Auto-detected if None
    num_sanity_val_steps: Optional[int] = None      # Auto-detected if None
    
    # Threshold configuration
    threshold_method: ThresholdMethod = ThresholdMethod.ADAPTIVE
    manual_threshold: Optional[float] = None
    
    # Callbacks and monitoring
    early_stopping: bool = True
    early_stopping_patience: int = 10
    early_stopping_metric: str = "image_AUROC"
    early_stopping_mode: str = "max"
    
    # Model saving
    save_path: Union[str, Path] = field(default_factory=lambda: Path.cwd() / "models")
    model_name_suffix: str = ""
    save_top_k: int = 1
    
    # Export formats
    export_formats: List[ExportType] = field(default_factory=lambda: [ExportType.TORCH])
    
    # Logging - Using anomalib defaults
    log_level: str = "INFO"  # Anomalib default
    enable_tensorboard: bool = False  # Anomalib default (not True)
    enable_csv_logger: bool = False   # Anomalib default (not True)
    
    # Advanced options
    seed: Optional[int] = None
    deterministic: bool = False
    benchmark: bool = True
    
    # Tiling (for large images) - Using anomalib defaults
    enable_tiling: bool = False
    tile_size: Optional[Tuple[int, int]] = None  # Anomalib default (disabled)
    stride: Optional[Tuple[int, int]] = None     # Anomalib default (disabled)
    
    def __post_init__(self):
        """Post-initialization validation, type conversion, and intelligent defaults."""
        # Convert paths to Path objects
        self.data_root = Path(self.data_root)
        self.save_path = Path(self.save_path)
        
        # Create save directory if it doesn't exist
        self.save_path.mkdir(parents=True, exist_ok=True)
        
        # Apply intelligent environment-based defaults FIRST
        smart_defaults = get_smart_defaults()
        
        if self.num_workers is None:
            self.num_workers = smart_defaults['num_workers']
            
        if self.train_batch_size is None:
            self.train_batch_size = smart_defaults['train_batch_size']
            
        if self.eval_batch_size is None:
            self.eval_batch_size = smart_defaults['eval_batch_size']
            
        if self.enable_progress_bar is None:
            self.enable_progress_bar = smart_defaults['enable_progress_bar']
            
        if self.num_sanity_val_steps is None:
            self.num_sanity_val_steps = smart_defaults['num_sanity_val_steps']
        
        # Auto-adjust accelerator for problematic environments
        env = detect_environment()
        if env['is_jupyter'] and env['platform'] == 'Windows' and self.accelerator == 'auto':
            self.accelerator = 'cpu'  # Force CPU on Windows Jupyter to avoid device issues
        
        # Log the intelligent adjustments
        if env['is_jupyter']:
            print(f"ü§ñ Jupyter environment detected - Applied smart defaults:")
            print(f"   ‚Ä¢ num_workers: {self.num_workers} (multiprocessing-safe)")
            print(f"   ‚Ä¢ batch_size: {self.train_batch_size} (memory-aware)")
            print(f"   ‚Ä¢ progress_bar: {self.enable_progress_bar} (clean output)")
            print(f"   ‚Ä¢ accelerator: {self.accelerator}")
        
        # Validate and convert model_name if it's a string
        if isinstance(self.model_name, str):
            try:
                self.model_name = ModelType(self.model_name.lower())
            except ValueError:
                valid_models = [m.value for m in ModelType]
                raise ValueError(f"Invalid model name: {self.model_name}. Valid options are: {valid_models}")
        
        # Validate and convert backbone if it's a string 
        if isinstance(self.backbone, str):
            try:
                self.backbone = BackboneType(self.backbone.lower())
            except ValueError:
                valid_backbones = [b.value for b in BackboneType]
                raise ValueError(f"Invalid backbone name: {self.backbone}. Valid options are: {valid_backbones}")
        
        # Validate threshold configuration
        if self.threshold_method == ThresholdMethod.MANUAL and self.manual_threshold is None:
            raise ValueError("Manual threshold value must be provided when using manual threshold method")
        
        # Validate image size
        if not isinstance(self.image_size, (tuple, list)) or len(self.image_size) != 2:
            raise ValueError("Image size must be a tuple/list of 2 integers")


# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 21
@patch_to(FlexibleTrainingConfig)
def to_dict(self) -> Dict[str, Any]:
    """Convert config to dictionary."""
    return asdict(self)

# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 22
@patch_to(FlexibleTrainingConfig)
def save_config(self, path: Union[str, Path]) -> None:
    """Save configuration to YAML file."""
    config_dict = self.to_dict()
    config_dict['data_root'] = str(config_dict['data_root'])
    config_dict['save_path'] = str(config_dict['save_path'])
        
    with open(path, 'w') as f:
        yaml.dump(config_dict, f, default_flow_style=False, indent=2)

# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 23
@patch_to(FlexibleTrainingConfig, classmethod)    
def from_dict(cls, config_dict: Dict[str, Any]) -> 'FlexibleTrainingConfig':
    """Create config from dictionary."""
    return cls(**config_dict)

# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 25
def _extract_model_inference_info(
    model # Model could be trained or exported model
    ) -> Dict[str, Any]:
    """Extract threshold and pixel statistics from trained model for inference.
    """
    if not hasattr(model, 'image_threshold') or not hasattr(model, 'pixel_threshold'):
        raise AttributeError("Model missing required threshold attributes. Ensure model is properly trained.")
    
    if not hasattr(model, 'normalization_metrics'):
        raise RuntimeError("Model normalization metrics not available. Model may not be fitted yet.")
    
    try:
        inference_info = {
            'image_threshold': float(model.image_threshold.value.item()) if hasattr(model.image_threshold.value, 'item') else float(model.image_threshold.value),
            'pixel_threshold': float(model.pixel_threshold.value.item()) if hasattr(model.pixel_threshold.value, 'item') else float(model.pixel_threshold.value),
            'pred_score_min': float(model.normalization_metrics.pred_scores.min.item()) if hasattr(model.normalization_metrics.pred_scores.min, 'item') else float(model.normalization_metrics.pred_scores.min),
            'pred_score_max': float(model.normalization_metrics.pred_scores.max.item()) if hasattr(model.normalization_metrics.pred_scores.max, 'item') else float(model.normalization_metrics.pred_scores.max),
            'anomaly_map_min': float(model.normalization_metrics.anomaly_maps.min.item()) if hasattr(model.normalization_metrics.anomaly_maps.min, 'item') else float(model.normalization_metrics.anomaly_maps.min),
            'anomaly_map_max': float(model.normalization_metrics.anomaly_maps.max.item()) if hasattr(model.normalization_metrics.anomaly_maps.max, 'item') else float(model.normalization_metrics.anomaly_maps.max)
        }
    except (AttributeError, TypeError) as e:
        raise RuntimeError(f"Failed to extract inference info from model: {e}")
    
    return inference_info

# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 33
def train_anomaly_model(
    config: Union[FlexibleTrainingConfig, Dict[str, Any], str, Path]
) -> Dict[str, Any]:
    """
    Train an anomaly detection model with maximum flexibility and production-ready error handling.
    
    
    Returns:
        Dictionary containing training results, model paths, and metrics.
    """
    # Parse configuration
    if isinstance(config, (str, Path)):
        config = FlexibleTrainingConfig.from_yaml(config)
    elif isinstance(config, dict):
        config = FlexibleTrainingConfig.from_dict(config)
    elif not isinstance(config, FlexibleTrainingConfig):
        raise TypeError(f"Config must be FlexibleTrainingConfig, dict, or path. Got {type(config)}")
    
    # Validate data root exists
    if not config.data_root.exists():
        raise FileNotFoundError(f"Data root path does not exist: {config.data_root}")
    
    try:
        # Helper function to safely get enum value
        def get_value(obj):
            return obj.value if hasattr(obj, 'value') else str(obj)
        
        print(f" Starting training with {get_value(config.model_name)} model")
        print(f" Normalization: {get_value(config.normalization_method)}")
        print(f" Image size: {config.image_size}")
        print(f" Threshold method: {get_value(config.threshold_method)}")
        
        # Create data module
        folder_datamodule = Folder(
            name=config.class_name,
            root=config.data_root,
            normal_dir=config.normal_dir,
            abnormal_dir=config.abnormal_dir,
            task=TaskType.CLASSIFICATION,
            train_batch_size=config.train_batch_size,
            eval_batch_size=config.eval_batch_size,
            num_workers=config.num_workers,
            image_size=config.image_size,
        )
        
        folder_datamodule.setup()
        print(f'{"="*100}')
        print(f'folder_datamodule.transform: {folder_datamodule.transform}')
        print(folder_datamodule.transform)
        print(f'{"="*100}')
        
        # Get model class and create model
        model_mapping = {
            ModelType.PADIM: Padim,
            ModelType.PATCHCORE: Patchcore,
            ModelType.CFLOW: Cflow,
            ModelType.FASTFLOW: Fastflow,
            ModelType.STFPM: Stfpm,
            ModelType.EFFICIENT_AD: EfficientAd,
            ModelType.DRAEM: Draem,
            ModelType.REVERSE_DISTILLATION: ReverseDistillation,
            ModelType.DFKDE: Dfkde,
            ModelType.DFM: Dfm,
            ModelType.GANOMALY: Ganomaly,
            ModelType.CFA: Cfa,
            ModelType.CSFLOW: Csflow,
            ModelType.DSR: Dsr,
            ModelType.FRE: Fre,
            ModelType.RKDE: Rkde,
            ModelType.UFLOW: Uflow,
        }
        
        model_class = model_mapping[config.model_name]
        
        # Create model with corrected parameters
        model_config = {
            'backbone': get_value(config.backbone)
        }
        
        # Add model-specific configurations
        if config.model_name in [ModelType.PADIM, ModelType.STFPM]:
            model_config['layers'] = config.layers

        if config.model_name in [ModelType.PADIM]:
            model_config['n_features'] = config.n_features
        print(f'{"="*100}')
        print(f'model_config: {model_config}')

        print(f'{"="*100}')
        
        model = model_class(**model_config)
        
        # Set up callbacks
        callbacks = []
        
        # Model checkpoint
        checkpoint_dir = config.save_path / "checkpoints" / config.class_name
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint_callback = ModelCheckpoint(
            dirpath=checkpoint_dir,
            filename=f"{get_value(config.model_name)}_{get_value(config.backbone)}_{{epoch:02d}}_{{image_AUROC:.4f}}",
            monitor=config.early_stopping_metric,
            mode=config.early_stopping_mode,
            save_top_k=config.save_top_k,
            save_last=True,
            verbose=True
        )
        callbacks.append(checkpoint_callback)
        
        # Early stopping
        if config.early_stopping:
            early_stop_callback = EarlyStopping(
                monitor=config.early_stopping_metric,
                patience=config.early_stopping_patience,
                mode=config.early_stopping_mode,
                verbose=True
            )
            callbacks.append(early_stop_callback)
        
        # Add thresholding callback for specific models
        if config.model_name in [ModelType.PADIM, ModelType.PATCHCORE, ModelType.STFPM, 
                                ModelType.CFLOW, ModelType.FASTFLOW]:
            from anomalib.metrics import ManualThreshold, F1AdaptiveThreshold
            threshold = F1AdaptiveThreshold() if config.threshold_method == ThresholdMethod.ADAPTIVE else ManualThreshold(config.manual_threshold)
            print(f" Added ThresholdCallback for {get_value(config.model_name)}")
        else:
            threshold = None
        
        # Add tiling callback if enabled
        if config.enable_tiling:
            from anomalib.callbacks import TilingConfigurationCallback
            # Use default values if None (anomalib will handle this)
            tile_size = config.tile_size if config.tile_size is not None else (256, 256)
            stride = config.stride if config.stride is not None else (128, 128)
            tiling_callback = TilingConfigurationCallback(
                tile_size=tile_size,
                stride=stride
            )
            callbacks.append(tiling_callback)
            print(f" Added TilingCallback with tile_size={tile_size}, stride={stride}")
        
        # Create engine
        engine = Engine(
            accelerator=config.accelerator,
            devices=config.devices,
            callbacks=callbacks,
            max_epochs=config.max_epochs,
            deterministic=config.deterministic,
            threshold=threshold,
            task=TaskType.CLASSIFICATION,
        )
        
        # Start training
        print(" Starting training...")
        start_time = datetime.now()
        
        engine.fit(model=model, datamodule=folder_datamodule)
        
        
        end_time = datetime.now()
        training_duration = end_time - start_time
        
        print(f" Training completed in {training_duration}")
        
        # Get results
        best_model_path = checkpoint_callback.best_model_path
        
        # Test the model
        #test_results = engine.test(
            #model=model,
            #datamodule=folder_datamodule,
        #)
        
        # Export model if requested
        export_paths = {}
        if config.export_formats:
            export_dir = config.save_path / "exports" / config.class_name
            export_dir.mkdir(parents=True, exist_ok=True)
            
            for export_format in config.export_formats:
                try:
                    export_path = engine.export(
                        model=model,
                        export_type=export_format,
                        export_root=export_dir
                    )
                    export_paths[get_value(export_format)] = str(export_path)
                    print(f" Exported {get_value(export_format)}: {export_path}")
                except Exception as e:
                    print(f" Export failed for {get_value(export_format)}: {str(e)}")
        
        # Extract model threshold and pixel statistics for inference
        model_inference_info = _extract_model_inference_info(model)
        
        # Compile results
        results = {
            'success': True,
            'config': config.to_dict(),
            'image_threshold': model_inference_info.get('image_threshold'),
            'pixel_threshold': model_inference_info.get('pixel_threshold'),
            'pred_score_min': model_inference_info.get('pred_score_min'),
            'pred_score_max': model_inference_info.get('pred_score_max'),
            'anomaly_map_min': model_inference_info.get('anomaly_map_min'),
            'anomaly_map_max': model_inference_info.get('anomaly_map_max'),
            'training_duration': str(training_duration),
            'best_model_path': str(best_model_path) if best_model_path else None,
            'export_paths': export_paths,
            #'test_results': test_results[0] if test_results else None,
            'anomalib_version': anomalib.__version__,
            'timestamp': datetime.now().isoformat()
        }
        
        print("üéâ Training completed successfully!")
        return results
        
    except Exception as e:
        print(f"‚ùå Training failed: {str(e)}")
        error_results = {
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__,
            'config': config.to_dict() if config else None,
            'timestamp': datetime.now().isoformat()
        }
        return error_results

# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 43
def _extract_model_inference_info(model) -> Dict[str, Any]:
    """Extract threshold and pixel statistics from trained model for inference.
    
    Args:
        model: Trained anomaly detection model
        
    Returns:
        Dictionary containing inference parameters including thresholds and normalization metrics
        
    Raises:
        AttributeError: If model doesn't have required threshold or normalization attributes
        RuntimeError: If model hasn't been trained or fitted yet
    """
    if not hasattr(model, 'image_threshold') or not hasattr(model, 'pixel_threshold'):
        raise AttributeError("Model missing required threshold attributes. Ensure model is properly trained.")
    
    if not hasattr(model, 'normalization_metrics'):
        raise RuntimeError("Model normalization metrics not available. Model may not be fitted yet.")
    
    try:
        inference_info = {
            'image_threshold': float(model.image_threshold.value.item()) if hasattr(model.image_threshold.value, 'item') else float(model.image_threshold.value),
            'pixel_threshold': float(model.pixel_threshold.value.item()) if hasattr(model.pixel_threshold.value, 'item') else float(model.pixel_threshold.value),
            'pred_score_min': float(model.normalization_metrics.pred_scores.min.item()) if hasattr(model.normalization_metrics.pred_scores.min, 'item') else float(model.normalization_metrics.pred_scores.min),
            'pred_score_max': float(model.normalization_metrics.pred_scores.max.item()) if hasattr(model.normalization_metrics.pred_scores.max, 'item') else float(model.normalization_metrics.pred_scores.max),
            'anomaly_map_min': float(model.normalization_metrics.anomaly_maps.min.item()) if hasattr(model.normalization_metrics.anomaly_maps.min, 'item') else float(model.normalization_metrics.anomaly_maps.min),
            'anomaly_map_max': float(model.normalization_metrics.anomaly_maps.max.item()) if hasattr(model.normalization_metrics.anomaly_maps.max, 'item') else float(model.normalization_metrics.anomaly_maps.max)
        }
    except (AttributeError, TypeError) as e:
        raise RuntimeError(f"Failed to extract inference info from model: {e}")
    
    return inference_info

# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 44
def validate_model_name(model_name: Union[str, ModelType])->ModelType:
    if isinstance(model_name, str):
        try:
            model_name = ModelType(model_name.lower())
        except ValueError:
            valid_models = [m.value for m in ModelType]
            raise ValueError(f"Invalid model name: {model_name}. Valid options are: {valid_models}")
    return model_name


# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 45
def validate_backbone_name(backbone_name: str)->BackboneType:
    """Validate the backbone name."""
    if isinstance(backbone_name, str):
        try:
            backbone_name = BackboneType(backbone_name.lower())
        except ValueError:
            valid_backbones = [b.value for b in BackboneType]
            raise ValueError(f"Invalid backbone name: {backbone_name}. Valid options are: {valid_backbones}")
    return backbone_name

validate_backbone_name("resnet18")

# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 47
@call_parse
def main_(
    data_root: str, # Name of the directory containing the data, inside this folder there should two other folder for normal and abnormal images
    class_name: str = "anomaly_detection", #  What anomaly class you are detection, default anomaly_detection
    normal_dir: str = "good", # Name of the directory containing normal images
    abnormal_dir: str = "bad", # Name of the directory containing abnormal images
    model_name: str = "padim", # Model to use for training, default padim
    backbone: str = "resnet18", # Backbone to use for training, default resnet18
    n_features: int = 100, # Number of features to use for training, default 100
    layers: list[str] = ['layer1', 'layer2', 'layer3'], # Layers to use for training, default ['layer1', 'layer2', 'layer3']
    image_size: tuple[int, int] = None, # Size of the images to use for training, uses anomalib default (256, 256) if None
    #normalization: str = "imagenet", # Normalization to use for training, default imagenet
    train_batch_size: int = None, # Batch size for training, auto-detected based on memory if None
    eval_batch_size: int = None, # Batch size for evaluation, auto-detected based on memory if None  
    num_workers: int = None, # Number of workers for data loading, auto-detected based on environment if None
    max_epochs: int = 100, # Maximum number of epochs to train, default 100
    accelerator: str = "auto", # Accelerator to use for training, default auto
    devices: str = "auto", # Devices to use for training, default auto
    save_path: str = "./models", # Path to save the model, default ./models
    seed: int = None, # Seed to use for training, default None
    export_formats: list[str] = ['torch'], # Formats to export the model, default ['torch']
    enable_tiling: bool = False, # Enable tiling for training, default False
    tile_size: tuple[int, int] = None, # Size of the tiles to use for training, uses anomalib default (None) if None
    stride: tuple[int, int] = None, # Stride to use for training, uses anomalib default (None) if None
    enable_tensorboard: bool = None, # Enable tensorboard for training, uses anomalib default (False) if None
    enable_csv_logger: bool = None, # Enable csv logger for training, uses anomalib default (False) if None
    log_level: str = None, # Log level to use for training, uses anomalib default ('INFO') if None
    enable_progress_bar: bool = None, # Enable progress bar, auto-detected based on environment if None
    num_sanity_val_steps: int = None, # Number of validation sanity steps, auto-detected based on environment if None
):
    """
    üöÄ Intelligent Anomaly Detection Training CLI with Anomalib Defaults
    ü§ñ Smart Auto-Detection Features:
    
    üí° Override any parameter by providing explicit values!
    """
    # Validate and convert string inputs to enums
    model_name = validate_model_name(model_name)
    backbone = validate_backbone_name(backbone)

    print(f"üöÄ Starting training with {model_name.value} model using {backbone.value} backbone")
    
    # Apply anomalib default values for None parameters
    # These are the standard defaults used by anomalib library
    ANOMALIB_DEFAULTS = {
        'image_size': (256, 256),      # Anomalib standard image size
        'tile_size': None,             # Disabled by default in anomalib
        'stride': None,                # Disabled by default in anomalib  
        'enable_tensorboard': False,   # Disabled by default in anomalib
        'enable_csv_logger': False,    # Disabled by default in anomalib
        'log_level': 'INFO',          # Standard logging level
    }
    
    # Use user values if provided, otherwise use anomalib defaults
    if image_size is None:
        image_size = ANOMALIB_DEFAULTS['image_size']
        print(f"   üìê Using anomalib default image_size: {image_size}")
    else:
        print(f"   üìê Using user-specified image_size: {image_size}")
        
    if tile_size is None:
        tile_size = ANOMALIB_DEFAULTS['tile_size']
        print(f"   üî≤ Using anomalib default tile_size: {tile_size}")
    else:
        print(f"   üî≤ Using user-specified tile_size: {tile_size}")
        
    if stride is None:
        stride = ANOMALIB_DEFAULTS['stride']
        print(f"   ‚ÜóÔ∏è  Using anomalib default stride: {stride}")
    else:
        print(f"   ‚ÜóÔ∏è  Using user-specified stride: {stride}")
        
    if enable_tensorboard is None:
        enable_tensorboard = ANOMALIB_DEFAULTS['enable_tensorboard']
        print(f"   üìä Using anomalib default enable_tensorboard: {enable_tensorboard}")
    else:
        print(f"   üìä Using user-specified enable_tensorboard: {enable_tensorboard}")
        
    if enable_csv_logger is None:
        enable_csv_logger = ANOMALIB_DEFAULTS['enable_csv_logger']
        print(f"   üìù Using anomalib default enable_csv_logger: {enable_csv_logger}")
    else:
        print(f"   üìù Using user-specified enable_csv_logger: {enable_csv_logger}")
        
    if log_level is None:
        log_level = ANOMALIB_DEFAULTS['log_level']
        print(f"   üîç Using anomalib default log_level: {log_level}")
    else:
        print(f"   üîç Using user-specified log_level: {log_level}")
    
    # Build config dict with resolved values (user-specified or anomalib defaults)
    config_params = {
        'class_name': class_name,
        'data_root': data_root,
        'normal_dir': normal_dir,
        'abnormal_dir': abnormal_dir,
        'image_size': image_size,
        'model_name': model_name,
        'backbone': backbone,
        'n_features': n_features,
        'layers': layers,
        'max_epochs': max_epochs,
        'accelerator': accelerator,
        'devices': devices,
        'save_path': save_path,
        'seed': seed,
        'export_formats': [ExportType(fmt) for fmt in export_formats],
        'enable_tiling': enable_tiling,
        'tile_size': tile_size,
        'stride': stride,
        'enable_tensorboard': enable_tensorboard,
        'enable_csv_logger': enable_csv_logger,
        'log_level': log_level,
    }
    
    # Only add parameters that were explicitly provided (not None)
    # This allows FlexibleTrainingConfig to use its intelligent defaults for None values
    if train_batch_size is not None:
        config_params['train_batch_size'] = train_batch_size
        print(f"   üì¶ Using user-specified train_batch_size: {train_batch_size}")
    
    if eval_batch_size is not None:
        config_params['eval_batch_size'] = eval_batch_size
        print(f"   üì¶ Using user-specified eval_batch_size: {eval_batch_size}")
        
    if num_workers is not None:
        config_params['num_workers'] = num_workers
        print(f"   ‚öôÔ∏è  Using user-specified num_workers: {num_workers}")
        
    if enable_progress_bar is not None:
        config_params['enable_progress_bar'] = enable_progress_bar
        print(f"   üìä Using user-specified enable_progress_bar: {enable_progress_bar}")
        
    if num_sanity_val_steps is not None:
        config_params['num_sanity_val_steps'] = num_sanity_val_steps
        print(f"   üß™ Using user-specified num_sanity_val_steps: {num_sanity_val_steps}")
    
    # Create config - this will apply smart defaults for any None/missing values
    config = FlexibleTrainingConfig(**config_params)
    
    return train_anomaly_model(config)

# %% ../../nbs/04_training.flexible_anomaly_trainer.ipynb 73
def run_inference_after_training(
    training_results: Dict[str, Any],
    validation_images: Optional[Union[str, Path, List[Union[str, Path]]]] = None,
    test_images: Optional[Union[str, Path, List[Union[str, Path]]]] = None,
    create_heatmaps: bool = True,
    poster_rows: int = 4,
    poster_cols: int = 4,
    output_folder: Optional[Union[str, Path]] = None
) -> Dict[str, Any]:
    """
    Convenience function to run inference and create posters directly from training results.
    
    Args:
        training_results: Results dictionary from train_anomaly_model()
        validation_images: Path to validation images folder or list of image paths
        test_images: Path to test images folder or list of image paths
        create_heatmaps: Whether to create heatmap posters (requires exported model)
        poster_rows: Number of rows in poster grid
        poster_cols: Number of columns in poster grid
        output_folder: Output folder (auto-generated if None)
        
    Returns:
        Dictionary with inference results and poster paths
    """
    
    # Validate training results
    if not training_results.get('success', False):
        raise ValueError("Training was not successful. Cannot proceed with inference.")
    
    # Get model path from training results
    model_path = None
    
    # Try exported model first (better for inference)
    export_paths = training_results.get('export_paths', {})
    if 'torch' in export_paths:
        model_path = export_paths['torch']
        print(f"üéØ Using exported model: {model_path}")
    elif training_results.get('best_model_path'):
        model_path = training_results['best_model_path']
        print(f"üéØ Using checkpoint model: {model_path}")
    else:
        raise ValueError("No valid model path found in training results")
    
    # Auto-generate output folder if not provided
    if output_folder is None:
        config = training_results.get('config', {})
        class_name = config.get('class_name', 'anomaly_detection')
        model_name = config.get('model_name', 'model')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if create_heatmaps:
            output_folder = f"inference_results_{class_name}_{model_name}_heatmaps_{timestamp}"
        else:
            output_folder = f"inference_results_{class_name}_{model_name}_{timestamp}"
    
    print(f"üöÄ Running inference after training")
    print(f"   Class: {training_results.get('config', {}).get('class_name', 'Unknown')}")
    print(f"   Model: {model_path}")
    print(f"   Heatmaps: {'Yes' if create_heatmaps else 'No'}")
    print(f"   Output: {output_folder}")
    
    # Run appropriate inference function
    if create_heatmaps:
        # Adjust columns for side-by-side if needed
        if poster_cols % 2 != 0:
            poster_cols += 1
            print(f"   Adjusted columns to {poster_cols} for heatmap layout")
            
        return create_inference_poster_with_heatmaps(
            model_path=model_path,
            validation_images=validation_images,
            test_images=test_images,
            output_folder=output_folder,
            poster_rows=poster_rows,
            poster_cols=poster_cols,
            heatmap_style="side_by_side",
            poster_title=f"{training_results.get('config', {}).get('class_name', 'Anomaly')} Detection"
        )
    else:
        return create_inference_poster(
            model_path=model_path,
            validation_images=validation_images,
            test_images=test_images,
            output_folder=output_folder,
            poster_rows=poster_rows,
            poster_cols=poster_cols,
            poster_title=f"{training_results.get('config', {}).get('class_name', 'Anomaly')} Detection"
        )

